{"cells":[{"cell_type":"markdown","metadata":{"id":"egRLexIGTU_V"},"source":["# Deep Learning with TensorFlow\n","## Summative assessment 2"]},{"cell_type":"markdown","metadata":{"id":"zDMgSANXTU_e"},"source":["#### Instructions\n","\n","There are 3 questions in this assessment. **You should attempt to answer all questions.** \n","\n","You can make imports as and when you need them throughout the notebook, and add code cells where necessary. Make sure your notebook executes correctly in sequence before submitting.\n","\n","You have 2 hours and 30 minutes to complete this assessment.\n","\n","#### How to submit\n","\n","When you have finished and are happy with your code, make sure all cells are executed and their outputs printed, and then save as an html file. You should upload and submit the following files to Turnitin on Blackboard **in a single zip file**:\n","\n","* Your completed jupyter notebook file (`.ipynb` file format)\n","* The executed notebook saved as an `.html` file\n","\n","You are also required to name your zip file as _'SurnameCID.zip'_, e.g. _Smith1234567.zip_. Do not submit multiple files. The submitted ipynb file must produce the output that appears in your html file.\n","\n","Make sure you submit your files before the exam deadline of **Thursday 12th May 11.40am** (extra 10 minutes is included for preparing and uploading the files).\n","\n","_Important:_ As this is assessed work you need to work on it individually. It must be your own and unaided work. You are not allowed to discuss the assessment with your fellow students or anybody else. All rules regarding academic integrity and plagiarism apply. Violations of this will be treated as an examination offence. In particular, letting somebody else copy your work constitutes an examination offence. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxKfOwVETU_g"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","tfd = tfp.distributions\n","tfb = tfp.bijectors\n","tfpl = tfp.layers"]},{"cell_type":"markdown","metadata":{"id":"0qPz1x92TU_i"},"source":["### Question 1 (Total 30 marks)"]},{"cell_type":"markdown","metadata":{"id":"4mTXSHQhTU_j"},"source":["In this question you will work with the [Human Activity Recognition (HAR) Using Smartphones]\n","\n","The dataset is also available to download from Blackboard, under Course Content -> Assessments and Mark Schemes -> Summative Assessment 2 - HAR dataset.\n","\n","You should download this dataset and store it in the current working directory, so that the data files are available inside the folder `'./data/HAR/'`. \n","\n","The dataset consists of the readings from an accelerometer (which measures acceleration) carried by a human doing different activities. The six activities are walking horizontally, walking upstairs, walking downstairs, sitting, standing and laying down. The accelerometer is inside a smartphone, and, every 0.02 seconds (50 times per second), it takes six readings: linear and gyroscopic acceleration in the x, y and z directions. Each example in the dataset consists of these six readings recorded at 128 time steps.\n","\n","The dataset can be loaded by running the following cell."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"id":"I2u_7w-qTU_k","executionInfo":{"status":"error","timestamp":1652346229234,"user_tz":-180,"elapsed":590,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"41589c94-c309-4bcd-a134-ae88869e0400"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e316b00fc601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhar_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/HAR/x_train.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhar_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/HAR/y_train.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhar_x_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/HAR/x_test.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/HAR/x_train.npy'"]}],"source":["import numpy as np\n","from pathlib import Path\n","\n","har_x_train = np.load(Path(\"./data/HAR/x_train.npy\"))\n","har_y_train = np.load(Path(\"./data/HAR/y_train.npy\"))\n","har_x_test = np.load(Path(\"./data/HAR/x_test.npy\"))\n","har_y_test = np.load(Path(\"./data/HAR/y_test.npy\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tegGF-XvTU_m"},"outputs":[],"source":["# These label names correspond with the integer labels loaded above\n","\n","class_names = [\n","    'walking horizontally', \n","    'walking upstairs', \n","    'walking downstairs', \n","    'sitting', \n","    'standing', \n","    'laying'\n","]"]},{"cell_type":"markdown","metadata":{"id":"fGGM0sT2TU_n"},"source":["Define a convolutional autoencoder for the HAR dataset with the specifications given in parts a) and b). This autoencoder model should use 1-dimensional convolutions and maxpooling layers, as well as transposed convolutions and upsampling layers. \n","\n","a) Create the encoder network which maps the input data to a latent space of 2 dimensions. It should use the following sequence of layers:\n","\n","* A convolutional layer with 16 filters, a kernel width of 5, a stride of 1, 'VALID' padding, and a ReLU activation\n","* A max pooling layer with pooling window width - and stride - of 2\n","* A convolutional layer with 8 filters, a kernel width of 5, a stride of 1, 'VALID' padding, and a ReLU activation\n","* A `Flatten` layer, followed by two `Dense` layers with 64 and 16 units respectively and a ReLU activation\n","* A final `Dense` layer with 2 neurons and no activation function\n","\n","Create the encoder network using the `Sequential` API and print the model summary. **(10 marks)** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9OTSmiwTU_o"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9MbNWHeTU_p"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tlbhvLyTU_q"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"4YU9Amh8TU_q"},"source":["b) Create the decoder network which maps the latent space encoding back into the data space to reconstruct the original data example. It is structurally built to be (approximately) the opposite to the encoder network. It should use the following sequence of layers:\n","\n","* Three `Dense` layers with 16, 64 and 464 units respectively, and a ReLU activation\n","* A layer that reshapes the incoming Tensor to have shape `(58, 8)`\n","* A transposed convolution with 8 filters, a kernel width of 5, a stride of 1, 'VALID' padding, and a ReLU activation\n","* A layer that upsamples the incoming Tensor with an upsampling factor of 2\n","* A transposed convolution with 6 filters, a kernel width of 5, a stride of 1, 'VALID' padding, and no activation function\n","\n","Create the decoder network using the `Sequential` API and print the model summary. **(10 marks)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Opeb8FATU_r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZ53ntEaTU_s"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DpflsezTU_s"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"lQdOZKjUTU_s"},"source":["c) You will now build and train the end-to-end autoencoder model, and inspect the latent space encodings before and after training.\n","\n","* Compute the latent space encodings from the freshly initialised (untrained) encoder network on the first 1000 examples of the test dataset.\n","* Define a `Model` object for the end-to-end autoencoder that passes the input through the encoder network and then the decoder network. Train this model for 15 epochs on the training data, using a batch size of 64, the RMSprop optimizer with a learning rate of 0.01, and the mean squared error loss function.\n","* Now compute the latent space encodings from the trained encoder network on the same first 1000 examples of the test dataset.\n","* Make two scatter plots, one for the untrained encodings and one for the trained encodings of the encoder network. The points in your scatter plots should be coloured according to the different classes in the dataset. \n","\n","**(10 marks)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yr2tnLhZTU_t"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iEh97YVTU_t"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kl0KQOshTU_u"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"15KWkjvSTU_u"},"source":["### Question 2 (Total 30 marks)\n","\n","a) The 3D Hénon map is the bijective map $f:\\mathbb{R}^3\\mapsto\\mathbb{R}^3$ defined as:\n","\n","$$\n","\\begin{align}\n","y_1 &= a_1x_1 + e^{a_2}(x_3 + x_2^2)\\\\\n","y_2 &= x_1\\\\\n","y_3 &= x_2\n","\\end{align}\n","$$\n","\n","where $a_1$ and $a_2$ are parameters.\n","\n","Implement this mapping as a custom bijector `Henon3d` by subclassing the `tfb.Bijector` class. Your class should implement the following methods:\n","\n","* `__init__`: initialiser should take arguments `a1` and `a2` and set them as class attributes. It should call the base class initialiser and set `forward_min_event_ndims=1`\n","* `_forward`: this should take an argument `x`, and implement the transformation $f: (x_1, x_2, x_3)\\mapsto (y_1, y_2, y_3)$ above\n","* `_inverse`: this should take an argument `y`, and implement the inverse transformation $f^{-1}: (y_1, y_2, y_3)\\mapsto (x_1, x_2, x_3)$. You should calculate this from the equations above\n","* `_forward_log_det_jacobian`: this should take an argument `x`, and return the log of the absolute value of the Jacobian determinant $\\log \\hspace{0.1ex}\\left|\\det J_f(\\mathbf{z}) \\right|$. You should calculate this from the equations above. \n","* `_inverse_log_det_jacobian`: this should take an argument `y`, and return the log of the absolute value of the Jacobian determinant of the inverse; $\\log \\hspace{0.1ex}\\left|\\det J_{f^{-1}}(\\mathbf{z}) \\right|$.\n","\n","The `_forward` and `_inverse` methods should account for any additional batch or sample dimensions.\n","\n","_Hint: in the `_forward` method, you will need to extract the components $x_1$, $x_2$ and $x_3$ by indexing the argument `x` before computing the transformation. You will need to recombine the output components $y_1$, $y_2$ and $y_3$ in the returned Tensor. Similarly for the `_inverse` method._\n","\n","**(20 marks)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuWTTXVaTU_v"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"0MjxTUBDTU_w"},"source":["b) You will now use your `Henon3d` custom bijector to construct a normalising flow model.\n","\n","* Define a Distribution object called `uniform` for the uniform distribution $U([-1, 1]^3)$. This Distribution object should have an event shape of `[3]` and an empty batch shape.\n","* Create three instances of the `Henon3d` bijector, called `f1`, `f2` and `f3`. You should pass scalar trainable Variables (initialised randomly) for the initialiser arguments `a1` and `a2` for each one (so you will create 6 `tf.Variable` objects in all).\n","* Define a bijector object `permutation` that performs the permutation of dimensions \n","$$(x_1, x_2, x_3) \\mapsto (x_2, x_3, x_1)$$\n","* Create a `TransformedDistribution` object that transforms samples from the `uniform` distribution through the sequence of bijections:\n","\n","`f1` $\\rightarrow$ `permutation` $\\rightarrow$ `f2` $\\rightarrow$ `permutation` $\\rightarrow$ `f3` $\\rightarrow$ `permutation`\n","\n","* Draw 5 samples from your `TransformedDistribution` object and print out the result. **(10 marks)**\n","\n","_NB: there is no training required in this question._"]},{"cell_type":"markdown","metadata":{"id":"BcQlI0I0TU_x"},"source":["### Question 3 (Total 40 marks)\n","\n","In this question you will implement a conditional VAE algorithm (C-VAE) for the [MNIST](https://keras.io/api/datasets/mnist/) dataset. This dataset can be downloaded by running the following cell."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"cUkpMkYITU_x","executionInfo":{"status":"ok","timestamp":1652348164988,"user_tz":-180,"elapsed":1391,"user":{"displayName":"ABC","userId":"13100262138380101532"}}},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.layers import Input, Dense \n","from tensorflow.keras.layers import Flatten, Dense\n","from keras.layers import BatchNormalization, Dropout, Flatten, Reshape, Lambda\n","from keras.layers import concatenate\n","from keras.models import Model\n","from keras.objectives import binary_crossentropy\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras import backend as K\n","from tensorflow.keras import Sequential\n","\n","\n","%matplotlib inline\n","\n","\n","from tensorflow.keras.datasets import mnist\n","(mnist_x_train, mnist_y_train), (mnist_x_val, mnist_y_val) = mnist.load_data()"]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBLZcOQ6j67L","executionInfo":{"status":"ok","timestamp":1652347129621,"user_tz":-180,"elapsed":215,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"875eb39c-e254-4d35-fd60-5f0c4b6c25b6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"49GwgqH8TU_y"},"source":["The C-VAE algorithm makes use of the class labels in the MNIST dataset. In particular, the class labels are provided as additional inputs to the encoder and decoder networks. So the approximate posterior $q_\\phi(z\\mid x, c)$ and the distribution $p_\\theta(x \\mid z, c)$ are now both conditioned on the class label $c$."]},{"cell_type":"markdown","metadata":{"id":"FonRH5x9TU_y"},"source":["a) Load the training and validation data into `tf.Dataset` objects. Both the training and validation Datasets should return tuples of image and label Tensors.\n","\n","Process the training and validation Datasets as follows, using the `map` method:\n","\n","* Rescale the image pixel values to the range $[0, 1]$\n","* Convert the integer labels to one-hot vectors (_hint: see [`tf.one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot)_)\n","* Return a nested tuple of image and labels Tensors of the form `((image, label), image)`\n","\n","The training Dataset should be shuffled (with buffer size 500) and both Datasets should be batched with batch size 64. Print the `element_spec` for one of the Datasets. **(8 marks)**"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dmz9uINsTU_y","executionInfo":{"status":"ok","timestamp":1652349997148,"user_tz":-180,"elapsed":278,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"9cd481b2-77a8-40c2-9571-b544af11ef83"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>"]},"metadata":{},"execution_count":21}],"source":["mnist_train_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n","\n","mnist_train_ds\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKj8gf5ETU_y","executionInfo":{"status":"ok","timestamp":1652350006871,"user_tz":-180,"elapsed":6,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"9b18db6f-a930-4bfb-eafd-02488c7e457b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, None, 28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(None, None), dtype=tf.uint8, name=None))>"]},"metadata":{},"execution_count":23}],"source":["def rescale_mnist(image, label):\n","    image = tf.cast(image, tf.float32)\n","    image = image / 255.\n","    return image, label\n","\n","mnist_train_ds = mnist_train_ds.shuffle(500).batch(64)\n","mnist_train_ds\n"]},{"cell_type":"markdown","metadata":{"id":"Y1e8e8wOTU_z"},"source":["Your C-VAE algorithm will be trained to optimise the following objective:\n","\n","$$\n","\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) := \\log p_\\theta(x \\mid \\hat{z}, c) + \\frac{1}{L} \\sum_{j=1}^L \\log p(z^{(j)}) − \\log q_\\phi(z^{(j)}\\mid x, c) \\\\\n","\\text{where } p(z) \\text{ is the prior distribution, and }\\hat{z}, z^{(j)} \\sim q_\\phi(z | x, c)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"jw4a4_jpTU_0"},"source":["b) Define the prior distribution as a zero mean Gaussian with identity covariance, acting on a one-dimensional latent space of dimension 2.\n","\n","Build the encoder network as an MLP that uses a probabilistic layer to output a `MultivariateNormalTriL` Distribution as the approximate posterior $q_\\phi(z \\mid x, c)$. The structure of the encoder should be as follows:\n","\n","* The encoder should flatten the input image, and concatenate it with the one-hot label vector.\n","* It should then pass this concatenated Tensor through two `Dense` layers with 512 and 32 units respectively, both with a ReLU activation\n","* The resulting Tensor should be passed through another `Dense` layer with enough units to parameterise the following `MultivariateNormalTriL` probabilistic layer. This `Dense` layer should not use an activation function.\n","* The following `MultivariateNormalTriL` probabilistic layer should output a `MultivariateNormalTriL` distribution over an event space of shape `[2]`\n","* The encoder network should also use a `KLDivergenceAddLoss` layer to add the KL approximation term $\\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(z^{(j)}) − \\log q_\\phi(z^{(j)}\\mid x, c)$ to the loss, using $L=5$\n","\n","You should create the encoder using the functional API as a multi-input model, where the inputs are the image and the one-hot vector label. Print the encoder summary. \n","\n","_Hint: You should use two `Input` layers for the image and labels Tensor respectively, and pass both of these inputs in a list `[image_input, label_input]` to the `inputs` argument of the `Model` class._ **(13 marks)**"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJ9sDuyRTU_0","executionInfo":{"status":"ok","timestamp":1652350050077,"user_tz":-180,"elapsed":779,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"4df43826-8d5e-4cf1-c703-aabfaf6d1224"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 28, 28)]          0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 100)               78500     \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras import Model\n","inputs = Input(shape=(28, 28))\n","h = Flatten()(inputs)\n","h = Dense(100, activation='relu')(h)\n","outputs = Dense(10, activation='softmax')(h)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd')\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"xzw_0I6STU_1"},"source":["c) Build the decoder network as an MLP according to the following specifications:\n","\n","* The inputs to the decoder are the 2-dimensional variable $z$ and the one-hot class label $c$\n","* The decoder should concatenate the latent variable and the class label to make a length-12 Tensor\n","* This should then be passed through two `Dense` layers with 32 and 512 units respectively, each with a ReLU activation function\n","* The resulting Tensor should be passed through another `Dense` layer with enough units to parameterise the following `IndependentBernoulli` probabilistic layer. This `Dense` layer should not use an activation function.\n","* The following `IndependentBernoulli` probabilistic layer should output a `Bernoulli` Distribution object over an event space of shape `[28, 28]`\n","\n","You should create the decoder using the functional API as a multi-input model, where the inputs are the latent variable and the one-hot vector label. Print the decoder summary. \n","\n","_Hint: Use the same `Input` Tensor for the class label that you used for the encoder network. You will only need to create one new `Input` Tensor for the latent variable $z$._ **(9 marks)**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"rSF-dk41TU_1"},"outputs":[],"source":["\n","z = Input(shape=(latent_dim, ))\n","input_lbl_d = Input(shape=(num_classes,), dtype='float32')\n","x = concatenate([z, input_lbl_d])\n","x = Dense(256)(x)\n","x = LeakyReLU()(x)\n","x = apply_bn_and_dropout(x)\n","x = Dense(28*28, activation='sigmoid')(x)\n","decoded = Reshape((28, 28, 1))(x)\n","\n","models[\"decoder\"] = Model([z, input_lbl_d], decoded, name='Decoder')\n","models[\"cvae\"]    = Model([input_img, input_lbl, input_lbl_d], \n","                            models[\"decoder\"]([models[\"encoder\"]([input_img, input_lbl]), input_lbl_d]), \n","                            name=\"CVAE\")\n","models[\"style_t\"] = Model([input_img, input_lbl, input_lbl_d], \n","                            models[\"decoder\"]([models[\"z_meaner\"]([input_img, input_lbl]), input_lbl_d]), \n","                            name=\"style_transfer\")\n","\n","\n","def cvae_loss(x, decoded):\n","    x = K.reshape(x, shape=(batch_size, 28*28))\n","    decoded = K.reshape(decoded, shape=(batch_size, 28*28))\n","    xent_loss = 28*28*binary_crossentropy(x, decoded)\n","    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","    return (xent_loss + kl_loss)/2/28/28\n","\n","return models, cvae_loss"]},{"cell_type":"markdown","metadata":{"id":"XperJnk4TU_2"},"source":["d) Now define a Model object `cvae` for the end-to-end architecture. The model should take the `[image, label]` Tensors as inputs, and return the Bernoulli Distribution object output by the decoder. Print the model summary. **(5 marks)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_2bxwfoTU_2"},"outputs":[],"source":["models, vae_loss = create_cvae()\n","cvae = models[\"cvae\"]"]},{"cell_type":"markdown","metadata":{"id":"oY-8gvrjTU_3"},"source":["e) Define the negative log-likelihood loss function that takes the arguments `y_true` and `y_pred`, where `y_pred` is a Distribution object output by the `cvae` model, and `y_true` is the ground truth Tensor object.\n","\n","Compile the `cvae` model using the Adam optimizer and the negative log-likelihood loss function. Train the model on the training dataset for 5 epochs. **(5 marks)**\n","\n","_NB: Even though it is not mathematically correct to use a Bernoulli distribution in the output of the decoder, this is commonly done in practice, and often works better than e.g. a Gaussian distribution._"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ffKvo4C_TU_3","executionInfo":{"status":"ok","timestamp":1652342932609,"user_tz":-180,"elapsed":28,"user":{"displayName":"ABC","userId":"13100262138380101532"}}},"outputs":[],"source":["def get_compiled_model(encoder, decoder):\n","    cvae = Model(inputs=encoder.inputs, outputs=decoder(encoder.outputs))\n","    \n","    def reconstruction_loss(batch_of_images, decoding_dist):\n","        return -tf.reduce_mean(decoding_dist.log_prob(batch_of_images))\n","    \n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n","    vae.compile(optimizer=optimizer, loss=reconstruction_loss, metrics=['mae'])\n","    return cvae"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"_Summative_assessment_2 (2).ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}