{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a210f66",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow\n",
    "## Summative assessment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bb3fd",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "There are 3 questions in this assessment. **You should attempt to answer all questions.** \n",
    "\n",
    "You can make imports as and when you need them throughout the notebook, and add code cells where necessary. Make sure your notebook executes correctly in sequence before submitting.\n",
    "\n",
    "You have 2 hours and 30 minutes to complete this assessment.\n",
    "\n",
    "#### How to submit\n",
    "\n",
    "When you have finished and are happy with your code, make sure all cells are executed and their outputs printed, and then save as an html file. You should upload and submit the following files to Turnitin on Blackboard **in a single zip file**:\n",
    "\n",
    "* Your completed jupyter notebook file (`.ipynb` file format)\n",
    "* The executed notebook saved as an `.html` file\n",
    "\n",
    "You are also required to name your zip file as _'SurnameCID.zip'_, e.g. _Smith1234567.zip_. Do not submit multiple files. The submitted ipynb file must produce the output that appears in your html file.\n",
    "\n",
    "Make sure you submit your files before the exam deadline of **Friday 18th March, 12.40pm** (extra 10 minutes is included for preparing and uploading the files).\n",
    "\n",
    "Please regularly check your email and Blackboard in case it is necessary to send any announcements during the assessment.\n",
    "\n",
    "_Important:_ As this is assessed work you need to work on it individually. It must be your own and unaided work. You are not allowed to discuss the assessment with your fellow students or anybody else. All rules regarding academic integrity and plagiarism apply. Violations of this will be treated as an examination offence. In particular, letting somebody else copy your work constitutes an examination offence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638da1d2",
   "metadata": {},
   "source": [
    "### Question 1 (Total 30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43011c2",
   "metadata": {},
   "source": [
    "This question uses the [CIFAR-10](https://keras.io/api/datasets/cifar10/) dataset, which you should load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(cifar10_x_train, cifar10_y_train), (cifar10_x_test, cifar10_y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51562895",
   "metadata": {},
   "source": [
    "a) Randomly select 20 examples from the CIFAR-10 training dataset and display the images along with their (integer) labels. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188c007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd4265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8f1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef622f6",
   "metadata": {},
   "source": [
    "b) Use the Sequential API to implement an MLP classifier model for the CIFAR-10 dataset (as loaded above) with the following spec:\n",
    "\n",
    "* The model first flattens the input image into a 1-D Tensor.\n",
    "* The model then passes the image through 3 Dense layers of width 64, 32 and 16 respectively, using a sigmoid activation function.\n",
    "* The final Dense layer has width 10, and no activation function.\n",
    "\n",
    "Print the model summary, and compile it with a suitable optimizer and loss function, and an accuracy metric. **(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccd4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d8937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bdcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9fd1b9",
   "metadata": {},
   "source": [
    "c) Create a custom layer called `GlobalMaxAveragePooling2D`, that performs a global pooling operation over the spatial dimensions (height and width). \n",
    "\n",
    "* The layer initialiser should take one required argument `data_format`, which can take the values `'channels_first'` or `'channels_last'`, corresponding to the ordering of dimensions in the inputs.   \n",
    "  * `'channels_first'` assumes that the input is of the form `(batch_size, channels, height, width)`\n",
    "  * `'channels_last'` assumes that the input is of the form `(batch_size, height, width, channels)`\n",
    "* The layer should compute the global **mean** pixel value per channel, as well as the global **max** pixel value per channel, i.e. it should reduce out the height and width dimensions\n",
    "* The layer should then concatenate the result of the two global pooling operations, to return a Tensor of shape `(batch_size, 2*channels)`\n",
    "* Your layer implementation should not make use of any other Keras layers or TensorFlow pooling functions. \n",
    "* Test your layer by instantiating it (create a `GlobalMaxAveragePooling2D` object) and calling it on a Tensor of shape `(2, 8, 6, 3)` filled with randomly sampled values. **(10 marks)**\n",
    "\n",
    "_Hint: the `tf.reduce_max`, `tf.reduce_mean` and `tf.concat` functions will be useful in defining the custom layer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2e26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe69691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabebb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4c0fde9",
   "metadata": {},
   "source": [
    "d) Use the Sequential API to implement a CNN classifier model for the CIFAR-10 dataset (as loaded above), using any number of `Conv2D` layers, one `GlobalMaxAveragePooling2D` layer and one `Dense` layer. Your `Conv2D` layers can use any hyperparameter settings. \n",
    "\n",
    "Your CNN model should have roughly the same number of trainable parameters as the MLP model above (within $\\pm 10\\%$). Print the model summary. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e46631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21096f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58250aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b80e46f4",
   "metadata": {},
   "source": [
    "### Question 2 (Total 30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e14a7e",
   "metadata": {},
   "source": [
    "This question uses the [CIFAR-10](https://keras.io/api/datasets/cifar10/) dataset, which you should load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(cifar10_x_train, cifar10_y_train), (cifar10_x_test, cifar10_y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2c4af",
   "metadata": {},
   "source": [
    "a) Create train and test `tf.data.Dataset` objects for the CIFAR-10 dataset, that return a tuple of input images and integer labels. \n",
    "\n",
    "You should then write the following functions to process the input images:\n",
    "\n",
    "* Write a function `rescale` that converts the images to type `tf.float32` and rescales the pixel values to lie in the range $[-1, 1]$\n",
    "* Write a function `grayscale` that converts the input images to grayscale by averaging the pixel values of the colour channels. The image Tensors should retain the channel axis, with length 1.\n",
    "* Write a function `random_flip` that randomly flips the input image horizontally, with probability $0.5$\n",
    "\n",
    "Apply each of the functions above in sequence to the train and test `tf.data.Dataset` objects using the `map` method and print out the `element_spec`. Do not batch or shuffle your Datasets. **(17 marks)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc58bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104c9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb029f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef840dc",
   "metadata": {},
   "source": [
    "b) Write a function `filter_classes` that you will use with the Datasets' `filter` method, to filter out examples with any of the class labels $[0, 1, 8, 9]$.\n",
    "\n",
    "Apply your function to the train and test Datasets with the `filter` method, and verify your function works correctly by iterating through the Datasets and confirming that the examples returned by the Datasets only contain the integer labels $[3, 4, 5, 6, 7]$. **(8 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1b307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887b73e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d5b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27bed0c4",
   "metadata": {},
   "source": [
    "c) Batch your Dataset objects with a batch size of 20 (but do not shuffle them). \n",
    "\n",
    "Draw one batch of examples from your training Dataset and display them along with their string label names. The label names are provided below. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727016f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These label names correspond with the integer labels loaded above\n",
    "\n",
    "class_names = [\n",
    "    'airplane', \n",
    "    'automobile', \n",
    "    'bird', \n",
    "    'cat', \n",
    "    'deer', \n",
    "    'dog', \n",
    "    'frog', \n",
    "    'horse', \n",
    "    'ship', \n",
    "    'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c3866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c457491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd43b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fcfd108",
   "metadata": {},
   "source": [
    "### Question 3 (Total 40 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5e098",
   "metadata": {},
   "source": [
    "This question uses the [MNIST](https://keras.io/api/datasets/mnist/) dataset, which you should load as follows (this question only uses the train split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71867ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(mnist_x_train, mnist_y_train), _ = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ed3f9",
   "metadata": {},
   "source": [
    "a) Create a `tf.data.Dataset` object from the train split arrays loaded above, that returns a tuple of input images and integer labels.\n",
    "\n",
    "* Process the input images by rescaling the pixel values to the range $[0, 1]$\n",
    "* Shuffle the Dataset with buffer size 1000\n",
    "* Batch the Dataset with batch size 64\n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a6a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834e09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1cf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9458fe28",
   "metadata": {},
   "source": [
    "b) Use the functional API to implement an MLP classification model for the MNIST dataset.\n",
    "\n",
    "Your model should first flatten the input image into a 1-D Tensor, and should have a single hidden layer with 100 neurons and a ReLU activation. The output layer is a softmax layer with 10 classes. Compile your model with a suitable loss function and the SGD optimizer. Print the model summary. **(7 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2c843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9cda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8901101a",
   "metadata": {},
   "source": [
    "c) A common method for finding a suitable learning rate for a given model and optimizer is to perform a dummy training run as a learning rate probe as follows:\n",
    "\n",
    "* Set the initial learning rate to a small value\n",
    "* Fix a number of iterations to train the model, `num_iterations` \n",
    "* After each iteration/batch, slightly increase the learning rate by multiplying by a fixed factor $q>1$\n",
    "* Repeat until the learning rate reaches a high value, recording the loss at each training iteration\n",
    "\n",
    "The aim is to obtain a plot that looks something like this:\n",
    "<img src=\"figures/example_loss_lr_curve.png\" alt=\"Life expectancy\" style=\"width: 350px;\"/>\n",
    "The loss decreases in the beginning, but eventually starts rising when it is too large. A good choice of learning rate is one slightly to the left of the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1b05b",
   "metadata": {},
   "source": [
    "Write a custom callback `LearningRateProbe` to implement the above scheme. \n",
    "\n",
    "* The initializer should take the arguments `num_iterations`, `initial_learning_rate` and `final_learning_rate`\n",
    "* The callback should set the optimizer learning rate to `initial_learning_rate` at the start of training, and then increase it by a factor of $q$ after each training iteration\n",
    "  * The factor $q$ should be calculated to satisfy:\n",
    "  \n",
    "  `initial_learning_rate * q**(num_iterations - 1) = final_learning_rate`\n",
    "  \n",
    "* After each training iteration, the callback should store the learning rate values and the loss values\n",
    "  \n",
    "_Hint: the optimizer learning rate is a non-trainable TensorFlow Variable stored at `model.optimizer.lr`, for a compiled model. This Variable should be updated after each training iteration as above._ **(18 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d256b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da772487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3c27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555a27b5",
   "metadata": {},
   "source": [
    "d) Use your `LearningRateProbe` callback to collect the loss values from training your (freshly initialized) model from part a) for a single epoch on the MNIST training data, with the batch size of 64.\n",
    "\n",
    "* Set `num_iterations` to 938 (number of batches of size 64 in a single epoch through a training set of 60,000 examples)\n",
    "* Set `initial_learning_rate` to $1e-8$\n",
    "* Set `final_learning_rate` to $10$\n",
    "* Train your model for 1 epoch, passing in your `LearningRateProbe` custom callback object\n",
    "* Plot the loss values against the learning rate, using a log scale for the x-axis\n",
    "* Calculate the learning rate that minimizes the loss curve, and suggest a good learning rate for this model, dataset and optimizer\n",
    "\n",
    "**(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11d38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0260bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec5f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dltf",
   "language": "python",
   "name": "dltf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
