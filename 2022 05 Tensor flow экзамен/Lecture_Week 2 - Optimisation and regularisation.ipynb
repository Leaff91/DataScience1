{"cells":[{"cell_type":"markdown","metadata":{"id":"61RbDEBBEBC9"},"source":["# Deep Learning with TensorFlow\n","### Week 2: Optimisation and regularisation"]},{"cell_type":"markdown","metadata":{"id":"5_ZumE8cEBDF"},"source":["## Contents\n","\n","[1. Introduction](#introduction)\n","\n","[2. Error backpropagation](#backprop)\n","\n","[3. Optimisers](#optimisers)\n","\n","[4. Automatic differentiation in TensorFlow (\\*)](#autodiff)\n","\n","[5. Weight regularisation, dropout and early stopping](#regularisation)\n","\n","[6. TensorFlow regularisers, Dropout layers, metrics and callbacks (\\*)](#tf_regularisation)\n","\n","[7. Batch normalisation (\\*)](#batchnorm)\n","\n","[References](#references)"]},{"cell_type":"markdown","metadata":{"id":"5IJ8tU9UEBDH"},"source":["<a class=\"anchor\" id=\"introduction\"></a>\n","## Introduction\n","\n","In the last week of the module we reviewed some important concepts in machine learning, including generalisation, validation, dataset splits, overfitting/underfitting and regularisation. and took a first look at the prototypical deep learning architecture, which is the multilayer perceptron.\n","\n","You also trained your first deep learning models in TensorFlow on the MNIST dataset, using the Sequential API, and learned the core methods `compile`, `fit`, `evaluate` and `predict`. You saw how the low level objects Tensors and Variables are including in these models to encapsulate mutable parameters and computational operations.\n","\n","In this week of the module we will focus on practical aspects of training deep learning models. We touched on methods of regularisation last week, and how they are important to combat overfitting. Here, we will look at three commonly used regularisation methods for deep learning models: $\\mathcal{l}^2$ (and $\\mathcal{l}^1$) regularisation, dropout and early stopping.\n","\n","We will then turn to the issue of optimising neural networks, and study the important backpropagation algorithm, with a focus on its application to MLP models. We will then walk through several popular optimisers that are used in practice. In this exposition, we will see how the vanishing gradients problem naturally occurs in the context of deep networks. We will then look at ways of mitigating this and stabilising information flow through the network using Glorot initialisation and batch normalisation.\n","\n","We will also learn how to implement all of these techniques in TensorFlow, and see how gradients can easily be computed using automatic differentiation tools. We will introduce callbacks and metrics, which are useful objects for monitoring your models during training and evaluation. "]},{"cell_type":"markdown","metadata":{"id":"f7N-XkVtEBDJ"},"source":["<a class=\"anchor\" id=\"backprop\"></a>\n","## Error backpropagation\n","\n","Gradient-based neural network optimisation can be seen as iterating over the following two main steps:\n","\n","1. Computation of the (stochastic) gradient of the loss function with respect to the model parameters\n","2. Use of the computed gradient to update the parameters\n","\n","We have already seen the parameter update rule according to stochastic gradient descent for a neural network model $f_\\theta:\\mathbb{R}^D\\mapsto Y$, where $Y$ is the target space (e.g. $\\mathbb{R}^{n_{L+1}}$ or $[0, 1]^{n_{L+1}}$):\n","\n","$$\n","\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_t L(\\theta_t; \\mathcal{D}_m),\\qquad t\\in\\mathbb{N}_0. \\label{sgd}\\tag{1}\n","$$\n","\n","In the above equation, the minibatch loss $L(\\theta_t; \\mathcal{D}_m)$ is calculated on a randomly drawn sample of data points from the training set,\n","\n","$$\n","L(\\theta_t; \\mathcal{D}_m) = \\frac{1}{M} \\sum_{x_i, y_i\\in\\mathcal{D}_m} l(y_i, f_{\\theta_t}(x_i)). \\label{minibatch_loss}\\tag{2}\n","$$\n","\n","where $\\mathcal{D}_m$ is the randomly sampled minibatch, $M = |\\mathcal{D}_m| \\ll |\\mathcal{D}_{train}|$ is the size of the minibatch, and we will denote $L_i:Y\\times Y\\mapsto \\mathbb{R}$, given by $L_i := l(y_i, f_\\theta(x_i))$, as the per-example loss. \n","\n","The update \\eqref{sgd} requires computation of the term $\\nabla_t L(\\theta_t; \\mathcal{D}_m)$ (from here we drop the $\\mathcal{D}_m$ in this expresesion for brevity); that is, the gradient of the loss function with respect to all of the model parameters, evaluated at the current parameter settings $\\theta_t$.\n","\n","The computation of the derivatives is done through applying the chain rule of differentiation. The algorithm for computing these derivatives in an efficient manner is known as **backpropagation**, and was popularised for use in neural network optimisation in [Rumelhart et al 1986b](#Rumelhart86b) and [Rumelhart et al 1986c](#Rumelhart86c), although the technique dates back earlier, see e.g. [Werbos](#Werbos94) which includes Paul Werbos' 1974 dissertation.\n","\n","In this section, we will derive the important backpropagation algorithm for finding the loss function derivatives for a multilayer perceptron.\n","\n","First recall the layer transformations in the MLP:\n","\n","\n","$$\n","\\begin{align}\n","\\mathbf{h}^{(0)} &:= \\mathbf{x}, \\label{fp1}\\tag{3}\\\\\n","\\mathbf{h}^{(k)} &= \\sigma\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, L,\\label{fp2}\\tag{4}\\\\\n","\\hat{\\mathbf{y}} &= \\sigma_{out}\\left( \\mathbf{w}^{(L)}\\mathbf{h}^{(L)} + b^{(L)} \\right),\\label{fp3} \\tag{5}\n","\\end{align}\n","$$\n","\n","where $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$, $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$, $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{n_k}$, $\\hat{\\mathbf{y}}\\in Y$, $\\sigma, \\sigma_{out}:\\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions that are applied element-wise, $n_0 := D$, and $n_k$ is the number of units in the $k$-th hidden layer. \n","\n","Also recall that we define the **pre-activations**\n","\n","$$\n","\\mathbf{a}^{(k)} = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\label{preactivations}\\tag{6}\n","$$\n","\n","and **post-activations**\n","\n","$$\n","\\mathbf{h}^{(k)} = \\sigma(\\mathbf{a}^{(k)}). \\label{activations}\\tag{7}\n","$$\n","\n","We will consider the gradient of the loss computed on a single data example $\\nabla_t {L}_i(\\theta_t)$, given the sum \\eqref{minibatch_loss}. We first compute the **forward pass** \\eqref{fp1}-\\eqref{fp3} and store the preactivations $\\mathbf{a}^{(k)}$ and post-activations $\\mathbf{h}^{(k)}$.\n","\n","<img src=\"figures/forward.png\" alt=\"Forward pass\" style=\"width: 800px;\"/>\n","<center>Pre-activations, post-activations, weights and biases in the forward pass</center>\n","\n","Consider the derivative of $L_i$ with respect to $w^{(k)}_{pq}$ and $b^{(k)}_p$. We have:\n","\n","$$\n","\\begin{align}\n","\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} &= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p} \\frac{\\partial a^{(k+1)}_p}{\\partial w^{(k)}_{pq}} \\\\\n","&= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p} h^{(k)}_q,\n","\\end{align}\n","$$\n","\n","where the second line follows from \\eqref{preactivations}. Similarly,\n","\n","$$\n","\\begin{align}\n","\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} &= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p} \\frac{\\partial a^{(k+1)}_p}{\\partial b^{(k)}_{p}} \\\\\n","&= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p}. \n","\\end{align}\n","$$\n","\n","We introduce the notation $\\delta^{(k)}_p := \\frac{\\partial L_i}{\\partial a^{(k)}_p}$, called the **error**. We then write\n","\n","$$\n","\\begin{align}\n","\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} &= \\delta^{(k+1)}_p h^{(k)}_q \\label{dldw_error}\\tag{8}\\\\\n","\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} &= \\delta^{(k+1)}_p. \\label{dldb_error}\\tag{9}\n","\\end{align}\n","$$\n","\n","We therefore need to compute the quantity $\\delta^{(k+1)}_p$ for each hidden and output unit in the network. Again using the chain rule, we have\n","\n","$$\n","\\begin{align}\n","\\delta^{(k)}_p \\equiv \\frac{\\partial L_i}{\\partial a^{(k)}_p} &= \\sum_{j=1}^{n_{k+1}} \\frac{\\partial L_i}{\\partial a^{(k+1)}_j} \\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p} \\\\\n","&= \\sum_{j=1}^{n_{k+1}} \\delta^{(k+1)}_j \\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p} \\label{recursive_error}\\tag{10}\n","\\end{align}\n","$$\n","\n","Combining \\eqref{preactivations} and \\eqref{activations} we see that\n","\n","$$\n","\\begin{align}\n","a^{(k+1)}_j &= \\sum_{l=1}^{n_k} w^{(k)}_{jl} \\sigma(a^{(k)}_l) + b^{(k)}_p \\label{activation_forward}\\tag{11}\\\\\n","\\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p} &= w^{(k)}_{jp} \\sigma'(a^{(k)}_p)\n","\\end{align}\n","$$\n","\n","where $\\sigma'$ is the derivative of the activation function. So from the above equation and \\eqref{recursive_error} we have\n","\n","$$\n","\\begin{align}\n","\\delta^{(k)}_p \\equiv \\frac{\\partial L_i}{\\partial a^{(k)}_p} &= \\sum_{j=1}^{n_{k+1}} \\delta^{(k+1)}_p \\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p}\\\\\n","&=  \\sigma'(a^{(k)}_p) \\sum_{j=1}^{n_{k+1}} w^{(k)}_{jp}  \\delta^{(k+1)}_p \\label{error_backprop}\\tag{12}\n","\\end{align}\n","$$\n","\n","Equation \\eqref{error_backprop} is analogous to \\eqref{activation_forward}, and describes the backpropagation of errors through the network. We can write it in the more concise form (analogous to \\eqref{activations}):\n","\n","$$\n","\\mathbf{\\delta}^{(k)} = \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)},\n","$$\n","\n","where $\\mathbf{\\sigma}'(\\mathbf{a}^{(k)}) = \\text{diag} ([\\mathbf{\\sigma}'(a^{(k)}_p)]_{p=1}^{n_k})$.\n","\n","<img src=\"figures/forward_backward_pass.png\" alt=\"Forward and backward passes\" style=\"width: 650px;\"/>\n","<center>Forward and backward passes</center>\n","\n","Now we can summarise the backpropagation algorithm as follows:\n","\n",">1. Propagate the signal forwards by passing an input vector $x_i$ through the network and computing all pre-activations and post-activations using $\\mathbf{a}^{(k)} = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)}$\n","> 2. Evaluate $\\mathbf{\\delta}^{(L+1)} = \\frac{\\partial L_i}{\\partial \\mathbf{a}^{(L+1)}}$ for the output neurons\n","> 3. Backpropagate the errors to compute $\\mathbf{\\delta}^{(k)}$ for each hidden unit using $\\mathbf{\\delta}^{(k)} = \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)}$\n","> 4. Obtain the derivatives of $L_i$ with respect to the weights and biases using $\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} = \\delta^{(k+1)}_p h^{(k)}_q,\\quad \n","\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} = \\delta^{(k+1)}_p$\n","\n","The backpropagation algorithm can easily be extended to apply to any directed acyclic graph, but we have presented it here in the case of MLPs for simplicity."]},{"cell_type":"markdown","metadata":{"id":"AeUxe_HPEBDQ"},"source":["<a class=\"anchor\" id=\"optimisers\"></a>\n","## Optimisers\n","\n","Recall the two main steps to training neural networks:\n","\n","1. Computation of the (stochastic) gradient of the loss function with respect to the model parameters\n","2. Use of the computed gradient to update the parameters\n","\n","Now that we have seen how gradients of the loss with respect to the parameters can be efficiently computed using the backpropagation algorithm (step 1), we will take a look at several popular gradient-based optimisation algorithms used in deep learning (step 2)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6POvA6juEBDR"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"DS8LiJ2KEBDT"},"source":["#### Stochastic gradient descent\n","We have already seen how stochastic gradient descent (SGD, [Robbins & Monro 1951](#Robbins51)) can be applied to optimise neural network parameters. \n","\n","Recall that SGD computes stochastic gradients by computing the loss on a minibatch of samples:\n","\n","$$\n","L(\\theta_t; \\mathcal{D}_m) = \\frac{1}{M} \\sum_{x_i, y_i\\in\\mathcal{D}_m} l(y_i, f_{\\theta_t}(x_i)),\n","$$\n","\n","where $\\mathcal{D}_m$ is a randomly sampled minibatch of training data points, $M = |\\mathcal{D}_m|$ is the size of the minibatch (typically much smaller than $|\\mathcal{D}_{train}|$). We then use the gradient $\\nabla\\tilde{L}(\\theta_t)$ to update the parameters according to the SGD update rule\n","\n","$$\n","\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta_t} L(\\theta_t; \\mathcal{D}_m),\\qquad t\\in\\mathbb{N}_0.\n","$$\n","\n","In TensorFlow, an SGD optimizer object can be instantiated from the `tf.keras.optimizers` module. \n","\n","Optimiser objects like this one can be passed directly into the `optimizer` keyword argument in `model.compile` instead of the string reference `'sgd'`. This is useful, for example if you want to change the learning rate default."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bm66_eKbEBDV"},"outputs":[],"source":["# Create an SGD optimiser\n","\n","sgd = tf.keras.optimizers.SGD()\n","\n","print(sgd.lr)  # Default learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRrdlQ9nEBDX"},"outputs":[],"source":["# Create a new sgd optimiser and change the learning rate \n","\n","sgd = tf.keras.optimizers.SGD(learning_rate=0.005)\n","\n","print(sgd.lr)"]},{"cell_type":"markdown","metadata":{"id":"Z9z1oIqeEBDY"},"source":["Stochastic gradient descent reduces redudancy in the gradient computation, and is faster than full (batch) gradient descent. However some challenges remain: \n","\n","* Convergence can still be very slow with SGD\n","* Setting the learning correctly can be difficult, involving trial and error\n","* Different weights might operate on different scales, and require different rates of learning\n","\n","Several optimisation algorithms have been proposed to help treat these problems.\n","\n","#### Momentum\n","\n","One common tweak to accelerate the slow convergence of SGD is to add momentum ([Qian 1999](#Qian99)):\n","\n","$$\n","\\begin{align}\n","\\mathbf{g}_t :=&~ \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m),\\\\\n","\\mathbf{v}_{t+1} =&~ \\beta \\mathbf{v}_t + \\eta\\mathbf{g}_t\\\\\n","\\theta_{t+1} =&~ \\theta_t - \\mathbf{v}_{t+1},\n","\\end{align}\n","$$\n","\n","where $\\beta\\ge0$ is the momentum term, and as before, $\\eta>0$ is the learning rate. When $\\beta=0$ then we recover plain SGD, but with $\\beta>0$ (a typical value is around 0.9), this gives the gradient a short term memory which often accelerates convergence.\n","\n","Momentum can be added when created an SDG optimizer using the `momentum` keyword argument. The default value is `0.0` (plain SGD)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FraJKB7-EBDZ"},"outputs":[],"source":["# Create an SGD optimiser with momentum\n","\n","sgd_with_momentum = tf.keras.optimizers.SGD(momentum=0.9)\n","\n","print(sgd_with_momentum.lr)\n","print(sgd_with_momentum.momentum)"]},{"cell_type":"markdown","metadata":{"id":"-FaX6688EBDZ"},"source":["#### Nesterov momentum\n","\n","A common variant of momentum is to use Nesterov momentum ([Nesterov 1983](#Nesterov83)), which computes the gradient correction after the accumulated gradient, instead of before:\n","\n","$$\n","\\begin{align}\n","\\mathbf{g}_t &= \\nabla_\\theta L(\\theta_t - \\beta\\mathbf{v}_t; \\mathcal{D}_m),\\\\\n","\\mathbf{v}_{t+1} &= \\beta \\mathbf{v}_t + \\eta\\mathbf{g}_t\\\\\n","\\theta_{t+1} &= \\theta_t - \\mathbf{v}_{t+1},\n","\\end{align}\n","$$\n","\n","The accumulated gradient approximates the next value of the parameters, and so by evaluating the gradient $\\nabla_\\theta\\tilde{L}(\\theta_t - \\beta\\mathbf{v}_t )$, this gives the optimiser a sense of 'look-ahead'.\n","\n","<img src=\"figures/nesterov_momentum.png\" alt=\"Nesterov momentum\" style=\"width: 550px;\"/>\n","\n","Nesterov momentum can be added to an SGD optimizer using the `nesterov` keyword argument. The default value is `False`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSdTHiInEBDa"},"outputs":[],"source":["# Create an SGD optimiser with momentum\n","\n","sgd_with_momentum = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n","\n","print(sgd_with_momentum.momentum)\n","print(sgd_with_momentum.nesterov)"]},{"cell_type":"markdown","metadata":{"id":"j-xm6Y_9EBDb"},"source":["#### Adagrad\n","\n","The Adagrad optimiser ([Duchi et al 2011](#Duchi11)) adapts the learning rate for each parameter, to account for different weights learning on different scales. Parameters that receive a gradient less frequently have larger updates, making Adagrad well suited to sparse data, where most of the features are zero in the data. It is used, for example, in [Pennington et al 2014](#Pennington14) to train GloVe word embedding vectors.\n","\n","The update rule is\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m),\n","$$\n","\n","where $G_t\\in\\mathbb{R}^{p\\times p}$ is a diagonal matrix where the diagonal element $(G_t)_{ii}$ is the sum of squares of gradients with respect to $\\theta_i$ up to time step $t$. In the above, the division and square root operations are performed element-wise, and $\\odot$ is the Hadamard product.\n","\n","Note that the resulting learning rates per parameter are monotonically decreasing, and eventually the algorithm effectively stops learning.\n","\n","The Adagrad optimiser can be instantiated as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArPDYVxIEBDc"},"outputs":[],"source":["# Create an Adagrad optimiser\n","\n","adagrad = tf.keras.optimizers.Adagrad(\n","    learning_rate=0.001, epsilon=1e-07,\n",")"]},{"cell_type":"markdown","metadata":{"id":"LS6eJblkEBDd"},"source":["In the above, all keyword arguments shown are the default settings.\n","\n","#### RMSprop\n","\n","RMSprop is an unpublished optimisation method that aims to resolve the vanishing learning rates of Adagrad (it appeared in Geoff Hinton's Coursera course [in lecture 6e](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)). It uses a decaying average of past squared gradients. \n","\n","The update rule is\n","\n","$$\n","\\begin{align}\n","\\mathbb{E}[\\mathbf{g}^2]_t &= \\rho \\mathbb{E}[\\mathbf{g}^2]_{t-1} + (1 - \\rho)(\\nabla_\\theta L(\\theta_t; \\mathcal{D}_m))^2\\\\\n","\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\mathbb{E}[\\mathbf{g}^2]_t + \\epsilon}} \\odot \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m)\n","\\end{align}\n","$$\n","\n","As before, the division and square root are performed element-wise, and $\\odot$ is the Hadamard product. The $\\rho$ term is typically set similar to momentum, around 0.9.\n","\n","The RMSprop can be instantiated as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q99vIYVrEBDd"},"outputs":[],"source":["# Create an RMSprop optimiser\n","\n","rmsprop = tf.keras.optimizers.RMSprop(\n","    learning_rate=0.001, rho=0.9, epsilon=1e-07\n",")"]},{"cell_type":"markdown","metadata":{"id":"2MCf6b7iEBDe"},"source":["Again the above are the default settings. Momentum can also be added to the `RMSprop` optimiser with the `momentum` keyword argument:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4szBLi0WEBDe"},"outputs":[],"source":["# Create an RMSprop optimiser with momentum\n","\n","rmsprop_with_momentum = tf.keras.optimizers.RMSprop(momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"Fl6j5D4sEBDf"},"source":["The RMSprop optimiser is also the default optimiser that is chosen if `model.compile` is called without the `optimizer` keyword argument.\n","\n","#### Adam\n","\n","The Adam optimiser ([Kingma 2015](#Kingma15)) is a popular optimisation algorithm, that also computes adaptive learning rates per parameter. It estimates first and second moments of the gradients, and the name stands for <ins>Ada</ins>ptive <ins>m</ins>oment estimation.\n","\n","The update rule is\n","\n","$$\n","\\begin{align}\n","\\mathbb{E}[\\mathbf{g}]_t &= \\beta_1\\mathbb{E}[\\mathbf{g}]_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m),\\\\\n","\\mathbb{E}[\\mathbf{g}^2]_t &= \\beta_2\\mathbb{E}[\\mathbf{g}^2]_{t-1} + (1 - \\beta_2) (\\nabla_\\theta L(\\theta_t; \\mathcal{D}_m))^2,\\\\\n","\\mathbf{m}_t &= \\mathbb{E}[\\mathbf{g}]_t / (1 - \\beta_1),\\\\\n","\\mathbf{v}_t &= \\mathbb{E}[\\mathbf{g}^2]_t / (1 - \\beta_2),\\\\\n","\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\mathbf{v}_t + \\epsilon}}\\odot \\mathbf{m}_t\n","\\end{align}\n","$$\n","\n","The $\\mathbf{m}_t$ and $\\mathbf{v}_t$ terms correct for an initial bias towards zero. Typical values are $\\beta_1 \\approx 0.9$, $\\beta_2 \\approx 0.999$ and $\\epsilon \\approx 10^{-7}$.\n","\n","The Adam optimiser can be instantiated as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6erK83NEBDf"},"outputs":[],"source":["# Create an Adam optimiser\n","\n","adam = tf.keras.optimizers.Adam()\n","\n","print(adam.lr)\n","print(adam.beta_1)\n","print(adam.beta_2)\n","print(adam.epsilon)"]},{"cell_type":"markdown","metadata":{"id":"3ADV76AjEBDf"},"source":["The above is a non-exhaustive list of optimisers that have been developed and are actively used in deep learning research and practice. See [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) for a complete of the optimisers that are available to use in Tensorflow.\n","\n","The code below will create a demonstration optimization run using one of each of the optimizers described above for the [Beale function](http://benchmarkfcns.xyz/benchmarkfcns/bealefcn.html), a common test function used to evaluate optimization algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9M4HhPjcEBDg"},"outputs":[],"source":["def beale(x, y):\n","    return (1.5 - x + x * y)**2 + (2.25 - x + x * (y**2))**2 + (2.625 - x + x * (y**3))**2\n","\n","def grad_beale(x, y):\n","    ddx = 2*(1.5 - x + x * y)*(y - 1) + 2*(2.25 - x + x * (y**2))*((y**2) - 1) + 2*(2.625 - x + x * (y**3))*((y**3)-1)\n","    ddy = 2*(1.5 - x + x * y)*(x) + 2*(2.25 - x + x * (y**2))*(2*y*x) + 2*(2.625 - x + x * (y**3))*(3*(y**2)*x)\n","    return [ddx, ddy]"]},{"cell_type":"markdown","metadata":{"id":"cP2lYWqSEBDg"},"source":["In this case the gradient is easy enough to calculate by hand as above. Later in this week you will learn how to use the [automatic differentiation](#autodiff) tools in TensorFlow to compute the gradient of any differentiable function for you.\n","\n","The cell below will run the optimization routine for 100 iterations for each optimizer, and plot the trajectories over the contour plot of the Beale function. Feel free to interrupt the cell execution and restart it to try a different random initial condition. You can also try changing the learning rates and other parameters to see the effect on convergence."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"rnxgCqIgEBDh"},"outputs":[],"source":["from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","x_init = tf.random.normal(())\n","y_init = tf.random.normal(())\n","\n","test_fn = beale\n","grad_fn = grad_beale\n","\n","X, Y = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n","Z = test_fn(X, Y)\n","levels = np.exp(np.linspace(0, 10, 25)) - 1\n","plt.figure(figsize=(13, 8))\n","plt.contour(X, Y, Z, levels, alpha=0.6, cmap='viridis')\n","plt.colorbar()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.scatter(x_init.numpy(), y_init.numpy())\n","plt.scatter(3, 0.5, marker='*', label='Optimum')\n","optimizers_config = [\n","    {\"name\": \"SGD\", \"kwargs\": {\"learning_rate\": 0.01}, \"label\": \"SGD\"},\n","    {\"name\": \"SGD\", \"kwargs\": {\"learning_rate\": 0.001, \"momentum\": 0.9, \"nesterov\": True}, \"label\": \"SGD-NAG\"},\n","    {\"name\": \"Adam\", \"kwargs\": {\"learning_rate\": 0.1}, \"label\": \"Adam\"},\n","    {\"name\": \"Adagrad\", \"kwargs\": {\"learning_rate\": 0.1}, \"label\": \"Adagrad\"},\n","    {\"name\": \"RMSprop\", \"kwargs\": {\"learning_rate\": 0.05}, \"label\": \"RMSprop\"}\n","]\n","optimizers, states, plot_data = [], [], []\n","for optimizer in optimizers_config:\n","    optimizers.append(getattr(tf.keras.optimizers, optimizer['name'])(**optimizer['kwargs']))\n","    states.append((tf.Variable(x_init, name='x_{}'.format(optimizer['name'])), \n","                   tf.Variable(y_init, name='y_{}'.format(optimizer['name']))))\n","    opt_plot, = plt.plot([x_init.numpy()], [y_init.numpy()], label=optimizer['label'])\n","    plot_data.append(opt_plot)\n","plt.title(\"Test optimization run with {} optimizers. Initial conditions x: {:.4f}, y: {:.4f}\".format(\n","    len(optimizers), x_init.numpy(), y_init.numpy()), fontsize=14)\n","\n","num_iterations = 100\n","for i in range(num_iterations):\n","    try:\n","        for optimizer, state, data in zip(optimizers, states, plot_data):\n","            grads = grad_fn(state[0], state[1])\n","            optimizer.apply_gradients(zip(grads, state))\n","            data.set_xdata(np.append(data.get_xdata(), state[0].numpy()))\n","            data.set_ydata(np.append(data.get_ydata(), state[1].numpy()))\n","        plt.text(0.01, 0.01, 'Iteration {}'.format(i+1), horizontalalignment='left', \n","                 verticalalignment='bottom', transform = plt.gca().transAxes, \n","                 fontsize=12, bbox=dict(facecolor='white', alpha=1.))\n","        plt.legend(fontsize=12)\n","        display.display(plt.gcf())\n","        display.clear_output(wait=True)\n","    except KeyboardInterrupt:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"lVMJgtlVEBDh"},"source":["There is no single answer for which optimiser is best to use, as it very often depends on the data and the model. Optimisers with adaptive learning rates tend to convergence faster in most situations, and are more frequently chosen than plain SGD in most situations. However, it is interesting to note that SGD has been shown to generalise better ([Hardt et al 2015](#Hardt15)), and techniques such as switching from Adam to SGD during training have been proposed ([Keskar & Socher 2017](#Keskar17)). Further methods include annealed learning rates, cyclic learning rates ([Smith 2015](#Smith15)) and decaying momentum ([Chen & Kyrillidis 2019](#Chen19)). Many advances have been made in recent years, and neural network optimisation is still very much an active research area.\n","\n","*Exercise.* Change the Beale function above for a different [test function](https://en.wikipedia.org/wiki/Test_functions_for_optimization) in the above code."]},{"cell_type":"markdown","metadata":{"id":"mWJA8-6dEBDh"},"source":["<a class=\"anchor\" id=\"autodiff\"></a>\n","## Automatic differentiation in TensorFlow\n","\n","One of the major advantages of using deep learning frameworks like TensorFlow is the ability to automatically compute gradients of any differentiable operation. When using the Keras `model.fit` API, TensorFlow applies the backpropagation equations automatically to compute gradients, and then uses the optimiser algorithm selected to update the parameters.\n","\n","In this section, we will see how lower-level tools in TensorFlow can be leveraged to compute gradients of differentiable expressions, and build a custom training loop that breaks down the training loop to give you extra flexibility when you need it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pB83BtVBEBDi"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"yd-ZTsqqEBDi"},"source":["Operations that you want to take gradients with respect to need to be defined inside a `tf.GradientTape` context:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xacro1SxEBDi"},"outputs":[],"source":["# Define a simple operation and take the gradient\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhKqapTEEBDj"},"outputs":[],"source":["# Take multiple derivatives\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aleQC6aEBDj"},"outputs":[],"source":["# Gradients can also be computed with respect to intermediate variables\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9-a4RcvEBDj"},"outputs":[],"source":["# Gradients can be taken once by default\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L248r0L3EBDk"},"outputs":[],"source":["# Variable objects are tracked automatically\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DzWdTNoEBDk"},"outputs":[],"source":["# Take gradients with respect to a layer operation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EehG8QpnEBDk"},"outputs":[],"source":["# Take gradients with respect to a model operation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8NN13BbWEBDl"},"source":["*Exercise.* Use `tf.GradientTape` to make a plot of the function $\\frac{dy}{dx}:[-5, 5]\\mapsto\\mathbb{R}$, where $y = \\sin (x^2) - \\frac{x^2}{4}$.\n","\n","#### Custom training loop\n","We will now see how to build a custom training loop using `tf.GradientTape` and optimiser objects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzsmIr3iEBDl"},"outputs":[],"source":["# Load the Fashion-MNIST dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2pZCoibEBDl"},"outputs":[],"source":["# Get the class labels\n","\n","classes = [\n","    \"T-shirt/top\",\n","    \"Trouser\",\n","    \"Pullover\",\n","    \"Dress\",\n","    \"Coat\",\n","    \"Sandal\",\n","    \"Shirt\",\n","    \"Sneaker\",\n","    \"Bag\",\n","    \"Ankle boot\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbvWLdqdEBDm"},"outputs":[],"source":["# View a few training data examples\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","n_rows, n_cols = 3, 5\n","random_inx = np.random.choice(x_train.shape[0], n_rows * n_cols, replace=False)\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n","fig.subplots_adjust(hspace=0.2, wspace=0.1)\n","\n","for n, i in enumerate(random_inx):\n","    row = n // n_cols\n","    col = n % n_cols\n","    axes[row, col].imshow(x_train[i])\n","    axes[row, col].get_xaxis().set_visible(False)\n","    axes[row, col].get_yaxis().set_visible(False)\n","    axes[row, col].text(10., -1.5, f'{classes[y_train[i]]}')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZltq9KKEBDm"},"outputs":[],"source":["# Build the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-2LMAhsEBDm"},"outputs":[],"source":["# Print the model summary\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLTtVdfEEBDn"},"outputs":[],"source":["# Define an optimiser\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x66OArBEBDn"},"outputs":[],"source":["# Define the loss function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k-k1LcJ2EBDn"},"source":["We will also use the `tf.data` module to load the training data into a `tf.data.Dataset` object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ap6iKHu8EBDn"},"outputs":[],"source":["# Load the data into a tf.data.Dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-exkOtoEBDo"},"outputs":[],"source":["# Iterate over the Dataset object\n","\n"]},{"cell_type":"markdown","metadata":{"id":"czTENtItEBDo"},"source":["`Dataset` objects come with `map` and `filter` methods for data preprocessing on the fly. For example, we can normalise the pixel values to the range $[0, 1]$ with the `map` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BTOykXOEBDo"},"outputs":[],"source":["# Normalise the pixel values\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mh3RRKQoEBDo"},"source":["We could also filter out data examples according to some criterion with the `filter` method. For example, if we wanted to exclude all data examples with label $9$ from the training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPtQPh1mEBDp"},"outputs":[],"source":["# Filter out all examples with label 9 (ankle boot)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTx42bc2EBDp"},"outputs":[],"source":["# Shuffle the dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WamlcBuiEBDp"},"outputs":[],"source":["# Batch the dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4E1CGbnEBDq"},"outputs":[],"source":["# Print the element_spec\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Km-c07eaEBDq"},"source":["We now have everything to write the custom training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Mj18PptAEBDq"},"outputs":[],"source":["# Build the custom training loop\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzOkVW5QEBDr"},"outputs":[],"source":["# Build a new model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCwj0kTuEBDr"},"outputs":[],"source":["# Optimise the custom training loop by compiling the training step into a graph\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zZ_g0LQoEBDs"},"source":["In many cases the data processing pipeline can also be optimised for performance gain, see [here](https://www.tensorflow.org/guide/data_performance) for more information."]},{"cell_type":"markdown","metadata":{"id":"-xgCKdkUEBDs"},"source":["<a class=\"anchor\" id=\"regularisation\"></a>\n","## Weight regularisation, dropout and early stopping\n","\n","Deep learning models are typically very over-parameterised, often with millions of parameters over many layers in the model. They are universal approximators (see e.g. [Cybenko](#Cybenko89) for the large width case, or [Lu et al](#Lu17) for the large depth case), and so overfitting can be a problem. When training neural networks, it is important to regularise them to prevent overfitting. As written above, there are several forms of regularisation, but in this section we will look at three in particular: weight regularisation, dropout and early stopping.\n","\n","#### $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation\n","Recall that for a linear model of the form\n","\n","$$\n","f(\\mathbf{x}) = \\sum_j w_j \\phi_j(\\mathbf{x}),\n","$$\n","\n","a typical regularisation is to add a sum of squares penalty term to the loss term to discourage the weights $w_j$ from growing too large. In this case, the regularised loss takes the form\n","\n","\n","$$\n","L(\\mathbf{w}, \\alpha) = L_0(\\mathbf{w}) + \\alpha_2 \\sum_i w_i^2,\n","$$\n","\n","where $L_0$ is the unconstrained loss function, and $\\alpha_2$ is a regularisation hyperparameter. This is $\\mathcal{l}^2$ regularisation.\n","\n","This form of regularisation is often referred to as **weight decay**, although the two terms are technically not the same. Weight decay ([Hanson & Pratt](#Hanson88)) is defined as a modification to the update rule, rather than to the loss function itself:\n","\n","$$\n","\\mathbf{\\theta}_{t+1} \\leftarrow (1 - \\lambda)\\theta_t - \\eta g_t,\n","$$\n","\n","where $\\theta\\in\\mathbb{R}^p$ is the model parameters, $\\lambda$, $\\eta$ are hyperparameters, and $g_t$ is the $t$-th batch update. In the case of stochastic gradient descent, the update $g_t = \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m)$ and the two formulations are equivalent. However, this is not the case for all gradient-based optimisers commonly used in deep learning.\n","\n","An alternative weight regularisation is $\\mathcal{l}^1$ regularisation, in which the sum of absolute values of the weights are added to the loss term:\n","\n","$$\n","L(\\mathbf{w}, \\alpha) = L_0(\\mathbf{w}) + \\alpha_1 \\sum_i |w_i|.\n","$$\n","\n","This form of regularisation encourages sparsity in the weights. Both $\\mathcal{l}^1$ and $\\mathcal{l}^2$ regularisation discourage the weights from growing too large, which restricts the capacity of the network.\n","\n","It is also possible to add a weighted combination of both $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation to the loss function."]},{"cell_type":"markdown","metadata":{"id":"IplE_M_REBDs"},"source":["#### Dropout\n","Dropout was introduced by [Srivastava et al](#Srivastava14) in 2014 as a regularisation technique for neural networks, that also has the effect of modifying the behaviour of neurons within a network.\n","\n","The following is taken from the paper abstract:\n","\n","> Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods.\n","\n","The method of dropout is to randomly 'zero out' neurons (or equivalently, weight connections) in the network during training according to a Bernoulli mask whose values are independently sampled at every iteration. \n","\n","Suppose $\\mathbf{W}^k\\in\\mathbb{R}^{n_{k+1}\\times n_{k}}$ is a weight matrix mapping neurons in layer $k$ to layer $k+1$:\n","\n","$$\n","\\mathbf{h}^{(k+1)} = \\sigma\\left( \\mathbf{W}^{(k)}\\mathbf{h}^{(k)} + \\mathbf{b}^{(k)} \\right)\n","$$\n","\n","We can view dropout as randomly replacing each column of $\\mathbf{W}^k$ with zeros with probability $p_k$. We can write this as applying a Bernoulli mask:\n","\n","$$\n","\\begin{align}\n","\\mathbf{W}^k &\\leftarrow \\mathbf{W}^k \\cdot\\text{diag} ([\\mathbf{z}_{k, j}]_{j=1}^{n_{k-1}})\\\\\n","\\mathbf{z}_{k, j} &\\sim \\text{Bernoulli}(p_k), \\qquad k=1,\\ldots, L,\n","\\end{align}\n","$$\n","\n","with $\\mathbf{W}^k\\in\\mathbb{R}^{n_{k+1}\\times n_{k}}$. The following diagrams illustrate the effect of dropout on a neural network.\n","\n","<img src=\"figures/no_dropout.png\" alt=\"MLP with a two hidden layers\" style=\"width: 700px;\"/>\n","<center>Neural network without dropout</center>\n","\n","<img src=\"figures/dropout1.png\" alt=\"MLP with a two hidden layers\" style=\"width: 700px;\"/>\n","<center>Neural network with dropout</center>\n","\n","By randomly dropping out neurons in the network, one obvious effect is that the capacity of the model is reduced, and so there is a regularisation effect. Each randomly sampled Bernoulli mask defines a new 'sub-network' that is smaller than the original. \n","\n","In addition, a key motivation of dropout is that it prevents neurons from co-adapting too much. Any neuron in the network is no longer able to depend on any other specific neurons being present, and so each neuron learns features that are more robust, and generalise better.\n","\n","In the figure below (taken from the [original paper](#Srivastava14)) we see features that are learned on the MNIST dataset for a model trained without dropout (left) and one trained with dropout (right). We see that the dropout model learns features that are much less noisy and more meaningful (it is detecting edges, textures, spots etc.) and help the model to generalise better. The non-dropout model's features suggest a large degree of co-adaptation, where the neurons depend on the specific combination of features in order to make good predictions on the training data.\n","\n","<img src=\"figures/dropout_no_dropout.png\" alt=\"Features with and without dropout\" style=\"width: 700px;\"/>\n","<center>Learned features in a neural network trained without dropout (left) and with dropout (right). From Srivastava et al 2014</center>\n","\n","Typically, dropout is applied only in the training phase. When making predictions, all weight connections  $\\mathbf{W}^k$ are restored, but rescaled by a factor of $p_k$ to take account for the fact that fewer connections were present at training.\n","\n","However, [Gal & Ghahramani](#Gal16) describe a Bayesian interpretation of dropout, and proposed that dropout is also applied at test time in order to obtain a Bayesian predictive distribution."]},{"cell_type":"markdown","metadata":{"id":"KeaGp5yYEBDt"},"source":["#### Early stopping\n","\n","You might have found in the last week that it is difficult to set a good number of epochs to train for ahead of time. In the simple MNIST example training is quick so it is not a problem to experiment, but in many cases training could take hours or days (or even longer!) and so this is not an option. \n","\n","Recall that deep learning models are usually vastly overparameterised, and have the capacity to drastically overfit. A simple but effective method is to simply stop the training before the model starts to overfit. The picture is similar to the balance between capacity and generalisation:\n","\n","<img src=\"figures/early_stopping.png\" alt=\"Early stopping\" style=\"width: 500px;\"/>\n","<center>Prediction error vs number of training epochs</center>\n","\n","With early stopping, the aim is to stop the training when the validation error is at a minimum. This means that the model needs to be regularly evaluated on a held-out validation set (that is not used for training), and the optimisation routine is terminated when the validation error starts to rise. Validation is normally performed once per epoch in the training run.\n","\n","In practice, the validation error measurements will be noisy, and so it is not a reliable measure to simply detect when the validation error increases and immediately stop the training. What is usually done is to periodically save model checkpoints (say once per epoch), and set a **patience** threshold, to specify a maximum number of validation runs that are allowed where the validation error does not improve upon the best score so far. If this patience threshold is reached, the training is terminated.\n","\n","The early stopping algorithm is outlined in pseudocode below."]},{"cell_type":"markdown","metadata":{"id":"dJo3XDEFEBDu"},"source":["Early stopping inputs: `val_metric`, `max_patience`\n","\n","-------\n",">```\n",">best_valid_loss = np.inf\n",">patience = 0\n",">\n",">for epoch in range(max_epochs):\n",">    epoch_train_loss = train_model(train_data, train_loss)\n",">    epoch_valid_loss = validate_model(valid_data, val_metric)\n",">    if epoch_valid_loss < best_valid_loss:\n",">        best_valid_loss = epoch_valid_loss\n",">        patience = 0\n",">    else:\n",">        patience += 1\n",">        \n",">    save_model(epoch)\n",">    \n",">    if patience >= max_patience:\n",">        break  # terminate training\n",">```\n","\n","-------"]},{"cell_type":"markdown","metadata":{"id":"6Y0sU5vqEBDu"},"source":["It is also possible to validate the model using a measure that is different to the loss function used for training the model. Therefore `val_metric` is also an input to the early stopping algorithm above."]},{"cell_type":"markdown","metadata":{"id":"rpRRtqMQEBDu"},"source":["Of course, all of the regularisation techniques mentioned here (and more) can be used together in deep learning models (and they often are)."]},{"cell_type":"markdown","metadata":{"id":"S2NJMpiOEBDu"},"source":["<a class=\"anchor\" id=\"tf_regularisation\"></a>\n","## TensorFlow regularisers, Dropout layers and callbacks\n","\n","In this section we will build on what we have covered already with the `Sequential` API, and include weight regularisers, `Dropout` layers, and introduce callback objects - these are very useful objects for dynamically performing operations during the training run. An example is the `EarlyStopping` callback."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F48hdickEBDu"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"7OB0AbypEBDv"},"source":["For this tutorial we will use the diabetes dataset from `sklearn`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JecsH1-6EBDv"},"outputs":[],"source":["# Load the diabetes dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PhHgKiAEBDv"},"outputs":[],"source":["# Print dataset description\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89y3M0DgEBDw"},"outputs":[],"source":["# Get the input and target data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eD1IwMcMEBDw"},"outputs":[],"source":["# Normalise the target data (this will make clearer training curves)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELL-tdu_EBDw"},"outputs":[],"source":["# Partition the data into training and validation sets\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"po6O0uVmEBDw"},"outputs":[],"source":["# Load the data into training and validation Dataset objects\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-PM098fEBDx"},"outputs":[],"source":["# Build the MLP model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzcJFk7bEBDx"},"outputs":[],"source":["# Print the model summary\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOj1pUV-EBDx"},"outputs":[],"source":["# Compile the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-2oAcQmEBDx"},"outputs":[],"source":["# Train the model, including validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"-tgJ8_VTEBDx"},"outputs":[],"source":["# Plot the training and validation loss\n","\n","import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Loss vs. epochs')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8k7pGhOhEBDy"},"source":["#### Regularise the model\n","\n","Both $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation can easily be included using the `kernel_regularizer` and `bias_regularizer` keyword arguments in the `Dense` layer.\n","\n","Dropout can also be easily included as an additional layer of our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8QF3mouEBDy"},"outputs":[],"source":["# Redefine the model using l2 regularisation and dropout\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZhkYTMBEBDy"},"outputs":[],"source":["# Compile the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYymcOGjEBDy"},"outputs":[],"source":["# Train the model, including validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"zzbKzIe2EBDz"},"outputs":[],"source":["# Plot the training and validation loss\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Loss vs. epochs')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uGEihnhHEBDz"},"source":["The $\\mathcal{l}^2$ regularisation and dropout have helped to reduce the overfitting of the model. \n","\n","\n","#### Callbacks\n","We can go one step further and introduce early stopping as well, and save the model weights at the best validation score. We can do this with callbacks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTVWIJipEBDz"},"outputs":[],"source":["# Create a new model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQ6T4e_IEBDz"},"outputs":[],"source":["# Compile the model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y7EU2vLbEBD0"},"source":["The `EarlyStopping` callback is a built-in callback in the `tf.keras.callbacks` module. You can see a complete list of built-in callbacks [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLKyJdZ4EBD0"},"outputs":[],"source":["# Create an EarlyStopping callback\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yciutEDfEBD0"},"outputs":[],"source":["# Train the model, including validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpf8H6YbEBD1"},"outputs":[],"source":["# Plot the training and validation metrics\n","\n","import numpy as np\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Loss vs. epochs')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.xticks(np.arange(len(history.history['loss'])))\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"W4pX2JKmEBD1"},"source":["#### Custom callbacks\n","\n","It is also possible (and often useful) to create custom callbacks to perform certain actions depending on the training progress. We will look at building a custom callback to save the model weights, dependent on the performance of a specified validation metric.\n","\n","Note that the `tf.keras.callbacks` module has the built-in callback `ModelCheckpoint`, which automatically handles model saving for Keras models. We will look at a slightly lower-level method for model saving, using the `tf.train` module."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3503K_REBD1"},"outputs":[],"source":["# Create a custom callback for saving the model weights\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXENtatYEBD2"},"outputs":[],"source":["# Create a new model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSHsisYeEBD2"},"outputs":[],"source":["# Compile the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93-rzWv9EBD2"},"outputs":[],"source":["# Train the model, including validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RXZOEkIEBD2"},"outputs":[],"source":["# Inspect the saved checkpoints\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"pesJQbleEBD3"},"outputs":[],"source":["# Plot the training and validation metrics\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.plot(history.history['val_mae'])\n","plt.title('Loss vs. epochs')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.xticks(np.arange(len(history.history['loss'])))\n","plt.legend(['Training', 'Val loss', 'Val MAE'], loc='upper right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUSiw5PWEBD3"},"outputs":[],"source":["# Re-initialise the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eR0mxeulEBD3"},"outputs":[],"source":["# Restore the best model weights\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CPYe181EBD3"},"outputs":[],"source":["# Clean up\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nxsmLQbCEBD4"},"source":["<a class=\"anchor\" id=\"batchnorm\"></a>\n","## Batch normalisation\n","\n","Batch normalisation ([Ioffe & Szegedy 2015](#Ioffe15)) is a widely used method in deep learning. It is used to normalise the distribution of internal activation values in the network, and greatly helps to stabilise learning especially in deep networks. \n","\n","The core issue is that of **covariate shift**, which is the change in distribution of input variables to a machine learning model. This can happen in datasets where there is some change in conditions in subsequent data collections, for example over time or location. Whilst the underlying target function might not have changed, the distribution of the input variables does change which means the model could perform poorly in the changed conditions.\n","\n","The following shows a simple example of a regression function that fails to generalise to new data points whose distribution has shifted from the training data, even though the underlying target function is the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_703Y_WEBD4"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.kernel_ridge import KernelRidge\n","\n","def target(x):\n","    return x**3 - 15 * x - 12\n","\n","n_samples = 100\n","x_train_all = np.linspace(-5, 5, n_samples)[..., np.newaxis]\n","x_train_sub = x_train_all[30:]\n","y_train_all = target(x_train_all) + 10 * np.random.randn(n_samples, 1)\n","y_train_sub = y_train_all[30:]\n","\n","kernel_regressor_sub = KernelRidge(alpha=1e-2, kernel='rbf', gamma=0.5)\n","kernel_regressor_sub.fit(x_train_sub, y_train_sub)\n","mse1 = np.mean((kernel_regressor_sub.predict(x_train_sub) - y_train_sub)**2)\n","\n","kernel_regressor_all = KernelRidge(alpha=1e-2, kernel='rbf', gamma=0.5)\n","kernel_regressor_all.fit(x_train_all, y_train_all)\n","\n","fig = plt.figure(figsize=(14, 5))\n","\n","fig.add_subplot(1, 2, 1)\n","plt.plot(x_train_sub, target(x_train_sub), '--')\n","plt.plot(x_train_all, kernel_regressor_sub.predict(x_train_all))\n","plt.scatter(x_train_sub, y_train_sub, alpha=0.5)\n","plt.title(\"Regression function and training data\")\n","plt.legend(['Target function', 'Kernel regressor'])\n","\n","fig.add_subplot(1, 2, 2)\n","plt.plot(x_train_all, target(x_train_all), '--')\n","plt.plot(x_train_all, kernel_regressor_sub.predict(x_train_all))\n","plt.scatter(x_train_all, y_train_all, alpha=0.5)\n","plt.title(\"Same regression function on shifted data\")\n","plt.legend(['Target function', 'Kernel regressor'])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Rn1_O2iHEBD4"},"source":["The same phenomenon can occur during the course of training deep learning models on large datasets, where stochastic minibatches are used in the optimisation procedure. Furthermore, since deep learning models can be viewed as hierarchical feature extractors, we can encounter problems of **internal covariate shift**, where the activation values in hidden layers also undergo changes of distribution due to changes in parameter values and activations in earlier layers. \n","\n","<img src=\"figures/internal_covariate_shift.png\" alt=\"Internal Covariate Shift\" style=\"width: 750px;\"/>\n","<center>Changes in weights and activations earlier in the network cause internal covariate shift in activations in later layers</center>\n","\n","Batch normalisation reduces the internal covariate shift by normalising the mean and variance of the activation values in a layer. Intuitively speaking, this means that although layer inputs will change over the course of training, they won't change so much that learning becomes very slow or unstable. Batch normalisation also has a slight regularisation effect on the network.\n","\n","For a layer with $n_k$-dimensional input $\\mathbf{h}^{(k)} = (h^{(k)}_1,\\ldots,h^{(k)}_{n_k})$, we normalise each input feature\n","\n","$$\n","\\hat{h}^{(k)}_j = \\frac{h^{(k)}_j - \\mathbb{E}[h^{(k)}_j]}{\\sqrt{\\text{Var}[h^{(k)}_j]}}.\n","$$\n","\n","In order to maintain full expressive power of the network, we make sure the final transformation can represent the identity:\n","\n","$$\n","z^{(k)}_j = \\gamma^{(k)}_j \\hat{h}^{(k)}_j + \\beta^{(k)}_j,\n","$$\n","\n","where $\\gamma^{(k)}_j$ and $\\beta^{(k)}_j$ are learned parameters. Note that setting $\\gamma^{(k)}_j = \\sqrt{\\text{Var}[h^{(k)}_j]}$ and $\\beta^{(k)}_j = \\mathbb{E}[h^{(k)}_j]$ recovers the original activations. However, now the model can control the mean and variance of activations within the hidden layer $\\mathbf{h}^{(k)}$ by tuning the parameters $\\gamma^{(k)}_j$ and $\\beta^{(k)}_j$. $\\mathbf{z}^{(k)}$ then becomes the new input to the next layer in the network.\n","\n","Statistics $\\mathbb{E}[h^{(k)}_j]$ and $\\text{Var}[h^{(k)}_j]$ are estimated over each minibatch $\\mathcal{D}_m$:\n","\n","$$\n","\\begin{align}\n","\\mu^{(k)}_{jm} &= \\frac{1}{M} \\sum_{i=1}^M h^{(k)}_{ij}\\\\\n","\\left(\\sigma^{(k)}_{jm}\\right)^2 &= \\frac{1}{M} \\sum_{i=1}^M (h^{(k)}_{ij} - \\mu^{(k)}_{jm})^2\\\\\n","\\hat{h}^{(k)}_{j} &= \\frac{h^{(k)}_{j} - \\mu^{(k)}_{jm}}{\\sqrt{\\left(\\sigma^{(k)}_{jm}\\right)^2 + \\epsilon}}\\\\\n","z^{(k)}_{j} &= \\gamma^{(k)}_j\\hat{h}^{(k)}_{j} + \\beta^{(k)}_j =: BN_{\\gamma^{(k)}, \\beta^{(k)}}\\left(h^{(k)}_{j}\\right)\n","\\end{align}\n","$$\n","\n","where $M = |\\mathcal{D}_m|$, and $h^{(k)}_{ij}$ is the activation value for the $j$-th for input $x_i\\in\\mathcal{D}_m$ in layer $k$.\n","\n","At training time, the estimates $\\mu^{(k)}_{jm}$ and $\\sigma^{(k)}_{jm}$ are computed on the minibatch $\\mathcal{D}_m$. In practical implementations, a running average of these estimates over the training run is used at test time.\n","\n","The batch normalisation calculation is fully differentiable, and so gradients can be backpropagated through this calculation as normal. \n","\n","The batch normalisation operation is implemented as a layer in TensorFlow. In the following experiment we will recreate the first example from the [original paper](#Ioffe15) on the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiECJR90EBD5"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAdH099_EBD5"},"outputs":[],"source":["# Load the MNIST dataset\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1-7wcziEBD5"},"outputs":[],"source":["# Create Dataset objects\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","\n","train_dataset.element_spec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3vvsdncEBD5"},"outputs":[],"source":["# Normalise pixel values in the Datasets\n","\n","def normalise_pixels(image, label):\n","    return (tf.cast(image, tf.float32) / 255., label)\n","\n","train_dataset = train_dataset.map(normalise_pixels)\n","test_dataset = test_dataset.map(normalise_pixels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5OuCbtSEBD5"},"outputs":[],"source":["# Shuffle and batch the dataset\n","\n","train_dataset = train_dataset.shuffle(1000).batch(60)\n","test_dataset = test_dataset.batch(60)"]},{"cell_type":"markdown","metadata":{"id":"dM4XBBaMEBD6"},"source":["We now define an MLP classifier model for the MNIST dataset. The following demonstrates how to build a model using [the functional API](https://www.tensorflow.org/guide/keras/functional)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nwqr1ujNEBD6"},"outputs":[],"source":["# Build the classifier model using the functional API\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ODaCLrsEBD6"},"outputs":[],"source":["# Print the model summary\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqOmrelEEBD6"},"outputs":[],"source":["# Define a loss function, optimiser and metric\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"R4qV5g9bEBD7"},"outputs":[],"source":["# Fit the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAr2yiO0EBD7"},"outputs":[],"source":["# Re-build the model with batch normalisation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuAGVe9MEBD8"},"outputs":[],"source":["# Compile the model \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"XPk_8e3gEBD9"},"outputs":[],"source":["# Fit the model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WnjAcXJ-EBD9"},"source":["We will compare the progress of the test accuracy in both models."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ihEm0HP7EBD9"},"outputs":[],"source":["# Plot the test accuracy\n","\n","import matplotlib.pyplot as plt\n","\n","plt.plot(bn_history.history['accuracy'])\n","plt.plot(no_bn_history.history['accuracy'], '--')\n","plt.legend(['With BN', 'Without BN'])\n","plt.ylabel(\"Test accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.title(\"Test accuracy vs epochs\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hG4c4JbHEBD9"},"source":["We see clearly in the above plot that the batch normalisation layers help the model to train faster, and to a higher accuracy.\n","\n","Batch normalisation reduces internal covariate shift, particularly early on in training. The distribution is more stable, making learning easier."]},{"cell_type":"markdown","metadata":{"id":"e0MJNnyyEBD-"},"source":["<a class=\"anchor\" id=\"references\"></a>\n","### References\n","\n","<a class=\"anchor\" id=\"Chen19\"></a>\n","* Chen, J. & Kyrillidis, A., (2019), \"Decaying Momentum Helps Neural Network Training\", arXiv preprint arXiv:1910.04952.\n","<a class=\"anchor\" id=\"Cybenko89\"></a>\n","* Cybenko, G. (1989) \"Approximations by superpositions of sigmoidal functions\", Mathematics of Control, Signals, and Systems, **2** (4), 303–314.\n","<a class=\"anchor\" id=\"Duchi11\"></a>\n","* Duchi, J., Hazan, E., & Singer, Y. (2011), \"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\", *Journal of Machine Learning Research*, **12**, 2121–2159.\n","<a class=\"anchor\" id=\"Gal16\"></a>\n","* Gal, Y. & Ghahramani, Z. (2016), \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\", Proceedings of The 33rd International Conference on Machine Learning, **48**, 1050-1059.\n","<a class=\"anchor\" id=\"Hanson88\"></a>\n","* Hanson, S. J. & Pratt, L. Y. (1988) \"Comparing biases for minimal network construction with back-propagation\", in *Proceedings of the 1st International Conference on Neural Information Processing Systems*,  177–185.\n","<a class=\"anchor\" id=\"Hardt15\"></a>\n","* Hardt, M., Recht, B., & Singer, Y. (2015), \"Train faster, generalize better: Stability of stochastic gradient descent\", in *Proceedings of the 33rd International Conference on International Conference on Machine Learning*, **48**, 1225-1234.\n","<a class=\"anchor\" id=\"Ioffe15\"></a>\n","* Ioffe, S. & Szegedy, C., \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", in *Proceedings of the 32nd International Conference on International Conference on Machine Learning*, **37**, 448–456.\n","<a class=\"anchor\" id=\"Keskar17\"></a>\n","* Keskar, N. S. * Socher, R. (2017), \"Improving Generalization Performance by Switching from Adam to SGD\", arXiv preprint, abs/1712.07628.\n","<a class=\"anchor\" id=\"Kingma15\"></a>\n","* Kingma, D. P. & Ba, J. L. (2015), \"Adam: a Method for Stochastic Optimization\", International Conference on Learning Representations, 1–13.\n","<a class=\"anchor\" id=\"Lu17\"></a>\n","* Lu, Z., Pu, H., Wang, F. Hu, Z., & Wang, L. (2017) \"The Expressive Power of Neural Networks: A View from the Width\", Advances in Neural Information Processing Systems 30. Curran Associates, Inc., 6231–6239.\n","<a class=\"anchor\" id=\"Nesterov83\"></a>\n","* Nesterov, Y. (1983), \"A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)\", Doklady ANSSSR (translated as Soviet. Math. Docl.), **269**, 543–547.\n","<a class=\"anchor\" id=\"Pennington14\"></a>\n","* Pennington, J., Socher, R. & Manning, C. D. (2014), \"Glove: Global vectors for word representation\", in *Proceedings of Empirical Methods in Natural Language Processing (EMNLP)*.\n","<a class=\"anchor\" id=\"Qian99\"></a>\n","* Qian, N. (1999), \"On the momentum term in gradient descent learning algorithms\", Neural Networks: The Official Journal of the International Neural Network Society, **12** (1), 145–151.\n","<a class=\"anchor\" id=\"Robbins51\"></a>\n","* Robbins, H. and Monro, S. (1951), \"A stochastic approximation method\", *The annals of mathematical statistics*, 400–407.\n","<a class=\"anchor\" id=\"Rumelhart86b\"></a>\n","* Rumelhart, D. E., Hinton, G., and Williams, R. (1986b), \"Learning representations by back-propagating errors\", Nature, **323**, 533-536.\n","<a class=\"anchor\" id=\"Rumelhart86c\"></a>\n","* Rumelhart, D. E., Hinton, G., and Williams, R. (1986c), \"Learning Internal Representations by Error Propagation\", in Rumelhart, D. E.; McClelland, J. L. (eds.), Parallel Distributed Processing : Explorations in the Microstructure of Cognition. Volume 1: Foundations, Cambridge, MIT Press.\n","<a class=\"anchor\" id=\"Smith15\"></a>\n","* Smith, L. N. (2015), \"Cyclical Learning Rates for Training Neural Networks\", arXiv preprint, abs/1506.01186.\n","<a class=\"anchor\" id=\"Srivastava14\"></a>\n","* Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014), \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", Journal of Machine Learning Research, **15**, 1929-1958.\n","<a class=\"anchor\" id=\"Werbos94\"></a>\n","* Werbos, P. J. (1994), \"The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting\", New York:, John Wiley & Sons."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Lecture_Week 2 - Optimisation and regularisation.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}