{"cells":[{"cell_type":"markdown","metadata":{"id":"XYVCd1zdKYFF"},"source":["# Deep Learning with TensorFlow\n","## Formative assessment\n","### Week 5: Variational autoencoder"]},{"cell_type":"markdown","metadata":{"id":"5PkSGtajKYFP"},"source":["#### Instructions\n","\n","In this notebook, you will write code to implement the variational autoencoder algorithm for an image dataset of celebrity faces. You will use the trained encoder and decoder networks to reconstruct and generate images. You will also see how the latent space encodes high-level information about the images.\n","\n","Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n","\n","`#### GRADED CELL ####`\n","\n","Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly.\n","\n","#### How to submit\n","\n","Complete all the tasks you are asked for in the notebook. When you have finished and are happy with your code, commit and push your changes to your repository. This will trigger the automated tests, which you will be able to check on GitHub.\n","\n","Make sure not to change the name or location of this notebook within your repository, or the automated tests will not be able to find it.\n","\n","#### Let's get started!\n","\n","We'll start by running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvTwOOa4KYFR"},"outputs":[],"source":["#### PACKAGE IMPORTS ####\n","\n","# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from pathlib import Path\n","\n","from tensorflow.keras import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D,  Conv2DTranspose, BatchNormalization\n","tfd = tfp.distributions\n","tfb = tfp.bijectors\n","tfpl = tfp.layers\n","\n","# If you would like to make further imports from tensorflow, add them here\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J36A1YSZKYFU"},"source":["<img src=\"figures/celeba.png\" alt=\"CelebA overview image\" style=\"width: 650px;\"/> \n","\n","#### The Large-scale CelebFaces Attributes (CelebA) Dataset\n","\n","For this assignment you will use a subset of the [CelebFaces Attributes (CelebA) dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). The full dataset contains over 200K images CelebA contains thousands of colour images of the faces of celebrities, together with tagged attributes such as 'Smiling', 'Wearing glasses', or 'Wearing lipstick'. It also contains information about bounding boxes and facial part localisation. CelebA is a popular dataset that is commonly used for face attribute recognition, face detection, landmark (or facial part) localization, and face editing & synthesis. \n","\n","* Z. Liu, P. Luo, X. Wang, and X. Tang. \"Deep Learning Face Attributes in the Wild\", Proceedings of International Conference on Computer Vision (ICCV), 2015.\n","\n","Your goal is to implement the variational autoencoder algorithm for a subset of the CelebA dataset. For practical reasons we will keep the dataset and the network size relatively small."]},{"cell_type":"markdown","metadata":{"id":"ta0wfFiDKYFW"},"source":["#### Load and preprocess the dataset\n","\n","For this assignment, you should first download a subset of the CelebA dataset that has been stored at the following link:\n","\n","https://drive.google.com/file/d/12o7l6bZZut7GyDykTDMO57qJTGuQcEr2/view?usp=sharing\n","\n","You should unzip this folder and save it at the location `\"./data/images\"`. Note that the full dataset can be downloaded from [the CelebA dataset webpage](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)), but this is not necessary for this assignment. \n","\n","In addition, attribute labels for the subset have been saved in the CSV file `\"./data/list_attr_celeba.csv\"`. We will use this in the last part of the assignment."]},{"cell_type":"markdown","metadata":{"id":"sx1c0R8jKYFX"},"source":["You should now write the following `load_dataset` function to create a `tf.data.Dataset` object from the files saved in the images folder.\n","\n","* The function takes `split` as an argument, which will be equal to one of the strings `\"train\"`, `\"val\"` or  `\"test\"`, `batch_size`, an optional `shuffle_buffer` argument and `image_dir` argument\n","* The function should create a Dataset containing the filepaths saved in the corresponding `split` subfolder of the `image_dir` directory\n","* The function should include a nested function used to map the Dataset\n","  * This function will take the `filepath` as an argument\n","  * It should read the contents of the file saved at `filepath` - this will be a jpeg image\n","  * It should then decode the jpeg and scale the pixel values to lie in the range $[0, 1]$\n","  * It should then return two identical copies of the image Tensor in a 2-tuple\n","* The function should then apply the nested function using the `map` method\n","* If `shuffle_buffer` is not None, then it should be used to shuffle the Dataset\n","* It should then batch the Dataset using the `batch_size` argument\n","* Finally, the function should prefetch the Dataset using the argument `tf.data.experimental.AUTOTUNE`\n","* The function should then return the Dataset\n","\n","_Hint: The Dataset can be created using_ `tf.data.Dataset.list_files`, _and using a wildcard character_ `'*.jpg'`_. Make sure that you set_ `shuffle=False` _when calling this method._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaOJrqd3KYFY"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def load_dataset(split, batch_size, shuffle_buffer=None, image_dir='./data/images'):\n","    \"\"\"\n","    This function should create a tf.data.Dataset object for one of the train/valid/test\n","    splits, according to the above specification.\n","    It should then return the Dataset.\n","    \"\"\"\n","    dataset = tf.data.Dataset.list_files('{}/{}/*.jpg'.format(image_dir, split), shuffle=False)\n","    \n","    def load_image(filepath):\n","        raw_img = tf.io.read_file(filepath) \n","        img_tensor = tf.image.decode_jpeg(raw_img, channels=3)\n","        img_tensor = tf.image.convert_image_dtype(img_tensor, tf.float32)\n","        return img_tensor, img_tensor\n","    \n","    dataset = dataset.map(load_image)\n","    if shuffle_buffer is not None:\n","        dataset = dataset.shuffle(shuffle_buffer)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e739GBPiKYFZ"},"outputs":[],"source":["# Use your function to obtain the train, valid and test Datasets\n","\n","train_ds = load_dataset('train', 32, shuffle_buffer=500)\n","valid_ds = load_dataset('val', 32)\n","test_ds = load_dataset('test', 8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaeFCayDKYFb"},"outputs":[],"source":["# Display a few examples\n","\n","n_rows, n_cols = 4, 8\n","f, axs = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n","\n","for img_batch, _ in train_ds.take(1):\n","    for n, image in enumerate(img_batch):\n","        i = n // n_cols\n","        j = n % n_cols\n","        axs[i, j].imshow(image)\n","        axs[i, j].axis('off')"]},{"cell_type":"markdown","metadata":{"id":"d4zUvvFGKYFc"},"source":["#### Define the prior distribution\n","\n","We will define a prior distribution that is a mixture of Gaussians. You should now complete the following function to define the mixture of Gaussians distribution for the prior, for a given number of components and latent space dimension. Each Gaussian component will have a diagonal covariance matrix. This distribution will have fixed mixing coefficients, but trainable means and standard deviations. \n","\n","* The function takes `num_modes` (number of components) and `latent_dim` as arguments\n","* Use the `tfd.MixtureSameFamily` for the prior distribution. Take a look at [the documentation](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily) for this distribution\n","  * The constructor takes a `mixture_distribution` and `components_distribution` as required arguments\n","  * The `mixture_distribution` should be fixed to a uniform `tfd.Categorical` distribution. This argument will therefore not contain any trainable variables\n","  * The `components_distribution` should be a `tfd.MultivariateNormalDiag` distribution with batch shape equal to `[num_modes]` and event shape equal to `[latent_dim]`. \n","    * The `tfd.MultivariateNormalDiag` distribution should have trainable `loc` parameter (initialised with a random normal distribution)  and trainable `scale_diag` parameter (initialised to ones)\n","    * The `scale_diag` variable should be enforced to be positive using `tfp.util.TransformedVariable` and the `tfb.Softplus` bijection\n","\n","The function should return the instance of the `tfd.MixtureSameFamily` distribution. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mmv4BTeEKYFd"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_prior(num_modes, latent_dim):\n","    \"\"\"\n","    This function should create an instance of a MixtureSameFamily distribution \n","    according to the above specification. \n","    The function takes the num_modes and latent_dim as arguments, which should \n","    be used to define the distribution.\n","    Your function should then return the distribution instance.\n","    \"\"\"\n","    prior = tfd.MixtureSameFamily(\n","                mixture_distribution=tfd.Categorical(probs=[1/num_modes]*num_modes),\n","                components_distribution=tfd.MultivariateNormalDiag(\n","                    loc=tf.Variable(tf.random.normal((num_modes, latent_dim))),\n","                    scale_diag=tfp.util.TransformedVariable(\n","                            tf.ones(shape=(num_modes, latent_dim)), tfb.Softplus())))\n","    return prior"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bA7mOBJBKYFe"},"outputs":[],"source":["# Define the prior distribution with 2 components and 50 dimensions for the latent space\n","\n","prior = get_prior(num_modes=2, latent_dim=50)"]},{"cell_type":"markdown","metadata":{"id":"ccr0-Xr9KYFf"},"source":["#### Define the encoder network\n","\n","We will now define the encoder network as part of the VAE. First, we will define the `KLDivergenceRegularizer` to use in the encoder network to add the KL divergence part of the loss. This should be defined according to the following specification:\n","\n","* The function takes the `prior_distribution` as an argument\n","* The function should use the `tfpl.KLDivergenceRegularizer` object to add the KL loss term\n","* The `tfpl.KLDivergenceRegularizer` should use a weight factor of 1.0 for the KL loss (standard ELBO objective)\n","* The KL loss cannot be computed exactly, so the `tfpl.KLDivergenceRegularizer` should compute a Monte Carlo approximation by drawing 3 samples from the posterior, and then averaging over the sample and batch axes\n","\n","Your function should then return the `tfpl.KLDivergenceRegularizer` object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAG6FY9-KYFf"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_kl_regularizer(prior_distribution):\n","    \"\"\"\n","    This function should create an instance of the KLDivergenceRegularizer \n","    according to the above specification. \n","    The function takes the prior_distribution, which should be used to define \n","    the distribution.\n","    Your function should then return the KLDivergenceRegularizer instance.\n","    \"\"\"\n","    kl_reg = tfpl.KLDivergenceRegularizer(prior_distribution, weight=1.0,\n","                                          test_points_fn=lambda q: q.sample(3),\n","                                          test_points_reduce_axis=(0, 1))\n","    return kl_reg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68OjUkMrKYFg"},"outputs":[],"source":["# Run your function to get the KLDivergenceRegularizer\n","\n","kl_regularizer = get_kl_regularizer(prior)"]},{"cell_type":"markdown","metadata":{"id":"UMdcSq9xKYFg"},"source":["You should now complete the following function to define the encoder network, according to the following specification:\n","\n","* The function takes the `latent_dim` and `kl_regularizer` as arguments\n","* Use the `Sequential` class to define the model, which has the following layers:\n","  * A Conv2D layer with 32 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding. It should also set the `input_shape` to `(64, 64, 3)`\n","  * BatchNormalization layer\n","  * Conv2D layer with 64 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n","  * BatchNormalization layer\n","  * Conv2D layer with 128 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n","  * BatchNormalization layer\n","  * Conv2D layer with 256 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n","  * BatchNormalization layer\n","  * Flatten layer\n","  * Dense layer with no activation function, and the right number of units to parameterise a `MultivariateNormalTriL` layer with event size equal to `latent_dim`\n","  * The final layer should be a `MultivariateNormalTriL` layer with event size equal to `latent_dim`, and it should use the `kl_regularizer` passed in as the argument as the activity regularizer\n","* In total, your model should have 11 layers. The function should then return the encoder model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5q7G2gK1KYFh"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_encoder(latent_dim, kl_regularizer):\n","    \"\"\"\n","    This function should build a CNN encoder model according to the above specification. \n","    The function takes latent_dim and kl_regularizer as arguments, which should be\n","    used to define the model.\n","    Your function should return the encoder model.\n","    \"\"\"\n","    encoder = Sequential([\n","        Conv2D(32, 3, activation='relu', strides=2,\n","               input_shape=(64, 64, 3), padding='same'), # (32, 32, 32)\n","        BatchNormalization(),\n","        Conv2D(64, 3, activation='relu', strides=2,\n","               padding='same'),  # (16, 16, 64)\n","        BatchNormalization(),\n","        Conv2D(128, 3, activation='relu', strides=2,\n","               padding='same'),  # (8, 8, 128)\n","        BatchNormalization(),\n","        Conv2D(256, 3, activation='relu', strides=2,\n","               padding='same'),  # (4, 4, 256)\n","        BatchNormalization(),\n","        Flatten(),\n","        Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim)),\n","        tfpl.MultivariateNormalTriL(event_size=latent_dim, activity_regularizer=kl_regularizer)\n","    ], name='encoder')  \n","    return encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnt2cygsKYFi"},"outputs":[],"source":["# Run your function to get the encoder\n","\n","encoder = get_encoder(latent_dim=50, kl_regularizer=kl_regularizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tbli7dIbKYFi"},"outputs":[],"source":["# Print the encoder summary\n","\n","encoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"SquC5feQKYFj"},"source":["#### Define the decoder network\n","\n","You should now define the decoder network for the VAE, using the `Sequential` API. This should be a neural network that returns an IndependentBernoulli distribution of `event_shape=(64, 64, 3)`.\n","\n","* The function takes the `latent_dim` as an argument\n","* Use the `Sequential` class to define the model with the following layers:\n","  * A Dense layer with 4096 units and ReLU activation. It should also set the `input_shape` to `(latent_dim,)`\n","  * A Reshape layer, that reshapes its input to `(4, 4, 256)`\n","  * BatchNormalization layer\n","  * Conv2DTranspose layer with 128 filters, 3x3 kernel size, ReLU activation, stride of 2x2 and 'SAME' padding\n","  * BatchNormalization layer\n","  * Conv2DTranspose layer with 64 filters, 3x3 kernel size, ReLU activation, stride of 2x2 and 'SAME' padding\n","  * BatchNormalization layer\n","  * Conv2DTranspose layer with 32 filters, 3x3 kernel size, ReLU activation, stride of 2x2 and 'SAME' padding\n","  * BatchNormalization layer\n","  * Conv2DTranspose layer with 3 filters, 3x3 kernel size, no activation function, stride of 2x2 and 'SAME' padding\n","  * A Flatten layer\n","  * The final layer should be a `IndependentBernoulli` layer with event size equal to `(64, 64, 3)`\n","* The Conv2DTranspose layers will need to be configured such that the final Conv2DTranspose layer outputs the correct shape Tensor for the `IndependentBernoulli` layer. You should pass in the `output_padding` argument to each of these layers.\n","* In total, your model should have 12 layers. The function should then return the decoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fr8YGE8FKYFk"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_decoder(latent_dim):\n","    \"\"\"\n","    This function should build a CNN decoder model according to the above specification. \n","    The function takes latent_dim as an argument, which should be used to define the model.\n","    Your function should return the decoder model.\n","    \"\"\"\n","    event_shape = (64, 64, 3)\n","    \n","    decoder = Sequential([\n","        Dense(4096, activation='relu', input_shape=(latent_dim,)),\n","        Reshape((4, 4, 256)),\n","        BatchNormalization(),\n","        Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same', output_padding=1),\n","        BatchNormalization(),\n","        Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same', output_padding=1),\n","        BatchNormalization(),\n","        Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same', output_padding=1),\n","        BatchNormalization(),\n","        Conv2DTranspose(3, 3, strides=2, padding='same', output_padding=1),\n","        Flatten(),\n","        tfpl.IndependentBernoulli(event_shape)\n","    ], name='decoder')\n","    return decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CopS0Xs_KYFk"},"outputs":[],"source":["# Run your function to get the decoder\n","\n","decoder = get_decoder(latent_dim=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"JMgyOe-EKYFk"},"outputs":[],"source":["# Print the decoder summary\n","\n","decoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"obo_TvBVKYFl"},"source":["#### Build and compile the end-to-end architecture\n","\n","Now that the encoder and decoder networks are defined, you should now complete the following function to build the complete encoder-decoder architecture and compile the model, ready for training. \n","\n","* The function takes the `encoder` and `decoder` networks as arguments\n","* It should combine these networks together into a single `Model`\n","* The model should then be compiled with the loss, optimizer and metric\n","  * The loss should be the average reconstruction loss over the batch\n","  * The optimizer should be the Adam optimizer with learning rate equal to 5e-4\n","  * Add the mean absolute error metric to measure the model's performance\n","* The function should then return the `Model` object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ty0M_OEKYFl"},"outputs":[],"source":[" #### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_compiled_model(encoder, decoder):\n","    \"\"\"\n","    This function should compute and return the average expected reconstruction loss,\n","    as defined above.\n","    The function takes batch_of_images and decoding_dist as arguments.\n","    The function should return the scalar average expected reconstruction loss.\n","    \"\"\"\n","    vae = Model(inputs=encoder.inputs, outputs=decoder(encoder.outputs))\n","    \n","    def reconstruction_loss(batch_of_images, decoding_dist):\n","        return -tf.reduce_mean(decoding_dist.log_prob(batch_of_images))\n","    \n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n","    vae.compile(optimizer=optimizer, loss=reconstruction_loss, metrics=['mae'])\n","    return vae"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFd8bNBdKYFm"},"outputs":[],"source":["# Run your function to define and compile the end-to-end architecture\n","\n","vae = get_compiled_model(encoder, decoder)"]},{"cell_type":"markdown","metadata":{"id":"A5-UX-rTKYFm"},"source":["#### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qc8uD0-OKYFm"},"outputs":[],"source":["# Fit the model\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n","vae.fit(train_ds, validation_data=valid_ds, epochs=40, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21qdh3wHKYFn"},"outputs":[],"source":["# Evaluate the model on the test set\n","\n","evaluation = vae.evaluate(test_ds, return_dict=True)\n","print(\"Test loss: {}, test MAE: {}\".format(evaluation['loss'], evaluation['mae']))"]},{"cell_type":"markdown","metadata":{"id":"Sy-UrhgmKYFn"},"source":["#### Compute reconstructions of test images\n","\n","We will now take a look at some image reconstructions from the encoder-decoder architecture.\n","\n","You should complete the following function, that uses `encoder` and `decoder` to reconstruct images from the test dataset. \n","\n","* This function takes the `encoder`, `decoder` and a Tensor `batch_of_images` as arguments\n","* It should then compute the reconstructions as follows:\n","  * Compute the means of the encoding distributions from passing the batch of images into the encoder\n","  * Pass these latent vectors through the decoder to get the output distribution\n","  * Compute the mean of the output distribution\n","* The function should then return the resulting Tensor, which will be of shape `(batch_size, 64, 64, 3)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfOH6sGPKYFn"},"outputs":[],"source":[" #### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def reconstruct(encoder, decoder, batch_of_images):\n","    \"\"\"\n","    This function should compute reconstructions of the batch_of_images according\n","    to the above instructions.\n","    The function takes the encoder, decoder and batch_of_images as inputs, which\n","    should be used to compute the reconstructions.\n","    The function should then return the reconstructions Tensor.\n","    \"\"\"\n","    mean_latent_vectors = encoder(batch_of_images).mean()\n","    mean_reconstructions = decoder(mean_latent_vectors).mean()\n","    return mean_reconstructions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aepWIBBfKYFo"},"outputs":[],"source":["# Use your function to compute and visualise reconstructions\n","\n","for test_batch, _ in test_ds.shuffle(100).take(1):\n","    reconstructions = reconstruct(encoder, decoder, test_batch)\n","\n","test_batch_size = 8\n","f, axs = plt.subplots(2, test_batch_size, figsize=(16, 6))\n","axs[0, 0].set_title(\"Original test images\", loc='left')\n","axs[1, 0].set_title(\"Reconstructed images\", loc='left')\n","for j in range(test_batch_size):\n","    axs[0, j].imshow(test_batch.numpy()[j])\n","    axs[1, j].imshow(reconstructions.numpy()[j])\n","    axs[0, j].axis('off')\n","    axs[1, j].axis('off')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"sDGywB2WKYFo"},"source":["#### Sample from the generative model\n","\n","You should complete the following `generate_images` function to generate new images. This function takes the prior distribution and decoder network as arguments, as well as the number of samples to generate. This function should be completed according to the following:\n","\n","* Sample a batch of `n_samples` images from the prior distribution, to obtain a latent vector Tensor of shape `(n_samples, 50)`\n","* Pass this batch of latent vectors through the decoder, to obtain an Independent Bernoulli distribution with batch shape equal to `[n_samples]` and event shape equal to `[64, 64, 3]`.\n","* Compute the means of the output distribution\n","* The function should then return the means Tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiWZE_kOKYFo"},"outputs":[],"source":[" #### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def generate_images(prior, decoder, n_samples):\n","    \"\"\"\n","    This function should compute generate new samples of images from the generative model,\n","    according to the above instructions.\n","    The function takes the prior distribution, decoder and number of samples as inputs, which\n","    should be used to generate the images.\n","    The function should then return the batch of generated images.\n","    \"\"\"\n","    sampled_latent_vectors = prior.sample(n_samples)\n","    sampled_images = decoder(sampled_latent_vectors).mean()\n","    return sampled_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9g9gAnTvKYFp"},"outputs":[],"source":["# Run your function to generate new images\n","\n","n_samples = 10\n","sampled_images = generate_images(prior, decoder, n_samples)\n","\n","f, axs = plt.subplots(1, n_samples, figsize=(16, 6))\n","\n","for j in range(n_samples):\n","    axs[j].imshow(sampled_images[j])\n","    axs[j].axis('off')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"8mjr1eL1KYFq"},"source":["#### Manipulate images in the latent space\n","\n","In this final section, we will see how the latent space encodes high-level information about the images, even though it has not been trained with any information apart from the images themselves.\n","\n","As mentioned earlier, each image in the CelebA dataset is labelled according to the attributes of the person pictured. The cell below will load these labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MCeYbs7KYFq"},"outputs":[],"source":["# Load the attribute labels\n","\n","labels = pd.read_csv(Path('./data/list_attr_celeba_subset.csv'))\n","labels.head()"]},{"cell_type":"markdown","metadata":{"id":"sA_UfiQVKYFr"},"source":["As can be seen above, each image is labelled with a binary indicator (1 true, -1 false), according to whether it posseses the attribute. The list of attributes contained in the `labels` DataFrame is shown below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZkzT-LaKYFr"},"outputs":[],"source":["# List the attributes contained in the DataFrame\n","\n","labels.columns[1:]"]},{"cell_type":"markdown","metadata":{"id":"r0CpIxmJKYFs"},"source":["We would like to perform some computations in the latent space, depending on the attribute values in the `labels` DataFrame. To do this, we will construct a new Dataset object, containing the images and attribute information.\n","\n","You should now complete the following `get_labelled_dataset` function to construct this new Dataset.\n","\n","* The function takes the arguments `split` (which again will be one of the strings `'train'`, `'val'` or `'test'`), an `attribute` string, the `labels` DataFrame and `image_dir` string\n","  * The `attribute` will be one of the column headers listed above\n","* As before, the function should create a Dataset containing the filepaths saved in the corresponding `split` subfolder of the `image_dir` directory\n","* The function should include a nested function used to map the Dataset similar to before\n","  * It should again read the contents of the file, decode the jpeg and scale the pixel values to lie in the range $[0, 1]$\n","  * It should then look up the `attribute` value for the image from the `labels` DataFrame\n","  * It should return a tuple containing the image Tensor, and scalar `tf.int32` label Tensor\n","* The function should then apply the nested function using the `map` method\n","* The function should then return the Dataset\n","\n","_Hint: convert the filenames and attribute columns of the_ `labels` _DataFrame into separate Tensor objects for use in the map function. The_ `tf.strings.split` _and_ `tf.where` _functions will be useful to extract the label for a given image._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZF1PLCj1KYFs"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_labelled_dataset(split, attribute, labels=labels, image_dir='./data/images'):\n","    \"\"\"\n","    This function should create a tf.data.Dataset object for one of the train/valid/test\n","    splits, according to the above specification.\n","    It should then return the Dataset.\n","    \"\"\"\n","    filenames = tf.constant(labels['image_id'])\n","    labels = tf.constant(labels[attribute], dtype=tf.int32)\n","    dataset = tf.data.Dataset.list_files('{}/{}/*.jpg'.format(image_dir, split), shuffle=False)\n","    \n","    def load_image(filepath):\n","        filename = tf.strings.split(filepath, '/')[-1]\n","        i = tf.where(filenames == filename)\n","        raw_img = tf.io.read_file(filepath) \n","        img_tensor = tf.image.decode_jpeg(raw_img, channels=3)\n","        img_tensor = tf.image.convert_image_dtype(img_tensor, tf.float32)\n","        label = labels[tf.squeeze(i)]\n","        return img_tensor, label\n","    \n","    dataset = dataset.map(load_image)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9X1PW_uKYFt"},"outputs":[],"source":["# Create the labelled Dataset from the train split\n","\n","labelled_train_ds = get_labelled_dataset('train', 'Eyeglasses', labels=labels)"]},{"cell_type":"markdown","metadata":{"id":"2zbPqVxWKYFt"},"source":["We now would like to compute the 'attribute vector' for the chosen attribute. This will be the average latent vector corresponding to all images that have the attribute, minus the average latent vector corresponding to all images that do not have the attribute. The intuition is that this vector will correspond the high-level property of adding the attribute to an image.\n","\n","You should now complete the following function to compute the attribute vector.\n","\n","* The function takes `labelled_dataset` as an argument, as well as the `encoder` network\n","* The function should compute the encoding distribution mean (latent vector) for all images that have the attribute, and (separately) all the images that do not\n","* It should then compute the average of each of these two sets of latent vectors\n","* It should then compute `avg_latent_with_attribute - avg_latent_without_attribute`. This is the attribute vector\n","* The function should then return the attribute vector as a numpy array of shape `(latent_dim,)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYIq2XMRKYFt"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_attribute_vector(labelled_dataset, encoder):\n","    \"\"\"\n","    This function should compute and return the attribute vector according \n","    to the above specification.\n","    \"\"\"\n","    latent_dim = encoder.layers[-1].output[0].shape[-1]\n","    with_attribute_ds = labelled_dataset.filter(lambda i, l: tf.math.equal(l, 1)).batch(128)\n","    without_attribute_ds = labelled_dataset.filter(lambda i, l: tf.math.equal(l, -1)).batch(128)\n","    \n","    avg_latent_with_attribute = np.empty(shape=(0, latent_dim), dtype=np.float32)\n","    for images, _ in with_attribute_ds:\n","        latents = encoder(images).mean()\n","        avg_latent_with_attribute = np.concatenate((avg_latent_with_attribute, latents.numpy()), \n","                                                   axis=0)\n","    avg_latent_with_attribute = np.mean(avg_latent_with_attribute, axis=0)    \n","    \n","    avg_latent_without_attribute = np.empty(shape=(0, latent_dim), dtype=np.float32)\n","    for images, _ in without_attribute_ds:\n","        latents = encoder(images).mean()\n","        avg_latent_without_attribute = np.concatenate((avg_latent_without_attribute, latents.numpy()), \n","                                                      axis=0)\n","    avg_latent_without_attribute = np.mean(avg_latent_without_attribute, axis=0)    \n","    \n","    return avg_latent_with_attribute - avg_latent_without_attribute"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dwi545nKYFu"},"outputs":[],"source":["# Get the attribute vector using your function\n","\n","attribute_vector = get_attribute_vector(labelled_train_ds, encoder)"]},{"cell_type":"markdown","metadata":{"id":"kPxHN8NLKYFu"},"source":["We can view this attribute vector by decoding it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-hORMdlKYFu"},"outputs":[],"source":["# Display the decoded attribute vector\n","\n","decoded_a = decoder(attribute_vector[np.newaxis, ...]).mean()\n","plt.imshow(decoded_a.numpy().squeeze())\n","plt.axis('off');"]},{"cell_type":"markdown","metadata":{"id":"el9eoq0yKYFu"},"source":["We can now use the attribute vector to add the attribute to an image reconstruction, where that attribute wasn't present before. To do this, we can just add the attribute vector to the latent vector encoding of the image, and then decode the result. We can also adjust the strength of the attribute vector by scaling with a multiplicative parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpKylxzwKYFv"},"outputs":[],"source":["# Add the attribute vector to a sample of images that don't have the attribute\n","\n","k = 2.5  # Weighting of attribute vector\n","num_examples = 8\n","labelled_test_ds = get_labelled_dataset('test', 'Eyeglasses', labels=labels).shuffle(100)\n","images_without_attribute = []\n","reconstructions = []\n","modified_images = []\n","for image, label in labelled_test_ds:\n","    if label == 1:  # Only proceses images without the attribute\n","        continue\n","    else:\n","        images_without_attribute.append(image.numpy())\n","        encoding = encoder(image[tf.newaxis, ...]).mean()\n","        decoded_image = decoder(encoding).mean()\n","        reconstructions.append(np.squeeze(decoded_image.numpy()))\n","        modified_encoding = encoding + (k * attribute_vector)\n","        modified_reconstruction = decoder(modified_encoding).mean()\n","        modified_images.append(np.squeeze(modified_reconstruction.numpy()))\n","    if len(modified_images) >= num_examples:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpqOZJWVKYFv"},"outputs":[],"source":["# Display the original images, their reconstructions, and modified reconstructions\n","\n","num_examples = 8\n","f, axs = plt.subplots(3, num_examples, figsize=(16, 6))\n","axs[0, 0].set_title(\"Original images\", loc='left')\n","axs[1, 0].set_title(\"Reconstructed images\", loc='left')\n","axs[2, 0].set_title(\"Images with added attribute\", loc='left')\n","for j in range(num_examples):\n","    axs[0, j].imshow(images_without_attribute[j])\n","    axs[1, j].imshow(reconstructions[j])\n","    axs[2, j].imshow(modified_images[j])\n","    for ax in axs[:, j]: ax.axis('off')\n","    \n","plt.tight_layout();"]},{"cell_type":"markdown","metadata":{"id":"pCYox1mdKYFv"},"source":["You could also try removing the attribute from images that possess the attribute, or experiment with a different attribute.\n","\n","In this assignment you have developed the variational autoencoder algorithm for the CelebA dataset, using tools from the TensorFlow Probability library, and used the trained networks to generate samples, compute reconstructions, and modify dataset images with high-level semantic information extracted from the latent space."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Week 5 - Formative assessment_ANSWERS(1) (2).ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}