{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow\n",
    "### Week 4: Normalising flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Change of variables formula](#changeofvariables)\n",
    "\n",
    "[3. Distributions (\\*)](#distributions)\n",
    "\n",
    "[4. Bijectors (\\*)](#bijectors)\n",
    "\n",
    "[5. NICE / RealNVP](#nicerealnvp)\n",
    "\n",
    "[6. Bijector subclassing (\\*)](#bijector_subclassing)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "So far in this module we have covered many of the fundamental building blocks of deep learning: from mathematical neurons and multilayer perceptrons, to optimisation and regularisation of deep learning models, and the important CNN architecture.\n",
    "\n",
    "In the remaining two weeks of the module, we will use these building blocks to focus our attention on the probabilistic approach to deep learning. This is an increasingly popular branch of deep learning that aims to make use of tools from probability theory to account for noise and uncertainty in the data. Probabilistic deep learning models make direct use of probability distributions and latent random variables in the model architecture.\n",
    "\n",
    "In this week of the course we will begin to familiarise ourselves with the [TensorFlow Probability](https://www.tensorflow.org/probability/) (TFP) library, which is built on TensorFlow to enable a closer integration between deep learning, probabilistic modelling and statistical analysis. In particular, we will learn about `Distribution` and `Bijector` objects in TFP.\n",
    "\n",
    "These objects will provide the tools we need to develop normalising flow deep learning models. Normalising flows are a class of generative models, that were first popularised in the context of variational inference by [Rezende & Mohamed 2015](#Rezende15), and in the context of density estimation by [Dinh et al 2015](#Dinh15). In this week, we will focus on using normalising flows to estimate continuous data distributions.\n",
    "\n",
    "When trained as a density estimator, this type of model is able to produce new instances that could plausibly have come from the same dataset that it is trained on, as well as tell you whether a given example instance is likely. However, for complex datasets the data distribution can be very difficult to model, so this is a highly nontrivial task in general. This is where the power of deep learning models can be leveraged to learn highly multimodal and complicated data distributions, and this type of model has been successfully applied to domains such as image generation ([Ho et al 2019](#Ho19)), noise modelling ([Abdelhamed et al 2019](#Abdelhamed19)), audio synthesis ([Prenger et al 2019](#Prenger19)), and video generation ([Kumar et al 2019](#Kumar19)).\n",
    "\n",
    "In this week, we will see how to construct the popular normalising flow architecture called RealNVP ([Dinh et al 2017](#Dinh17))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"changeofvariables\"></a>\n",
    "## Change of variables formula\n",
    "\n",
    "The approach taken by normalising flows to solve the density estimation task is to take an initial, simple density, and transform it - possibly using a series of parameterised transformations - to produce a rich and complex distribution. \n",
    "\n",
    "If these transformations are smooth and invertible, then we are able to evaluate the density of the complex transformed distribution. This property is important, because it then allows to train such a model using maximum likelihood. This is the idea behind normalising flows. The invertible transformations themselves are what constitute the bijectors module in the Tensorflow Probability library.\n",
    "\n",
    "We'll start this week by reviewing the change of variables formula, which forms the mathematical basis of normalising flows.\n",
    "\n",
    "#### Statement of the formula\n",
    "Let $Z := (z_1,\\ldots,z_D)\\in\\mathbb{R}^D$ be a $D$-dimensional continuous random variable, and suppose that $f:\\mathbb{R}^D\\mapsto\\mathbb{R}^D$ is a smooth, invertible transformation. Now consider the change of variables $X = f(Z)$, with $X=(x_1,\\ldots,x_D)$, and denote the probability density functions of the random variables $Z$ and $X$ by $p_Z$ and $p_X$ respectively.\n",
    "\n",
    "The change of variables formula states that\n",
    "\n",
    "$$\n",
    "p_X(\\mathbf{x}) = p_Z(\\mathbf{z})\\cdot\\left|\\det J_f(\\mathbf{z}) \\right|^{-1},\\label{cov_f}\\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}, \\mathbf{z}\\in\\mathbb{R}^D$, and $J_f(\\mathbf{z})\\in\\mathbb{R}^{D\\times D}$ is the **Jacobian** of the transformation $f$, given by the matrix of partial derivatives\n",
    "\n",
    "$$\n",
    "J_f(\\mathbf{z}) = \\left[ \n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial f_1}{\\partial z_1}(\\mathbf{z}) & \\cdots & \\frac{\\partial f_1}{\\partial z_D}(\\mathbf{z})\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f_D}{\\partial z_1}(\\mathbf{z}) & \\cdots & \\frac{\\partial f_D}{\\partial z_d}(\\mathbf{z})\\\\\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "and $\\left|\\det J_f(\\mathbf{z}) \\right|$ is the absolute value of the determinant of the Jacobian matrix. Note that (1) can also be written in the log-form\n",
    "\n",
    "$$\n",
    "\\log p_X(\\mathbf{x}) = \\log p_Z(\\mathbf{z}) - \\log \\hspace{0.1ex}\\left|\\det J_f(\\mathbf{z}) \\right|. \\label{logcov_f}\\tag{2}\n",
    "$$\n",
    "\n",
    "Furthermore, we can equivalently consider the transformation $Z = f^{-1}(X)$. Then the change of variables formulae can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_Z(\\mathbf{z}) &= p_X(\\mathbf{x})\\cdot\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right|^{-1},\\label{cov_finv}\\tag{3}\\\\\n",
    "\\log p_Z(\\mathbf{z}) &= \\log p_X(\\mathbf{x}) - \\log \\hspace{0.1ex}\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right|.\\label{logcov_finv}\\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### A simple example\n",
    "We will demonstrate the change of variables formula with a simple example. Let $Z=(z_1, z_2)$ be a 2-dimensional random variable that is uniformly distributed on the unit square $[0, 1]^2 =: \\Omega_Z$. We also define the transformation $f:\\mathbb{R}^2 \\mapsto \\mathbb{R}^2$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(z_1, z_2) = (\\lambda z_1, \\mu z_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for some nonzero $\\lambda, \\mu\\in\\mathbb{R}$. The random variable $X=(x_1, x_2)$ is given by $X = f(Z)$. \n",
    "\n",
    "<img src=\"figures/change_of_variables.pdf\" alt=\"Change of variables example in 2D\" style=\"width: 750px;\"/>\n",
    "<center>Linearly transformed uniformly distributed random variable</center>\n",
    "\n",
    "Since $\\int_{\\Omega_Z}p_Z(\\mathbf{z})d\\mathbf{z} = 1$ and $\\mathbf{z}$ is uniformly distributed, we have that \n",
    "\n",
    "$$\n",
    "p_Z(\\mathbf{z}) = 1 \\quad\\text{for}\\quad \\mathbf{z}\\in\\Omega_Z.\n",
    "$$\n",
    "\n",
    "The random variable $X$ is uniformly distributed on the region $\\Omega_X = f(\\Omega_Z)$ as shown in the figure above (for the case $\\lambda, \\mu>0$). Since again $\\int_{\\Omega_X}p_X(\\mathbf{x})d\\mathbf{x} = 1$, the probability density function for $X$ must be given by \n",
    "\n",
    "$$\n",
    "p_X(\\mathbf{x}) = \\frac{1}{|\\Omega_X|} = \\frac{1}{|\\lambda\\mu |}\\quad\\text{for}\\quad \\mathbf{x}\\in\\Omega_X.\n",
    "$$\n",
    "\n",
    "This result corresponds to the equations \\eqref{cov_f}-\\eqref{logcov_finv} above. In this simple example, the transformation $f$ is linear, and the Jacobian matrix is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_f(\\mathbf{z}) = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\lambda & 0\\\\\n",
    "0 & \\mu\n",
    "\\end{array}\n",
    "\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The absolute value of the determinant is $\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right| = |\\lambda\\mu | \\ne 0$. Equation \\eqref{cov_f} then implies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_X(\\mathbf{x}) &= p_Z(\\mathbf{z})\\cdot\\left|\\det J_f(\\mathbf{z}) \\right|^{-1}\\\\\n",
    "&= \\frac{1}{|\\lambda\\mu|}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Writing in the log-form as in equation \\eqref{logcov_f} gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_X(\\mathbf{x}) &= \\log p_Z(\\mathbf{z}) - \\log \\hspace{0.1ex}\\left|\\det J_f(\\mathbf{z}) \\right|\\\\\n",
    "&= \\log (1) - \\log |\\lambda\\mu|\\\\\n",
    "&= - \\log |\\lambda\\mu|.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Sketch of proof in 1-D\n",
    "We now provide a sketch of the proof of the change of variables formula in one dimension. Let $Z$ and $X$ be random variables such that $X = f(Z)$, where $f : \\mathbb{R}\\mapsto\\mathbb{R}$ is a $C^k$ diffeomorphism with $k\\ge 1$. The change of variables formula in one dimension can be written\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(z)􏰃\\cdot\\left| \\frac{d}{dz}f(z) \\right|^{-1},\\qquad\\text{(cf. equation \\eqref{cov_f})}\n",
    "$$\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(z)􏰃\\cdot\\left| \\frac{d}{dx}f^{-1}(x) \\right|.\\qquad\\text{(cf. equation \\eqref{cov_finv})}\n",
    "$$\n",
    "\n",
    "_Sketch of proof._ For $f$ to be invertible, it must be strictly monotonic. That means that for all $x^{(1)}, x^{(2)}\\in\\mathbb{R}$ with $x^{(1)} < x^{(2)}$, we have $f(x^{(1)}) < f(x^{(2)})$ (strictly monotonically increasing) or $f(x^{(1)}) > f(x^{(2)})$ (strictly monotonically decreasing).\n",
    "\n",
    "<img src=\"figures/change_of_variables_monotonic.pdf\" alt=\"Monotonic functions\" style=\"width: 600px;\"/>\n",
    "<center>Sketch of monotonic functions: (a) strictly increasing, (b) strictly decreasing</center>\n",
    "\n",
    "Suppose first that $f$ is strictly increasing. Also let $F_X$ and $F_Z$ be the cumulative distribution functions of the random variables $X$ and $Z$ respectively. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_X(x) &= P(X \\le x)\\\\\n",
    "&= P(f(Z) \\le x)\\\\\n",
    "&= P(Z \\le f^{-1}(x))\\qquad\\text{(since $f$ is monotonically increasing)}\\\\\n",
    "&= F_Z(f^{-1}(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By differentiating on both sides with respect to $x$, we obtain the probability density function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_X(x) &= \\frac{d}{dx}F_X(x)\\\\\n",
    "&= \\frac{d}{dx} F_Z(f^{-1}(x))\\\\\n",
    "&= \\frac{d}{dz}F_Z(z)\\cdot\\frac{d}{dx}f^{-1}(x)\\\\\n",
    "&= p_Z(z)\\frac{d}{dx}f^{-1}(x) \\label{pdfx_inc}\\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now suppose first that $f$ is strictly decreasing. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_X(x) &= P(X \\le x)\\\\\n",
    "&= P(f(Z) \\le x)\\\\\n",
    "&= P(Z \\ge f^{-1}(x))\\qquad\\text{(since $f$ is monotonically decreasing)}\\\\\n",
    "&= 1 - F_Z(f^{-1}(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Again differentiating on both sides with respect to $x$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_X(x) &= \\frac{d}{dx}F_X(x)\\\\\n",
    "&= -\\frac{d}{dx} F_Z(f^{-1}(x))\\\\\n",
    "&= -F_Z'(f^{-1}(x))\\frac{d}{dx}f^{-1}(x)\\\\\n",
    "&= -p_Z(z)\\frac{d}{dx}f^{-1}(x) \\label{pdfx_dec}\\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now note that the inverse of a strictly monotonically increasing (resp. decreasing) function is again strictly monotonically increasing (resp. decreasing). This implies that the quantity $\\frac{d}{dx} f^{-1}(x)$ is positive in \\eqref{pdfx_inc} and negative in \\eqref{pdfx_dec}, and so these two equations can be combined into the single equation:\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(z)\\left|\\frac{d}{dx}f^{-1}(x)\\right|\n",
    "$$\n",
    "\n",
    "which completes the proof.\n",
    "\n",
    "#### Application to normalising flows\n",
    "Normalising flows are a class of models that exploit the change of variables formula to estimate an unknown target data density. \n",
    "\n",
    "Suppose we have data samples $\\mathcal{D}:=\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\}$, with each $\\mathbf{x}^{(i)}\\in\\mathbb{R}^D$, and assume that these samples are generated i.i.d. from the underlying distribution $p_X$. \n",
    "\n",
    "A normalising flow models the distribution $p_X$ using a random variable $Z$ (also of dimension $D$) with a simple distribution $p_Z$ (e.g. an isotropic Gaussian), such that the random variable $X$ can be written as a change of variables $X = f_\\theta(Z)$, where $\\theta$ is a parameter vector that parameterises the smooth invertible function $f_\\theta$. \n",
    "\n",
    "The function $f_\\theta$ is modelled using a neural network with parameters $\\theta$, which we want to learn from the data. An important point is that this neural network must be designed to be invertible, which is not the case in general with deep learning models. In practice, we often construct the neural network by composing multiple simpler blocks together. In TensorFlow Probability, these simpler blocks are the _bijectors_ that we will study in the first part of the week.\n",
    "\n",
    "We use the principle of maximum likelihood to learn the optimal parameters $\\theta$; that is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{ML} &:= \\arg \\max_{\\theta} P(\\mathcal{D}; \\theta)\\\\\n",
    "&= \\arg \\max_{\\theta} \\log P(\\mathcal{D}; \\theta).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In order to compute $\\log P(\\mathcal{D}; \\theta)$ we can use the change of variables formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathcal{D}; \\theta) &= \\prod_{\\mathbf{x}\\in\\mathcal{D}}  p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot\\left|\\hspace{0.1ex}\\det J_{f_\\theta^{-1}}(\\mathbf{x}) \\hspace{0.1ex}\\right|\\\\\n",
    "\\log P(\\mathcal{D}; \\theta) &= \\sum_{x\\in\\mathcal{D}} \\log p_Z(f_\\theta^{-1}(\\mathbf{x})) + \\log \\hspace{0.1ex}\\left|\\hspace{0.1ex}\\det J_{f_\\theta^{-1}}(\\mathbf{x}) \\hspace{0.1ex}\\right|\\label{logliknf}\\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term $p_Z(f_\\theta^{-1}(\\mathbf{x}))$ can be computed for a given data point $\\mathbf{x}\\in\\mathcal{D}$ since the neural network $f_\\theta$ is designed to be invertible, and the distribution $p_Z$ is known. The term $\\det J_{f_\\theta^{-1}}(\\mathbf{x})$ is also computable, although this also highlights another important aspect of normalising flow models: they should be designed such that the determinant of the Jacobian can be efficiently computed.\n",
    "\n",
    "The log-likelihood \\eqref{logliknf} is usually optimised as usual in minibatches, with gradient-based optimisation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"distributions\"></a>\n",
    "## Distributions\n",
    "\n",
    "In this section we will look at `Distribution` objects in TensorFlow Probability, which are naturally one of the fundamental building blocks of the library. The main operations that we'll be using are sampling, and computing log-probabilities. \n",
    "\n",
    "A key point in understanding the interface and behaviour of these objects is that they are designed to perform vectorised computations for efficiency. This means that single objects are able to handle batches of distributions, samples, and log-probability computations. We'll see that this means we have to think quite carefully about the shapes of Tensors that we're using, and what these shapes mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following namespace for the TensorFlow Probability library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "print(tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate distributions\n",
    "We will first create some univariate distributions. There is a wide range of distributions available in the [distributions module](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions), of which we will only be using a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a univariate Normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw multiple samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass a shape to the sample method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prob / log-prob of test points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute prob / log-prob of a batch of test points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single `Distribution` object can represent a batch of distributions of the same type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an exponential distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched exponential distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a first use of broadcasting when computing log-probabilities with a batched distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate distributions\n",
    "In the distributions seen so far, the `event_shape` property has been empty, indicating that the distribution is univariate. Here, we look at multivariate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multivariate Gaussian distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from the multivariate Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a multivariate Gaussian using [`MultivariateNormalTriL`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalTriL), by passing in the lower triangular matrix $L$ such that $LL^T = \\Sigma$, where $\\Sigma$ is the covariance matrix. This is the Cholesky decomposition (see also [`tf.linalg.cholesky`](https://www.tensorflow.org/api_docs/python/tf/linalg/cholesky))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a multivariate Gaussian with MultivariateNormalTriL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from the multivariate Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further ways of constructing a multivariate Gaussian: see the docs for [`MultivariateNormalDiagPlusLowRank`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiagPlusLowRank), [`MultivariateNormalFullCovariance`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance) and [`MultivariateNormalLinearOperator`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalLinearOperator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate distributions can also be batched together, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched multivariate Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch and event shape are properties of the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also sample from a batched, multivariate distribution. The following shows the ordering of shapes that we should always keep in mind when working with `Distribution` objects:\n",
    "\n",
    "`(sample_shape, batch_shape, event_shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sample from the batched multivariate Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Take a look at the following Distribution object and call to the `log_prob` method. Work out what the shape of the resulting Tensor will be before you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn3 = tfd.MultivariateNormalDiag(loc=[[[2., 0., 0.5], [1., -0.5, 2.]]], scale_diag=[0.5, 1., 1.5])\n",
    "test_pts = tf.random.normal((5, 1, 2, 1))\n",
    "mvn3.log_prob(test_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent distribution\n",
    "The `Independent` distribution is often useful to manipulate batch and event shapes, and define multivariate distributions from univariate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched Bernoulli distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the second batch dimension into the event space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log_probs on both distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `Independent` distribution shifts all batch dimensions except the first into the event space. This can be changed with the `reinterpreted_batch_ndims` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the reinterpreted_batch_ndims keyword argument\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-probs with the new distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Construct a distribution object over a three-dimensional event space $(X_1, X_2, X_3)$, where each $X_i$ are independently distributed according to a Bernoulli distribution where the probability of a 0 event is equal to 0.9, 0.7 and 0.5 respectively. Use your distribution object to show that the log probability of the event $P(X_1, X_2, X_3) = (1, 1, 1)$ is equal to -4.199705."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"bijectors\"></a>\n",
    "## Bijectors\n",
    "\n",
    "In this section we will look at bijectors, which are another fundamental building block in TensorFlow Probability. Bijectors constitute the invertible and differentiable transformations that we will use to construct normalising flows. The [bijectors module](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors) has a range of in-built bijector functions, which can be composed to make complex transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two simple bijectors are the `Scale` and `Shift` bijectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scale and Shift bijectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples from a standard Normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the samples through the forward method of each bijector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the original and transformed samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(13, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(z.numpy(), bins=50)\n",
    "plt.title(\"Original samples\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(x.numpy(), bins=50)\n",
    "plt.title(\"Transformed samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the bijectors together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pass the transformed samples through the inverse method of each bijector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batched bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched Softfloor bijector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass some test points through the bijector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed samples\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(test_pts, transformed_pts[:, 0], label='temperature=0.01')\n",
    "plt.plot(test_pts, transformed_pts[:, 1], label='temperature=0.1')\n",
    "plt.plot(test_pts, transformed_pts[:, 2], label='temperature=1.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing log-probs of transformed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Exp bijector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the exp of the standard Normal samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the change of variables formula:\n",
    "\n",
    "$$\n",
    "\\log p_X(\\mathbf{x}) = \\log p_Z(\\mathbf{z}) - \\log \\hspace{0.1ex}\\left|\\det J_f(\\mathbf{z}) \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use log_det_jacobian to compute log_probs of transformed samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the inverse transformation:\n",
    "\n",
    "$$\n",
    "\\log p_X(\\mathbf{x}) = \\log p_Z(\\mathbf{z}) + \\log \\hspace{0.1ex}\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the calculation with the inverse_log_det_jacobian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TransformedDistribution\n",
    "The `TransformedDistribution` class provides a consistent API for distributions defined by bijectors and base distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-normal distribution with TransformedDistribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the log-probs of the transformed samples are the same as under a log-normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformedDistribution` objects can also be defined by calling the bijector on the base distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the log-normal TransformedDistribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TransformedDistribution` infers the batch shape by broadcasting the batch shapes of the base distribution and the bijector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TransformedDistribution from a batched bijector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new TransformedDistribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a scaling lower triangular matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a bijector that operates on a rank >= 1 event space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TransformedDistribution with a batch and event shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the transformed distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Construct the distribution $\\mathcal{N}(\\mu, \\Sigma)$, where $\\mu = [0.5, -0.5]^T$ and $\\Sigma = \\left[\\begin{array}{cc} 2 & 1\\\\ 1 & 2\\end{array}\\right]$, first using a `tfd.MultivariateNormalTriL` object, and then using a `tfd.TransformedDistribution` object with a zero-mean Gaussian with identity covariance matrix as a base distribution. Verify that the two representations are mathematically equivalent by computing log probs on a given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"nicerealnvp\"></a>\n",
    "## NICE / RealNVP\n",
    "\n",
    "#### NICE\n",
    "NICE stands for \"nonlinear independent components estimation\", and is a deep learning architecture framework for density estimation tasks. A key motivation for the proposed framework given in the abstract of the [original paper](#Dinh15) is as follows:\n",
    "\n",
    "> It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables.\n",
    "\n",
    "As with many normalising flow examples, a typical choice for a base distribution would be an isotropic Gaussian, which is then transformed by the deep learning model. An important aspect is the efficient calculation of the Jacobian determinant of the transformation. \n",
    "\n",
    "In this section, we will describe the NICE architecture, and the RealNVP architecture that is built upon it. We will follow the exposition of the original papers, and think of the forward transformation as acting on the data input example. Note however that this is in contrast to the usual bijector convention of using the forward transformation for sampling, and the inverse transformation for computing log probs.\n",
    "\n",
    "#### Affine coupling layer\n",
    "The basic building block of the NICE architecture is the affine coupling layer. Given an input $\\mathbf{x}\\in\\mathbb{R}^D$, we split it into two blocks $(\\mathbf{x}_{1:d}, \\mathbf{x}_{d+1:D})$, where $d<D$ (usually $d\\approx D / 2$), and apply a transformation of the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d},\\label{nice_acl1}\\tag{1}\\\\\n",
    "\\mathbf{z}_{d+1:D} &= \\mathbf{x}_{d+1:D} + t(\\mathbf{x}_{1:d}),\\label{nice_acl2}\\tag{2}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $t:\\mathbb{R}^d\\mapsto\\mathbb{R}^{D-d}$ is an arbitrarily complex function, such as a neural network. It is easy to see that the coupling layer as above has an identity Jacobian matrix, and is trivially invertible:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d},\\\\\n",
    "\\mathbf{x}_{d+1:D} &= \\mathbf{z}_{d+1:D} - t(\\mathbf{z}_{1:d}).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Several coupling layers can be composed together to obtain a more complex, layered transformation. Note that a coupling layer leaves part of its input unchanged, and so the roles of the two subsets should be interchanged in alternating layers. \n",
    "\n",
    "If we examine the Jacobian, we can see that at least three coupling layers are needed to allow all dimensions to influence each other (this is left as an exercise for the reader). In the NICE paper, networks were composed with four coupling layers.\n",
    "\n",
    "#### RealNVP\n",
    "RealNVP stands for real-valued, non-volume preserving ([Dinh et al 2017](#Dinh17)). It was a follow-up work to the NICE paper, in which the affine coupling layer was modified as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d},\\label{realnvp_acl1}\\tag{3}\\\\\n",
    "\\mathbf{z}_{d+1:D} &= \\mathbf{x}_{d+1:D}\\odot \\exp(s(\\mathbf{x}_{1:d})) + t(\\mathbf{x}_{1:d}),\\label{realnvp_acl2}\\tag{4}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $s$ and $t$ stand for scale and translation, and are both functions that map from $\\mathbb{R}^d$ to $\\mathbb{R}^{D-d}$. The name RealNVP emphasises the fact that the transformation \\eqref{realnvp_acl1}-\\eqref{realnvp_acl2} is no longer volume-preserving, as is the case with \\eqref{nice_acl1}-\\eqref{nice_acl2}, due to the additional scaling provided by the term $\\exp(s(\\mathbf{x}_{1:d}))$. We use the network output $s(\\mathbf{x}_{1:d})$ as a log-scale parameter for numerical stability.\n",
    "\n",
    "As before, the inverse transformation is no more complex than the forward propagation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d},\\label{realnvp_inv_acl1}\\tag{5}\\\\\n",
    "\\mathbf{x}_{d+1:D} &= (\\mathbf{z}_{d+1:D} - t(\\mathbf{z}_{1:d})) \\odot \\exp(-s(\\mathbf{z}_{1:d})).\\label{realnvp_inv_acl2}\\tag{6}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<img src=\"figures/affine_coupling_layer.png\" alt=\"RealNVP: forward pass\" style=\"width: 800px;\"/>\n",
    "<center>The forward and inverse passes of the RealNVP affine coupling layer</center>\n",
    "\n",
    "Now, the Jacobian is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\mathbb{I}_d & \\mathbf{0}\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{d+1:D}}{\\partial \\mathbf{x}_{1:d}} & \\text{diag}\\,(\\exp (s(\\mathbf{x}_{1:d})))\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{D\\times D}\n",
    "$$\n",
    "\n",
    "and the log of the absolute value of the Jacobian determinant is easily calculated as $\\sum_j s(\\mathbf{x}_{1:d})_j$.\n",
    "\n",
    "#### Spatial and channel-wise masking\n",
    "Observe that the partitioning $\\mathbf{x}\\rightarrow (\\mathbf{x}_{1:d}, \\mathbf{x}_{d+1:D})$ can be implemented using a binary mask $b\\in\\{0, 1\\}^{n_h\\times n_w\\times c}$, so that the forward pass \\eqref{realnvp_acl1}-\\eqref{realnvp_acl2} can be written\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})).\\label{realnvp_acl}\\tag{7}\n",
    "$$\n",
    "\n",
    "Similarly, the inverse pass \\eqref{realnvp_inv_acl1}-\\eqref{realnvp_inv_acl2} can be written\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = b\\odot \\mathbf{z} + (1-b)\\odot((\\mathbf{z}-t(b\\odot\\mathbf{z}))\\odot \\exp(-s(b\\odot \\mathbf{z}))).\\label{realnvp_inv_acl}\\tag{8}\n",
    "$$\n",
    "\n",
    "RealNVP implements two types of masking for image data $\\mathbf{x}\\in\\mathbb{R}^{n_h\\times n_w\\times c}$: spatial checkerboard and channel-wise masking. A spatial checkerboard mask applies the same partitioning to every channel dimension, as illustrated in the following figure.\n",
    "\n",
    "<img src=\"figures/checkerboard_mask.png\" alt=\"Checkerboard masking\" style=\"width: 600px;\"/>\n",
    "<center>Spatial checkerboard masking in RealNVP. (a) A layer input $\\mathbf{h}\\in\\mathbb{R}^{6\\times 6\\times 4}$ without masking, and (b) multiplied elementwise by a spatial checkerboard mask $b_s\\in\\{0, 1\\}^{6\\times 6}$, which is broadcast along the channel dimension</center>\n",
    "\n",
    "A channel mask instead operates along the channel dimension, and applies the same partitioning at every spatial location, as in the following figure.\n",
    "\n",
    "<img src=\"figures/channel_mask.png\" alt=\"Channel masking\" style=\"width: 600px;\"/>\n",
    "<center>Channel masking in RealNVP. (a) A layer input $\\mathbf{h}\\in\\mathbb{R}^{6\\times 6\\times 4}$ without masking, and (b) multiplied elementwise by a channel mask $b_c\\in\\{0, 1\\}^{4}$, which is broadcast across the spatial dimensions</center>\n",
    "\n",
    "As in the NICE framework, we want to ensure that all dimensions are able to interact with each other. The RealNVP architecture consists of three layers of alternating checkerboard masks, where the partitions are permuted. \n",
    "\n",
    "<img src=\"figures/alternating_masks.png\" alt=\"Alternating masks\" style=\"width: 900px;\"/>\n",
    "<center>Three affine coupling layers, with alternating masks in between layers</center>\n",
    "\n",
    "#### Squeeze operation\n",
    "In the RealNVP architecture, after the three affine coupling layers with checkerboard masking there is a squeeze operation, where the spatial dimensions of the layer are divided into $2\\times 2\\times c$ subsquares, and reshaped into $1\\times 1\\times 4c$. The figure below illustrates this operation for a single channel:\n",
    "\n",
    "<img src=\"figures/squeeze.png\" alt=\"Squeeze operation\" style=\"width: 600px;\"/>\n",
    "<center>The squeeze operation. The spatial dimensions are halved, and the channel dimension is quadrupled</center>\n",
    "\n",
    "Following the squeeze operation, there are three more affine coupling layers, this time using channel masking, and again permuting the partitions between each layer.\n",
    "\n",
    "#### Multiscale architecture\n",
    "The final component of the RealNVP framework is the multiscale architecture. With the squeeze operation, the spatial dimensions are downsampled, but the channel dimensions are increased. In order to reduce the overall layer sizes in the deeper layers, dimensions are factored out as latent variables at regular intervals.\n",
    "\n",
    "After one of the blocks of coupling-squeeze-coupling described above, half of the dimensions are factored out as latent variables, while the other half is further processed through subsequent layers. \n",
    "\n",
    "<img src=\"figures/factor_out_latent_variables.png\" alt=\"Multiscale architecture\" style=\"width: 800px;\"/>\n",
    "<center>Example showing how latent variables are factored out in the multiscale architecture. A layer input $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{8\\times 8\\times 2}$ will be reshaped to a $4\\times4\\times8$-shaped tensor after the coupling-squeeze-coupling block. Half of this tensor is absorbed into the base distribution as a latent variable $\\mathbf{z}^{(k+1)}\\in\\mathbb{R}^{4\\times 4\\times 4}$ and the remainder $\\mathbf{h}^{(k+1)}\\in\\mathbb{R}^{4\\times 4\\times 4}$ is processed through further layers of the network</center>\n",
    "\n",
    "The complete RealNVP model has multiple levels of the multiscale architecture. This results in latent variables that represent different scales of features in the model. After a number of these levels, the final scale does not use the squeezing operation, and instead applies four affine coupling layers with alternating checkerboard masks to produce the final latent variable.\n",
    "\n",
    "<img src=\"figures/realnvp.png\" alt=\"Multiscale architecture\" style=\"width: 800px;\"/>\n",
    "<center>The end-to-end RealNVP architecture. Each scale consists of a block of 3 coupling layers (with checkerboard mask), squeeze, 3 coupling layers (with channel mask), followed by half of the dimensions factored out as a latent variable. The final scale consists only of 4 coupling layers (with checkerboard mask) to produce the final latent variable</center>\n",
    "\n",
    "The following summarises the forward pass $\\mathbf{z} = f(\\mathbf{x})$ of the overall architecture with $L$ scales. The functions $f^{(1)},\\ldots,f^{(L-1)}$ consist of the coupling-squeeze-coupling block, whereas the function $f^{(L)}$ consists of 4 coupling layers with checkerboard masks.\n",
    "\n",
    ">\n",
    ">$$\\begin{align}\\mathbf{h}^{(0)}&=\\mathbf{x}\\\\ (\\mathbf{z}^{(k+1)}, \\mathbf{h}^{(k+1)})&=f^{(k+1)}(\\mathbf{h}^{(k)}),\\qquad k=0,\\ldots, L-2\\\\ \\mathbf{z}^{(L)}&= f^{(L)}(\\mathbf{h}^{(L-1)})\\\\\n",
    "\\mathbf{z} &= (\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(L)})\\end{align}$$\n",
    ">\n",
    "\n",
    "The latent variables factored out at each scale are reshaped and concatenated to produce a single latent variable $\\mathbf{z} = (\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(L)})$, which is assumed to be distributed according to a known base distribution (e.g. a diagonal Gaussian).\n",
    "\n",
    "As a final note, the architecture described in this section was further developed with the Glow model ([Kingma and Dhariwal 2018](#Kingma18)), where the checkerboard and channel-wise masking was replaced with 1x1 convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"bijector_subclassing\"></a>\n",
    "## Bijector subclassing\n",
    "\n",
    "In this section we will build a partial implementation of the RealNVP architecture. In particular, we will use bijector subclassing to implement the affine coupling layer as a bijector object, using a binary mask:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} &= b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})) & \\text{(forward pass)}\\\\\n",
    "\\mathbf{x} &= b\\odot \\mathbf{z} + (1-b)\\odot((\\mathbf{z}-t(b\\odot\\mathbf{z}))\\odot \\exp(-s(b\\odot \\mathbf{z}))) & \\text{(inverse pass)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AffineCouplingLayer class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a `shift_and_log_scale_fn` with the following structure:\n",
    "\n",
    "<img src=\"figures/shift_and_log_scale_fn.png\" alt=\"Shift and log-scale network\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example shift_and_log_scale_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a binary mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the AffineCouplingLayer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Use the `forward_log_det_jacobian` and `inverse_log_det_jacobian` methods to compute the log Jacobian determinant on some dummy inputs. Verify that the `forward_log_det_jacobian` method gives the same as first computing the forward transformation, and then taking the negative of the `inverse_log_det_jacobian` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two moons dataset\n",
    "We will now create a normalising flow using the `AffineCouplingLayer` and train it on a two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(train_data[:1000, 0], train_data[:1000, 1], alpha=0.2)\n",
    "plt.title(\"Two moons data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train the normalising flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bijectors chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformed distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class SaveSamples(Callback):\n",
    "    \n",
    "    def __init__(self, plot_folder='./plots', **kwargs):\n",
    "        super(SaveSamples, self).__init__(**kwargs)\n",
    "        self.plot_folder = plot_folder\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def plot(self, num):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        ax = plt.gca()\n",
    "        plt.xlim([-1.5, 2.5])\n",
    "        plt.ylim([-1, 1.5])\n",
    "        ax.set_aspect('equal')\n",
    "        samples = flow.sample(2000)\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./plots/{:05d}.png\".format(num))\n",
    "        plt.close()\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not os.path.exists(self.plot_folder):\n",
    "            os.makedirs(self.plot_folder)\n",
    "        self.iteration = 0\n",
    "        self.plot(self.iteration + 1)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.iteration += 1\n",
    "        if self.iteration % 30 == 0:\n",
    "            self.plot((self.iteration // 30) + 1)\n",
    "        \n",
    "save_samples = SaveSamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.ylabel(\"NLL\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a movie file\n",
    "\n",
    "! ffmpeg -i ./plots/%05d.png -c:v libx264 -vf fps=10 -pix_fmt yuv420p -start_number 00000 samples.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"samples.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model transformations\n",
    "\n",
    "samples = flow.sample(2000)\n",
    "noise = realnvp_bijector.inverse(samples)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15,10))\n",
    "h = noise\n",
    "for i, (bij, ax) in enumerate(zip(bijectors, axs.flat)):\n",
    "    ax.scatter(h[:, 0], h[:, 1], alpha=0.1)\n",
    "    ax.set_title(f\"After {i} steps of flow\")\n",
    "    h = bij.forward(h)\n",
    "axs[2, 2].scatter(samples[:, 0], samples[:, 1], alpha=0.1)\n",
    "axs[2, 2].set_title(\"Model samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "filelist = [ f for f in os.listdir('./plots') if f.endswith(\".png\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./plots', f))\n",
    "if os.path.exists('samples.mp4'):\n",
    "    os.remove('samples.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Try re-running the two moons example again, but using a bi-modal base distribution. Is the flow able to more easily approximate the two moons distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Abdelhamed19\"></a>\n",
    "* Abdelhamed, A., Brubaker, M. A. & Brown, M. S. (2019), \"Noise flow: Noise modeling with conditional normalizing flows\", in *Proceedings of the IEEE International Conference on Computer Vision*, 3165–3173.\n",
    "<a class=\"anchor\" id=\"Dinh15\"></a>\n",
    "* Dinh, L., Krueger, D. & Bengio, Y. (2015),\"NICE: Non-linear Independent Components Estimation\", in *3rd International Conference on Learning Representations, (ICLR)*, San Diego, CA, USA, May 7-9, 2015.\n",
    "<a class=\"anchor\" id=\"Dinh17\"></a>\n",
    "* Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2017), \"Density estimation using Real NVP\",  in *5th International Conference on Learning Representations, (ICLR)*, Toulon, France, April 24-26, 2017.\n",
    "<a class=\"anchor\" id=\"Ho19\"></a>\n",
    "* Ho, J., Chen, X., Srinivas, A., Duan, Y., & Abbeel, P. (2019), \"Flow++: Improving flow-based generative models with variational dequantization and architecture design\", in *Proceedings of the 36th International Conference on Machine Learning, ICML*.\n",
    "<a class=\"anchor\" id=\"Kingma18\"></a>\n",
    "* Kingma, D. P. & Dhariwal, P. (2018), \"Glow: Generative Flow with Invertible 1x1 Convolutions\", in *Advances in Neural Information Processing Systems*, **31**, 10215--10224.\n",
    "<a class=\"anchor\" id=\"Kumar19\"></a>\n",
    "* Kumar, M., Babaeizadeh, M., Erhan, D., Finn, C., Levine, S., Dinh, L. & Kingma, D. (2019), \"VideoFlow: A Flow-Based Generative Model for Video\", in *Workshop on Invertible Neural Nets and Normalizing Flows*, ICML, 2019.\n",
    "<a class=\"anchor\" id=\"Larochelle11\"></a>\n",
    "* Larochelle, H. & Murray, I. (2011), \"The Neural Autoregressive Distribution Estimator\", *Proceedings of Machine Learning Research*, **15**, 29-37.\n",
    "<a class=\"anchor\" id=\"Prenger19\"></a>\n",
    "* Prenger, R., Valle, R., & Catanzaro, B. (2019), \"Waveglow: A flow-based generative network for speech synthesis\", in *Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)*, IEEE, 3617-3621.\n",
    "<a class=\"anchor\" id=\"Rezende15\"></a>\n",
    "* Rezende, D. & Mohamed, S. (2015), \"Variational Inference with Normalizing Flows\", in *Proceedings of Machine Learning Research*, **37**, 1530-1538.\n",
    "<a class=\"anchor\" id=\"vandenOord16a\"></a>\n",
    "* van den Oord, A., Kalchbrenner, N. & Kavukcuoglu, K. (2016a), \"Pixel Recurrent Neural Networks\", *Proceedings of Machine Learning Research*, **48**, 1747-1756.\n",
    "<a class=\"anchor\" id=\"vandenOord16b\"></a>\n",
    "* van den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu, K., Vinyals, O. & Graves, A. (2016b), \"Conditional Image Generation with PixelCNN Decoders\", *Advances in Neural Information Processing Systems*, **29**, 4790-4798.\n",
    "<a class=\"anchor\" id=\"vandenOord16c\"></a>\n",
    "* van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. & Kavukcuoglu, K. (2016c), \"WaveNet: A Generative Model for Raw Audio\", arXiv preprint, abs/1609.03499.\n",
    "<a class=\"anchor\" id=\"vandenOord18\"></a>\n",
    "* van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E., Cobo, L., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., & Hassabis, D. (2018), \"Parallel WaveNet: Fast high-fidelity speech synthesis\", in *Proceedings of the 35th International Conference on Machine Learning*, **80**, 3918–3926."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
