{"cells":[{"cell_type":"markdown","metadata":{"id":"GCI6_S0oJzC7"},"source":["# Deep Learning with TensorFlow\n","## Formative assessment\n","### Week 1: Introduction to Deep Learning"]},{"cell_type":"markdown","metadata":{"id":"pyPRKRSHJzDG"},"source":["#### Instructions\n","\n","In this notebook, you will write code to implement a linear regression classifier in TensorFlow. You will implement the analytic solution, as well as a low-level training loop to update parameters using stochastic gradient descent. Finally, you will train a deep learning MLP regression model using the high-level Keras API.\n","\n","Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n","\n","`#### GRADED CELL ####`\n","\n","Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly.\n","\n","#### How to submit\n","\n","Complete all the tasks you are asked for in the notebook. When you have finished and are happy with your code, commit and push your changes to your repository. This will trigger the automated tests, which you will be able to check on GitHub.\n","\n","Make sure not to change the name or location of this notebook within your repository, or the automated tests will not be able to find it.\n","\n","#### Let's get started!\n","\n","We'll start by running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"OODh0ep_JzDJ","executionInfo":{"status":"ok","timestamp":1652306707798,"user_tz":-180,"elapsed":6425,"user":{"displayName":"ABC","userId":"13100262138380101532"}}},"outputs":[],"source":["#### PACKAGE IMPORTS ####\n","\n","# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n","\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","\n","# If you would like to make further imports from Tensorflow, add them here\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wmjerfelJzDN"},"source":["<img src=\"figures/life_expectancy_wikipedia.png\" alt=\"Life expectancy\" style=\"width: 450px;\"/>\n","<center><font style=\"font-size:12px\">source: <a href=https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy>wikipedia</a></font></center>\n","\n","#### The WHO Life Expectancy dataset\n","In this formative assessment, you will use the [WHO Life Expectancy dataset](https://www.kaggle.com/kumarajarshi/life-expectancy-who) from Kaggle. This dataset was collected from the Global Health Observatory (GHO) data repository under the World Health Organization (WHO), for the purpose of health data analysis. The dataset includes multiple factors affecting life expectancy across 133 countries, divided into the broad categories of immunization related factors, mortality factors, economical factors and social factors.\n","\n","Your goal is to use TensorFlow to model the dataset using linear regression and deep MLP networks."]},{"cell_type":"markdown","metadata":{"id":"BfXVJA3RJzDO"},"source":["#### Load and subset the data"]},{"cell_type":"code","execution_count":2,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":378},"id":"SocqMVhJJzDP","executionInfo":{"status":"error","timestamp":1652306737696,"user_tz":-180,"elapsed":676,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"6aa98d3b-6c0f-44e3-bc41-7507748b16fe"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-38d2df0dbd58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this cell to load and describe the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Life Expectancy Data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Life Expectancy Data.csv'"]}],"source":["# Run this cell to load and describe the data\n","\n","df = pd.read_csv(Path(\"./data/Life Expectancy Data.csv\"))\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"o8m6_tiKJzDQ"},"source":["We will work the following columns from the DataFrame:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RH6D9HPJzDS"},"outputs":[],"source":["# This is the list of columns to use from the DataFrame\n","\n","cols = ['Life expectancy ', 'Adult Mortality', 'Alcohol', ' BMI ',\n","        'Polio', 'Total expenditure', 'Diphtheria ', ' HIV/AIDS', \n","        'GDP', 'Income composition of resources', 'Schooling']"]},{"cell_type":"markdown","metadata":{"id":"4U5OQLSdJzDT"},"source":["You should now complete the following function, according to the following specifications:\n","\n","* Extract the columns above from the loaded DataFrame\n","* Remove any rows with `NaN` values\n","* Define a 1-D numpy array using the values in the `Life expectancy ` column. This will be the target variable\n","* Define a 2-D numpy array using the values from all remaining columns. This array should have shape `(num_examples, num_features)`. These will be the input variables\n","* The function should then return the tuple of constant `tf.Tensor` objects `(input_variables, target_variable)` of type `tf.float32`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5Xc9qTRJzDU"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_inputs_and_targets(dataframe, columns):\n","    \"\"\"\n","    This function takes in the loaded DataFrame and column list as above, and extracts the\n","    numpy arrays as described above.\n","    Your function should return a tuple (input_variables, target_variable) of Tensors.\n","    \"\"\"\n","    dataframe = dataframe[columns]\n","    dataframe = dataframe.dropna()\n","    y = tf.constant(dataframe[columns[0]].values, dtype=tf.float32)\n","    X = tf.constant(dataframe[columns[1:]].values, dtype=tf.float32)\n","    return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mfjf__JBJzDV"},"outputs":[],"source":["# Run your function to get the input and target Tensors\n","\n","X, y = get_inputs_and_targets(df, cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-LAEgAFJzDW"},"outputs":[],"source":["# Split the data into training and test sets and standardise the input scales\n","\n","X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2) #, random_state=100)\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","X_train, y_train = tf.constant(X_train), tf.constant(y_train)\n","X_test, y_test = tf.constant(X_test), tf.constant(y_test)"]},{"cell_type":"markdown","metadata":{"id":"FyF-cqRPJzDY"},"source":["#### Linear regression model\n","\n","We will fit a simple model of the form\n","\n","$$\n","y = f_\\theta(\\mathbf{x}) + \\epsilon,\n","$$\n","\n","where $y\\in\\mathbb{R}$ is the target variable, $\\mathbf{x}\\in\\mathbb{R}^{10}$ are the input features, $\\theta\\in\\mathbb{R}^{11}$ are the model parameters, $\\epsilon\\sim\\mathcal{N}(0, 1)$ is the observation noise random variable, and $f_\\theta:\\mathbb{R}^{10}\\mapsto\\mathbb{R}$ is given by\n","\n","$$\n","\\begin{align}\n","f_\\theta(\\mathbf{x}) &= \\theta_0 + \\sum_{m=1}^{10} \\theta_m x_m\\\\\n","&= \\sum_{m=0}^{10} \\theta_m x_m.\n","\\end{align}\n","$$\n","\n","In the second line above we have defined $x_0=1$ to be the constant feature. The maximum likelihood solution is given by the normal equation\n","\n","$$\n","\\theta_{ML} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y},\n","$$\n","\n","where $\\mathbf{X}\\in\\mathbb{R}^{N\\times M}$ is the data matrix, $\\mathbf{y}\\in\\mathbb{R}^N$ are the targets, $N$ is the number of data examples, and $M$ are the number of features (including the constant feature).\n","\n","You should now complete the following function to implement the normal equation to compute the maximum likelhood solution. Your code should only use TensorFlow functions. \n","\n","* The arguments to the function are an `inputs` Tensor of shape `(num_examples, num_features)`, and a `targets` Tensor of shape `(num_examples,)`\n","* The function should add a column of ones as the first column to the `inputs` Tensor for the constant feature\n","* The function should output a 1-D Tensor of parameters of length `(num_features + 1,)` (the first entry will be the bias)\n","\n","_Hint: check [the docs](https://www.tensorflow.org/api/stable) for relevant TensorFlow functions, including the_ [`tf.linalg`](https://www.tensorflow.org/api_docs/python/tf/linalg) _module._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"724zZLNWJzDY"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def normal_equation(inputs, targets):\n","    \"\"\"\n","    This function takes in inputs and targets Tensors, and implements the normal equation\n","    as above, only using TensorFlow functions.\n","    Your function should return a Tensor for the maximum likelihood solution for the parameters.\n","    \"\"\"\n","    N = inputs.shape[0]\n","    inputs = tf.concat((tf.ones((N, 1), dtype=tf.float32), inputs), axis=1)\n","    Xt = tf.transpose(inputs)\n","    XtX = tf.linalg.matmul(Xt, inputs)\n","    XtXinv = tf.linalg.inv(XtX)\n","    Xty = tf.tensordot(Xt, targets, axes=1)\n","    return tf.tensordot(XtXinv, Xty, axes=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NT-LI9iJzDZ"},"outputs":[],"source":["# Run your function to compute the ML estimate\n","\n","theta_ml = normal_equation(X_train, y_train)\n","bias_ml, weights_ml = theta_ml[0], theta_ml[1:]\n","print(\"MLE weights:\")\n","print(weights_ml)\n","print(\"MLE bias:\")\n","print(bias_ml)"]},{"cell_type":"markdown","metadata":{"id":"dnp9d9dpJzDa"},"source":["#### Stochastic gradient descent\n","\n","You will now implement the stochastic gradient descent (SGD) algorithm to find the MLE using optimization. To do this, you will make use of the `tf.Variable` class. Recall that a Variable object is a special kind of Tensor that is _mutable_, so we will use it for the model parameters.\n","\n","First, you should complete the following `get_variables` function to create Variable objects for the weights and bias of the linear regression model, as well as an iteration counter Variable.\n","\n","* The function takes `num_features` as an argument\n","* The bias should be a `tf.Variable` with scalar shape, type `tf.float32`, and an initial value of zero. Set the name argument of this Variable to `\"bias\"`\n","* The weights should be a 1-D `tf.Variable` of length `num_features`, type `tf.float32`, and with initial values sampled from a standard normal distribution. Set the name argument of this Variable to `\"weights\"`\n","* Both weights and bias Variables should be trainable\n","* Finally, the function should create a scalar Variable of type `tf.int32`, initialised to zero, with name argument set to `\"iteration\"`. This Variable should be non-trainable\n","* The function should return the tuple of Variables `(weights, bias, iteration)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvPe--ReJzDb"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def get_variables(num_features):\n","    \"\"\"\n","    This function takes in the number of features as an argument, and creates tf.Variable objects\n","    for the linear regression model weights and bias, as well as an iteration counter Variable.\n","    Your function should return a tuple of two tf.Variable objects (weights, bias, iteration).\n","    \"\"\"\n","    bias = tf.Variable(0., dtype=tf.float32, name='bias')\n","    weights = tf.Variable(tf.random.normal((num_features,)), dtype=tf.float32, name='weights')\n","    iteration = tf.Variable(0, dtype=tf.int32, name='iteration', trainable=False)\n","    return weights, bias, iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VARVKpY1JzDb"},"outputs":[],"source":["# Run your function to create the Variables\n","\n","weights, bias, iteration = get_variables(num_features=10)"]},{"cell_type":"markdown","metadata":{"id":"HpZrym8IJzDc"},"source":["Now define the model itself by completing the following function. This function implements $f_\\theta(\\mathbf{x}) = \\theta_0 + \\sum_{m=1}^{10} \\theta_m x_m$ as above.\n","\n","* The function takes an `inputs` Tensor, `weights` and `bias` Variables as input\n","* The `inputs` Tensor could be a batch of inputs of shape `(batch_size, num_features)`, or a single set of inputs of shape `(num_features,)`\n","* The function should return the output Tensor $f_\\theta(\\mathbf{x})$\n","* The output Tensor should have shape `(batch_size,)` (if passed a batch of inputs), or else should be a scalar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DScOuOlUJzDc"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def f(inputs, weights, bias):\n","    \"\"\"\n","    This function takes in an inputs Tensor, weights and bias Variables. It should compute and \n","    return the output Tensor prediction. \n","    \"\"\"\n","    return bias + tf.tensordot(inputs, weights, axes=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPxTZUUsJzDd"},"outputs":[],"source":["# Test your function on some dummy inputs\n","\n","inputs = tf.random.normal((3, 10), dtype=tf.float32)\n","print(f(inputs, weights, bias))\n","\n","inputs = tf.random.normal((10,), dtype=tf.float32)\n","print(f(inputs, weights, bias))"]},{"cell_type":"markdown","metadata":{"id":"M7Uxrbs4JzDe"},"source":["We will need to define the loss function to optimise. As we have assumed Gaussian noise $\\epsilon\\sim\\mathcal{N}(0, 1)$ and we are looking to find the maximum likelihood solution, this will be the mean squared error loss. Recall that SGD provides a cheaper estimate of the full gradient, by computing the gradient on a minibatch of data points, instead of the full dataset. The loss function that you should implement is therefore:\n","\n","$$\n","\\tilde{L}_{MSE}(\\theta) = \\frac{1}{M} \\sum_{\\mathbf{x}_i, y_i\\in\\mathcal{D}_m} (y_i - \\hat{y}_i)^2\n","$$\n","\n","where $\\hat{y}_i = f_\\theta(\\mathbf{x}_i)$, $(\\mathbf{x}_i, y_i)$ is an example input and output from the randomly sampled minibatch $\\mathcal{D}_m$ of training data points, and $M = |\\mathcal{D}_m|$ is the size of the minibatch. The function specifications are as follows:\n","\n","* The `mse` function takes two Tensors as arguments: `y_true` and `y_pred`\n","* As SGD computes gradients on minibatches, these two Tensors will have shape `(batch_size,)`\n","* The loss function should compute and return the mean squared error loss (MSE) as a scalar Tensor\n","* Use only TensorFlow functions inside your function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZHyG1iuJzDe"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def mse(y_true, y_pred):\n","    \"\"\"\n","    This function takes a batch of 'ground truth' values y_true and a corresponding batch\n","    of model predictions y_pred, and computes the mean squared error.\n","    Your function should return the MSE as a scalar Tensor.\n","    \"\"\"\n","    return tf.reduce_mean(tf.square(y_true - y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIdXM_S4JzDf"},"outputs":[],"source":["# Compute the initial loss on a batch of inputs\n","\n","mse(y_train[:32], f(X_train[:32], weights, bias))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEzp0q31JzDf"},"outputs":[],"source":["# Compute the train and test loss of the MLE\n","\n","print(\"MLE train loss: {}\".format(mse(y_train, f(X_train, weights_ml, bias_ml))))\n","print(\"MLE test loss: {}\".format(mse(y_test, f(X_test, weights_ml, bias_ml))))"]},{"cell_type":"markdown","metadata":{"id":"kXWIXzQ-JzDf"},"source":["The following function implements the update step of SGD, that we will use inside the training loop. Recall this update uses the gradient of the loss with respect to the model parameters to make the update:\n","\n","$$\n","\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_\\theta \\tilde{L}_{MSE}(\\theta_t),\\qquad t\\in\\mathbb{N}_0,\n","$$\n","\n","where $\\eta>0$ is the learning rate.\n","\n","* The `sgd_update` function takes the following arguments:\n","  * `model_fn` is the function that defines the predictive function (the function `f` above)\n","  * `inputs` and `targets` are the minibatch inputs and targets Tensors, of shape `(batch_size, 10)` and `(batch_size,)` respectively\n","  * `w` and `b` are the Variables that represent the model parameters\n","  * The `learning_rate` is the SGD hyperparameter\n","* The function should compute the SGD update step (assuming the mean squared error loss as above), updating the `w` and `b` Variables accordingly, using the `learning_rate` passed in. It will not return anything; the Variables are updated in-place."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atQOxdLlJzDg"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def sgd_update(model_fn, inputs, targets, w, b, learning_rate=0.01):\n","    \"\"\"\n","    This function takes the model function, inputs batch, targets batch, weights Variable,\n","    bias Variable and learning rate as arguments. It should update the Variables w and b\n","    using the SGD update rule above for the MSE loss.\n","    \"\"\"\n","    y_pred = model_fn(inputs, w, b)\n","    error = 2 * tf.expand_dims(y_pred - targets, axis=1)  # (batch_size, 1)\n","    w_grads = inputs * error  # (batch_size, num_features)\n","    b_grad = error\n","    average_w_grad = tf.reduce_mean(w_grads, axis=0)  # (num_features,)\n","    average_b_grad = tf.reduce_mean(b_grad)  # scalar\n","    w.assign_sub(learning_rate * average_w_grad)\n","    b.assign_sub(learning_rate * average_b_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2llFisgBJzDg"},"outputs":[],"source":["# Test your SGD update function\n","\n","print(\"Before the update:\")\n","print(weights)\n","print(bias)\n","sgd_update(f, X_train[:32], y_train[:32], weights, bias, learning_rate=0.05)\n","print(\"\\nAfter the update:\")\n","print(weights)\n","print(bias)"]},{"cell_type":"markdown","metadata":{"id":"lJIxyHd9JzDg"},"source":["You are now ready to write the training loop in the following function. The training loop consists of a pre-defined number of epochs, where one epoch is one complete pass through the training dataset. Within an epoch, there is an inner loop where the algorithm iterates through the training data, pulling out a minibatch of data at each iteration, and using it to update the weights and biases according to the SGD update rule. \n","\n","You should complete the following `training_loop` function according to the specifications:\n","\n","* The function takes the following arguments:\n","  * `num_epochs`: a positive integer that defines the number of epochs to run the training loop\n","  * `model_fn`: as before, the function that defines the predictive function\n","  * `training_data`: a 2-tuple of Tensors `(inputs, targets)` for the complete training data\n","  * `batch_size`: a positive integer that defines the number of examples in each minibatch\n","  * `w`: the Variable that represents the model weights\n","  * `b`: the Variable that represents the model bias\n","  * `iteration`: a Variable used for counting the total number of updates\n","  * `mse`: the loss function to evaluate the model (this will be your `mse` function above)\n","  * `sgd_update`: the function that implements the SGD update (this will be your `sgd_update` function above)\n","  * `learning_rate`: the learning rate for the SGD update\n","* The function should iterate through the training data `num_epochs` times\n","* On each pass through the training data, the function should extract `batch_size` examples from the inputs and targets provided in `training_data`\n","  * The minibatches should be pulled from the training data in sequence. That is, the first minibatch will be the first `batch_size` elements of the inputs and targets, the second minibatch will be the next `batch_size` elements, and so on. Bear in mind that the last minibatch of the epoch may be a different size\n","  * At each iteration, the `iteration` Variable should be incremented by one\n","* For each minibatch, the parameters `w` and `b` should be updated according to `sgd_update`, using the `learning_rate` provided\n","* After every update, the model loss should be evaluated on the current minibatch using the `mse` function and appended to a list as a scalar float\n","* The list of losses should then be returned by the function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NR8iQd8WJzDh"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def training_loop(num_epochs, model_fn, training_data, batch_size, w, b, iteration, \n","                  mse=mse, sgd_update=sgd_update, learning_rate=0.01):\n","    \"\"\"\n","    This function executes the training loop according to the specifications above. \n","    It should run for num_epochs passes through the training data, updated the model\n","    parameters using the sgd_update function at every iteration.\n","    The function should return the list of losses computed on every minibatch at each iteration,\n","    using the mse function.\n","    \"\"\"\n","    train_inputs, train_targets = training_data\n","    num_train_examples = train_inputs.shape[0]\n","    if num_train_examples % batch_size == 0:\n","        iterations_per_epoch = num_train_examples // batch_size\n","    else:\n","        iterations_per_epoch = num_train_examples // batch_size + 1\n","    losses = []\n","    for epoch in range(num_epochs):\n","        print(\"Epoch {}\".format(epoch))\n","        for i in range(iterations_per_epoch):\n","            iteration.assign_add(1)\n","            minibatch_inputs = train_inputs[i*batch_size: (i+1)*batch_size]\n","            minibatch_targets = train_targets[i*batch_size: (i+1)*batch_size]\n","            sgd_update(model_fn, minibatch_inputs, minibatch_targets, w, b, learning_rate)\n","            losses.append(mse(model_fn(minibatch_inputs, w, b), minibatch_targets).numpy())\n","    print(\"Training completed!\")\n","    return losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZ86-tWoJzDh"},"outputs":[],"source":["# Re-initialise the model parameters and run the training loop\n","\n","weights, bias, iteration = get_variables(num_features=10)\n","losses = training_loop(20, f, (X_train, y_train), 128, weights, bias, iteration=iteration, \n","                       mse=mse, sgd_update=sgd_update, learning_rate=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF189xImJzDh"},"outputs":[],"source":["# Plot the losses\n","\n","plt.plot(losses)\n","plt.title(\"Loss vs iterations\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"MSE loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eamMc7exJzDi"},"outputs":[],"source":["# Compute the train and test loss of the learned weights\n","\n","print(\"Model train loss: {}\".format(mse(y_train, f(X_train, weights, bias))))\n","print(\"Model test loss: {}\".format(mse(y_test, f(X_test, weights, bias))))"]},{"cell_type":"markdown","metadata":{"id":"TFyg0WKaJzDi"},"source":["Compare your learned weights and bias to the exact solution computed earlier. They should be fairly close:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZyzZzPoJzDi"},"outputs":[],"source":["# Print the learned weights and bias\n","\n","print(\"Learned weights:\")\n","print(weights.numpy())\n","print(\"Learned bias:\")\n","print(bias.numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmDd8oxQJzDj"},"outputs":[],"source":["# Print the exact weights and bias\n","\n","print(\"Exact ML weights:\")\n","print(weights_ml.numpy())\n","print(\"Exact ML bias:\")\n","print(bias_ml.numpy())"]},{"cell_type":"markdown","metadata":{"id":"_ZlsjQPJJzDj"},"source":["#### MLP with Keras API\n","\n","In the final part of this assignment, you will use the Keras API to build an MLP model to fit the data.\n","\n","First, we will see how the linear regression model above can be implemented much quicker using the `Sequential` class.\n","\n","In the following function, you should build a `Sequential` model with just one `Dense` layer, which has a single output unit, and no activation function. This is the same as the linear regression model above.\n","\n","* The function takes the `input_shape` as an argument, which should be used in the `Dense` layer initializer to specify the input shape\n","* The function should build and return the `Sequential` object with one Dense layer with a single neuron, and no activation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlLTvG9xJzDk"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def sequential_linear_regression(input_shape):\n","    \"\"\"\n","    This function takes the input_shape as argument to build a Sequential model as \n","    specified above. \n","    The function should then return the Sequential model.\n","    \"\"\"\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(1, activation=None, input_shape=input_shape)\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exqWaB95JzDk"},"outputs":[],"source":["# Run your function to build the model and print the model summary\n","\n","model = sequential_linear_regression(input_shape=(10,))\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"IsFDPbbLJzDl"},"source":["You should now compile and fit the model to the training data. \n","\n","* The following function takes the following arguments:\n","  * `sequential_model`: a Sequential model to fit to the training data\n","  * `num_epochs`: a positive integer that defines the number of epochs to train the model\n","  * `training_data`: a 2-tuple of Tensors (inputs, targets) for the complete training data\n","  * `batch_size`: a positive integer that defines the number of examples in each minibatch\n","* The function should compile the model with the mean squared error loss and the SGD optimizer\n","* The function should then fit the model to the training data for `num_epochs` epochs and save the returned history object\n","* Your function should then return the history object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uedH3bbkJzDl"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def compile_and_fit(sequential_model, num_epochs, training_data, batch_size):\n","    \"\"\"\n","    This function should compile and fit the sequential_model as described above. \n","    The function should then return the history object that is returned from the fit method.\n","    \"\"\"\n","    X_train, y_train = training_data\n","    sequential_model.compile(loss='mse', optimizer='sgd')\n","    history = sequential_model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okqUu1L4JzDl"},"outputs":[],"source":["# Run your function to compile and fit the model\n","\n","history = compile_and_fit(model, num_epochs=20, training_data=(X_train, y_train), batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8J3mt9LJzDl"},"outputs":[],"source":["# Plot the losses\n","\n","plt.plot(history.history['loss'])\n","plt.title(\"Loss vs epochs\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"MSE loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpU4udzKJzDm"},"outputs":[],"source":["# Compute the train and test loss of the Sequential model\n","\n","print(\"Model train loss: {}\".format(model.evaluate(X_train, y_train, verbose=0)))\n","print(\"Model test loss: {}\".format(model.evaluate(X_test, y_test, verbose=0)))"]},{"cell_type":"markdown","metadata":{"id":"yHFYJj06JzDm"},"source":["Compare your model's weights and bias to the exact ML solution:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzSv0-1tJzDm"},"outputs":[],"source":["# Print the model's weights and bias\n","\n","print(\"Learned weights:\")\n","print(model.layers[0].kernel.numpy())\n","print(\"Learned bias:\")\n","print(model.layers[0].bias.numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1E2U2CNJzDm"},"outputs":[],"source":["# Print the exact weights and bias\n","\n","print(\"Exact ML weights:\")\n","print(weights_ml.numpy())\n","print(\"Exact ML bias:\")\n","print(bias_ml.numpy())"]},{"cell_type":"markdown","metadata":{"id":"Wts5JtRUJzDn"},"source":["Let's see if we can improve the model's performance by increasing its capacity. \n","\n","You should now complete the following function to build, compile and fit a new Sequential model.\n","\n","* This function takes the following arguments:\n","  * `input_shape`: to use in the first layer of the model to set the input shape\n","  * `num_epochs`: a positive integer that defines the number of epochs to train the model\n","  * `training_data`: a 2-tuple of Tensors (inputs, targets) for the complete training data\n","  * `batch_size`: a positive integer that defines the number of examples in each minibatch\n","* This `Sequential` model should use two `Dense` layers:\n","  * The first `Dense` layer will be a hidden layer with 16 units and a sigmoid activation function\n","  * The output layer will again be a `Dense` layer with a single neuron and no activation function\n","* You should again compile your model with the mean squared error loss function and SGD optimizer\n","* You should again fit your model for `num_epochs` epochs with a batch size of `batch_size` on the `training_data`\n","* Your function should return a tuple containing the model and the history object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wbb7pOtdJzDn"},"outputs":[],"source":["#### GRADED CELL ####\n","\n","# Complete the following function. \n","# Make sure to not change the function name or arguments.\n","\n","def build_and_train_mlp(input_shape, num_epochs, training_data, batch_size):\n","    \"\"\"\n","    This function takes the input_shape, num_epochs, training_data tuple and batch_size\n","    as arguments. It should build, compile and fit the MLP model as described above.\n","    The function should then return the tuple (mlp_model, history)\n","    \"\"\"\n","    X_train, y_train = training_data\n","    mlp_model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(16, activation='sigmoid', input_shape=input_shape),\n","        tf.keras.layers.Dense(1)\n","    ])\n","    mlp_model.compile(loss='mse', optimizer='sgd')\n","    history = mlp_model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n","    return mlp_model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ctRdtSX8JzDn"},"outputs":[],"source":["# Run your function to build and train the MLP model\n","\n","mlp_model, history = build_and_train_mlp(input_shape=(10,), num_epochs=20, \n","                                         training_data=(X_train, y_train), batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"aLdXpD5TJzDn"},"outputs":[],"source":["# Plot the losses\n","\n","plt.plot(history.history['loss'])\n","plt.title(\"Loss vs epochs\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"MSE loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"NJSzzC6qJzDo"},"outputs":[],"source":["# Compute the train and test loss of the MLP\n","\n","print(\"Model train loss: {}\".format(mlp_model.evaluate(X_train, y_train, verbose=0)))\n","print(\"Model test loss: {}\".format(mlp_model.evaluate(X_test, y_test, verbose=0)))"]},{"cell_type":"markdown","metadata":{"id":"KKvm3N42JzDo"},"source":["Did the model performance improve? \n","\n","Further gains could be made by training the model for longer, and/or increasing its capacity further. However, we need to be aware of overfitting and should then use a held-out validation set for model selection. \n","\n","In the next week of the module we will see how to properly validate our models, as well as regularisation methods to combat overfitting. We will also expand our options for network optimisation, including studying the all-important backpropagation algorithm, and further develop our skills with TensorFlow."]}],"metadata":{"kernelspec":{"display_name":"dltf","language":"python","name":"dltf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Week 1 - Formative assessment_ANSWERS(1) (2).ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}