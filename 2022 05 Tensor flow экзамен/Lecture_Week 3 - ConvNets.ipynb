{"cells":[{"cell_type":"markdown","metadata":{"id":"p2POPCVwERsL"},"source":["# Deep Learning with TensorFlow\n","### Week 3: ConvNets"]},{"cell_type":"markdown","metadata":{"id":"OJH4vxGWERsU"},"source":["## Contents\n","\n","[1. Introduction](#introduction)\n","\n","[2. Convolutional neural networks](#convnets)\n","\n","[3. CNNs and feature maps (\\*)](#cnnsfeaturemaps)\n","\n","[4. Padding and strides](#paddingstrides)\n","\n","[References](#references)"]},{"cell_type":"markdown","metadata":{"id":"bi5vMl3tERsW"},"source":["<a class=\"anchor\" id=\"introduction\"></a>\n","## Introduction\n","\n","The last week of the module focused on optimisation and regularisation of deep learning models. You learned about several techniques and concepts that are essential for successfully training large deep learning models in practice, including optimiser algorithms, weight regularisation, dropout, early stopping and batch normalisation. You also worked through the derivation of the backpropagation algorithm for multilayer perceptrons, which is an efficient algorithm for computing gradients of a loss function with respect to the model parameters. \n","\n","You also learned some lower-level functionalities of TensorFlow, which increase the flexibility of the models and the training routine. These included how to use `tf.GradientTape` context for automatic differentation in TensorFlow, and using it to implement a custom training loop. You saw how callbacks can be used to perform actions depending on how the training progresses, such as early stopping or model saving. You also learned how to implement custom callbacks and custom layers in a model.\n","\n","This week of the module focuses on another important model architecture. The multilayer perceptron is a general deep learning model architecture, which makes use of several fully connected (dense) layers. However it is often not the optimal choice, and in this week we will see how to build convolutional neural networks (CNNs).\n","\n","We will also see how to implement this architecture in TensorFlow, either using the Sequential API or functional API."]},{"cell_type":"markdown","metadata":{"id":"U7-Z63gpERsX"},"source":["<a class=\"anchor\" id=\"convnets\"></a>\n","## Convolutional neural networks\n","\n","A convolutional neural network (CNN) is a type of neural network with a special structure. It can be seen as a special case of the multilayer perceptron architecture that builds certain assumptions into the design of the model, in particular using **local connectivity** and **equivariance**.\n","\n","An important motivating application for CNNs is **computer vision**, as the architectural design of these networks mimics the visual system, where neurons respond to stimulus in a restricted region of the visual field ([Hubel 1959](#Hubel59)). This concept led initially to the development of the neocognitron ([Fukushima 1980](#Fukushima80)), and later to the modern convolutional neural network trained by backpropagation ([LeCun et al 1989](#LeCun89)). \n","\n","In this exposition, we will focus on CNNs for image processing, using 2-D convolutions. However, convolutional neural networks have also been very successful when applied to time series data, using 1-D convolutions. They can also be applied to 3-D image processing tasks, or video analysis, with 3-D convolutions.\n","\n","#### The convolution operation\n","The convolution operation for two (Lebesgue integrable) functions $h$ and $k$ is defined as\n","\n","$$\n","(h * k)(t) = \\int_{-\\infty}^\\infty h(\\tau)k(t - \\tau)d\\tau.\n","$$\n","\n","It can be described as the weighted average of the function $h$ according to the weighting function (or **kernel**) $k$ at each point in time $t$. As $t$ changes, the weighting function emphasises different parts of the input function $h$.\n","\n","In practice (and in the context of CNNs), we need to discretise the data, and work instead with discrete convolutions:\n","\n","$$\n","(h * k)(t) = \\sum_{\\tau=-\\infty}^\\infty h(\\tau)k(t - \\tau).\n","$$\n","\n","In this case, we can assume that both $h$ and $k$ (and the convolution $(h * k)$) take integer arguments. In addition, when the kernel function $k$ has finite support, we can write the above as a finite summation.\n","\n","In convolutional neural networks with image inputs, a 2-D discrete convolution is used:\n","\n","$$\n","(\\mathbf{h} * \\mathbf{k})(i, j) = \\sum_{m, n} h(m, n) k(i - m, j - n).\n","$$\n","\n","In the above, we will consider $h(i, j)$ and $k(i, j)$ to denote the $(i, j)$-th elements of the matrices $\\mathbf{h}\\in\\mathbb{R}^{n_h\\times n_w}$ and $\\mathbf{k}\\in\\mathbb{R}^{k_h\\times k_w}$ respectively, where $n_h$ and $n_w$ are the image height and width in pixels, and $k_h$ and $k_w$ are the kernel height and width in pixels. We will use bold lower case for the matrices $\\mathbf{h}$ and $\\mathbf{k}$ for consistency of notation with previous sections.\n","\n","In practice, many libraries implement the **cross-correlation** operation, which is the same as above but changing the orientation of the arguments:\n","\n","$$\n","(\\mathbf{h} * \\mathbf{k})(i, j) = \\sum_{m, n} h(i + m, j + n) k(m, n). \\label{crosscorrelation}\\tag{1}\n","$$\n","\n","We will refer to the above operation in CNNs as a **convolution**.\n","\n","As an example, consider a grayscale image $\\mathbf{x} =: \\mathbf{h}^{(0)}\\in\\mathbb{R}^{7\\times 7}$ with height and width $n_h=n_w=7$, as illustrated in the following figure.\n","\n","<img src=\"figures/1_channel_no_padding_crop.png\" alt=\"Input grayscale image\" style=\"width: 550px;\"/>\n","<center>Pixels in a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7}$</center>\n","\n","Suppose we define a kernel matrix $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$. The operation \\eqref{crosscorrelation} can be visualised in the following animation, as sweeping the convolutional kernel $\\mathbf{k}$ across the input image. At each step, the weights of the kernel matrix $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$ are multiplied pointwise by the values of the pixels outlined in red and summed together to produce the pre-activation of the neuron in the next hidden layer.\n","\n","<img src=\"figures/1_channel_no_padding.gif\" alt=\"2D convolution on a single channel input\" style=\"width: 600px;\"/>\n","<center>2-D convolution with kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7}$</center>\n","\n","In convolutional neural networks, the convolutional layers usually consist of the operation \\eqref{crosscorrelation} plus a bias term, followed by a pointwise activation function:\n","\n","$$\n","\\mathbf{h}^{(k)} = \\sigma\\left((\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)}) + b^{(k-1)}\\right). \\label{convlayer}\\tag{2}\n","$$\n","\n","In the above, the superscript $(k)$ indexes the layer as before, and the bias $b^{(k-1)}\\in\\mathbb{R}$ is added pointwise to the output of the convolution operation $(\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)})\\in\\mathbb{R}^{(n_h-k_h+1)\\times (n_w-k_w+1)}$.\n","\n","Different values of the kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$ will pick out different features in the input $\\mathbf{h}^{(k-1)}$. The output of \\eqref{convlayer} is sometimes referred to as a **feature map**, and the kernel $\\mathbf{k}$ is also referred to as a **filter**.\n","\n","Note that the operation described above introduces a translational **equivariance** property in convolutional layers. That is, if the input image is translated, then the activations in the next hidden layer are also translated accordingly. Another way to view this is that the convolutional kernel searches for the same features across the input image.\n","\n","#### Multi-channel inputs and outputs\n","The operations \\eqref{crosscorrelation} and \\eqref{convlayer} can easily be extended to inputs with multiple channels. Consider, for example, an input RGB image, which has three channel values per pixel.\n","\n","<img src=\"figures/3_channel_no_padding_crop.png\" alt=\"Input RGB image\" style=\"width: 550px;\"/>\n","<center>Pixels in an RGB input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3}$</center>\n","\n","The input is now a rank-3 tensor $\\mathbf{x} = \\mathbf{h}^{(0)}\\in\\mathbb{R}^{7\\times 7\\times 3}$, and correspondingly we require a rank-3 kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{k_h\\times k_w\\times 3}$. For illustration we will again choose $k_h=3$, $k_w=4$. The operation \\eqref{crosscorrelation} now becomes\n","\n","$$\n","(\\mathbf{h} * \\mathbf{k})(i, j) = \\sum_{m, n, p} h(i + m, j + n, p) k(m, n, p).\n","$$\n","\n","This is visualised in the following animation, where the kernel $\\mathbf{k}$ again sweeps over the input image, this time multiplying a $3\\times 4\\times 3$ block of input pixels (outlined in red) elementwise by the values of the kernel tensor $\\mathbf{k}$, and summing the results to produce the output neuron pre-activation.\n","\n","<img src=\"figures/3_channel_no_padding.gif\" alt=\"2D convolution on a 3-channel (RGB) input\" style=\"width: 550px;\"/>\n","<center>2-D convolution with kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 3}$ operating on an RGB input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3}$</center>\n","\n","In convolutional layers, many filters are stacked on top of each other, so as to produce a multichannel output. In practice, we implement this with a rank-4 kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{k_h, k_w, c_{in}, c_{out}}$, where $c_{in}$ are the number of channels in the input, and $c_{out}$ are the number of channels in the output:\n","\n","$$\n","(\\mathbf{h} * \\mathbf{k})(i, j, q) = \\sum_{m, n, p} h(i + m, j + n, p) k(m, n, p, q).\n","$$\n","\n","This is visualised in the following animation, for an input $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3}$ and kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 3\\times 2}$.\n","\n","<img src=\"figures/3_2_channel_no_padding.gif\" alt=\"2D convolution on an 3-channel (RGB) input with 2 filters\" style=\"width: 550px;\"/>\n","<center>2-D convolution with kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 3}$ operating on an RGB input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3\\times 2}$ with 2 filters</center>\n","\n","The convolutional layer operation \\eqref{convlayer} now becomes the following\n","\n","$$\n","\\mathbf{h}^{(k)} = \\sigma\\left((\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)}) + \\mathbf{b}^{(k-1)}\\right),\\label{convlayer_multichannel}\\tag{3}\n","$$\n","\n","where again the superscript $(k)$ indexes the layer, and the bias $\\mathbf{b}^{(k-1)}\\in\\mathbb{R}^{c_{out}}$ is added pixel-wise to the output of the convolution operation $(\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)})\\in\\mathbb{R}^{(n_h-k_h+1)\\times (n_w-k_w+1)\\times c_{out}}$.\n","\n","#### Pooling layers\n","In many CNN models, convolutional layers are alternated with pooling layers. Pooling layers downsample the spatial dimensions of a layer by computing a summary statistic of (often non-overlapping) regions of the input layer's post-activations.\n","\n","A typical pooling layer type is the **max pooling** layer ([Zhou & Chellappa 1988](#Zhou88)) which takes the maximum activation in a region as the single neuron activation for that region. For example, we could divide the input layer into $2\\times2$ squares and take the maximum value for each square. This results in halving the spatial dimensions of the following layer.\n","\n","<img src=\"figures/max_pooling.png\" alt=\"Max pooling\" style=\"width: 550px;\"/>\n","<center>Max pooling using $2\\times2$ pooling windows</center>\n","\n","Pooling operations are usually performed separately for each input channel, so that the spatial dimensions are downsampled, but the channel dimensions stay the same. Other common pooling operations include average pooling, or computing the $\\mathcal{l}^2$ norm of the pixel values within each input region."]},{"cell_type":"markdown","metadata":{"id":"EFRoe4CNERsd"},"source":["<a class=\"anchor\" id=\"cnnsfeaturemaps\"></a>\n","## CNNs and feature maps\n","\n","In this section we will use the `Conv2D` and `MaxPool2D` layer to implement the convolution and pooling operations described above, and see how these easily fits into our existing model-building workflow.\n","\n","We will also see the effect of different kernel tensor choices on the output feature maps, and look at more complex feature maps from a pre-trained model."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lBM8wMjcERse","executionInfo":{"status":"ok","timestamp":1652305385234,"user_tz":-180,"elapsed":6027,"user":{"displayName":"ABC","userId":"13100262138380101532"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"tbK2a0J-ERsh"},"source":["The `Conv2D` and `MaxPool2D` classes are imported from the `tf.keras.layers` module just as the `Flatten` and `Dense` layers we have already worked with. Note that there are also 1-D and 3-D variants of these layers available, which both work in a similar way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6Ja69XOERsi"},"outputs":[],"source":["# Define a dummy model with Conv2D and MaxPool2D layers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwResLF5ERsk"},"outputs":[],"source":["# Print the model summary\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEtYCIC-ERsl"},"outputs":[],"source":["# Inspect the layer variables' shapes\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hZOoLVYGERsm"},"source":["#### Edge detection filters\n","The kernels (or filters) in CNNs are typically learned with backpropagation. However, simple low-level features such as edge detection kernels can also be designed by hand. In this section we will see the output of such low-level kernels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfM85JhjERsm"},"outputs":[],"source":["# Define a simple model with a Conv2D layer\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XKYkOuEzERsn"},"source":["A shape dimension of `None` indicates that the model can take flexible input sizes in this dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcBVWxuNERso"},"outputs":[],"source":["# Inspect the model's weights\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ogbmzoz2ERso","executionInfo":{"status":"error","timestamp":1652305389167,"user_tz":-180,"elapsed":664,"user":{"displayName":"ABC","userId":"13100262138380101532"}},"outputId":"9597d9d0-18e0-4efb-a73d-ec4892806580","colab":{"base_uri":"https://localhost:8080/","height":396}},"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-116dc6f63efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./figures/oscar.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return read_file_eager_fallback(\n\u001b[0;32m--> 567\u001b[0;31m           filename, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    588\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 590\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    591\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     _execute.record_gradient(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: ./figures/oscar.png; No such file or directory [Op:ReadFile]"]}],"source":["# Load an image as grayscale\n","\n","import matplotlib.pyplot as plt\n","\n","image = tf.io.read_file(\"./figures/oscar.png\")\n","image = tf.io.decode_png(image, channels=1)\n","plt.figure(figsize=(8, 6))\n","plt.imshow(image, cmap='gray')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9woSHLCBERsp"},"source":["A simple and intuitive edge detection kernel is the [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qasfA7IiERsq"},"outputs":[],"source":["# Define simple edge detection filters\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aJWHNR2ERsq"},"outputs":[],"source":["# Set the model kernel\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hn_odqCjERsq"},"outputs":[],"source":["# Compute the feature maps\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EQpezNrERsr"},"outputs":[],"source":["# View the image and feature map\n","\n","fig = plt.figure(figsize=(17, 6))\n","fig.add_subplot(121)\n","plt.imshow(image, cmap='gray')\n","plt.axis('off')\n","fig.add_subplot(122)\n","plt.imshow(gx, cmap='gray')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xA8q0QlcERsr"},"source":["#### Extract learned features from a pre-trained model\n","In this section we will load a CNN model that has been pre-trained on the [ImageNet](http://www.image-net.org) dataset, which is a large scale image classification dataset which to date has over 20,000 categories and over 14 million images. Large deep learning models trained on this dataset tend to learn general, useful representations of image features that can be used for a range of image processing tasks.\n","\n","Below we will load the VGG-19 model ([Simonyan & Zisserman 2015](#Simonyan15)), which is available to load as a pre-trained model in the [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) module. This might take a minute or two to download the first time you run the cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rqIai3WERss"},"outputs":[],"source":["# Load the VGG-19 model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zh9XJmhtERss"},"outputs":[],"source":["# Print the model summary\n","\n"]},{"cell_type":"markdown","metadata":{"id":"icglyLpcERst"},"source":["We will visualise the features extracted by this model at different levels of hierarchy for the following image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMM3OOfRERst"},"outputs":[],"source":["# Load a colour image\n","\n","image = tf.io.read_file(\"./figures/hoover_dam.JPEG\")\n","image = tf.io.decode_jpeg(image, channels=3)\n","plt.figure(figsize=(6, 10))\n","plt.imshow(image)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x8JDcRBFERst"},"source":["We will use the [functional API](https://www.tensorflow.org/guide/keras/functional) to create a multi-output model that outputs different hidden layer outputs within the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OF2oVUTERst"},"outputs":[],"source":["# Define the multi-output model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"io-WvX8oERsu"},"outputs":[],"source":["# View the model inputs and outputs Tensors\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9CykIH0ERsu"},"outputs":[],"source":["# Extract the hierarchical features for this image\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"5UkD91PkERsu"},"outputs":[],"source":["# Visualise the features\n","\n","import numpy as np\n","\n","n_rows, n_cols = 2, 3\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 14))\n","fig.subplots_adjust(hspace=0.05, wspace=0.2)\n","\n","for i in range(len(features)):\n","    feature_map = features[i]\n","    num_channels = feature_map.shape[-1]\n","    row = i // n_cols\n","    col = i % n_cols\n","    if i == 0:\n","        axes[row, col].imshow(image)\n","        axes[row, col].set_title('Original image')\n","    else:\n","        random_feature = np.random.choice(num_channels)\n","        axes[row, col].imshow(feature_map[0, ..., random_feature])\n","        axes[row, col].set_title('{}, channel {} of {}'.format(layer_names[i-1], random_feature + 1, num_channels))\n","        \n","    axes[row, col].get_xaxis().set_visible(False)\n","    axes[row, col].get_yaxis().set_visible(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fb2fzg4uERsv"},"source":["*Exercise:* load one of your own images to view the features extracted by the VGG-19 network."]},{"cell_type":"markdown","metadata":{"id":"3tg8OWqcERsv"},"source":["<a class=\"anchor\" id=\"paddingstrides\"></a>\n","## Padding and strides\n","\n","Padding and strides are additional properties of convolutional (and pooling) layers that can give some flexibility over the spatial dimensions of the output. \n","\n","In this section, we will define these properties and the effects they have on the input and output dimensions of a convolutional layer. For a more complete guide to convolutional arithmetic, see [Dumoulin & Visin 2016](#Dumoulin16)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clgBa1abERsv"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"ZmNzGF_4ERsw"},"source":["#### Padding\n","Recall that in our earlier example, an input grayscale image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ convolved with a kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 1\\times 1}$ produced an output of size $\\mathbf{h}\\in\\mathbb{R}^{5\\times 4\\times 1}$:\n","\n","<img src=\"figures/1_channel_no_padding.gif\" alt=\"2D convolution on a single channel input\" style=\"width: 600px;\"/>\n","<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ produces an output $\\mathbf{h}\\in\\mathbb{R}^{5\\times 4\\times 1}$</center>\n","\n","In general, for a spatial dimension of size $i$ and a kernel of width $k$, the output size $o$ is given by\n","\n","$$\n","o = i - k + 1 \\label{valid}\\tag{4}\n","$$\n","\n","In many model architectures, it is desirable to keep the spatial dimensions the same in the output of a convolutional layer. This can be achieved by padding the input layer with zeros.\n","\n","In the case of our kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times1\\times1}$, if we add 2 zeros in the first dimension and 3 zeros in the second dimension (distributed on either side of the image), this will result in an output $\\mathbf{h}\\in\\mathbb{R}^{7\\times 7\\times 1}$ that has the same spatial dimensions as the input $\\mathbf{x}$. This type of padding is known as \"SAME\" padding.\n","\n","<img src=\"figures/1_channel_with_padding.gif\" alt=\"2D convolution on a single channel input with 'SAME' padding\" style=\"width: 600px;\"/>\n","<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ with \"SAME\" padding</center>\n","\n","Now, if $p$ zeros are added to our input size $i$ with kernel width $k$, then the output size $o$ is given by\n","\n","$$\n","o = i + p - k + 1 \\label{padding}\\tag{5}\n","$$\n","\n","If $p = k-1$ (\"SAME\" padding) then the $o = i$. If $p = 0$ (\"VALID\" padding) then we recover \\eqref{valid} and $o = i-k+1$. There is also \"FULL\" padding where $p = 2(k-1)$, so that $o = i + k - 1$, although this is less common.\n","\n","In TensorFlow, we can easily apply zero padding to convolutional layers with the `padding` keyword argument, which can be set to `\"VALID\"` (the default) or `\"SAME\"`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_n074KlERsw"},"outputs":[],"source":["# Create a CNN with 'VALID' and 'SAME' padding\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPool2D\n","\n","inputs = Input(shape=(32, 32, 3))\n","h = Conv2D(16, (3, 5), padding=\"VALID\", activation='relu')(inputs)\n","h = MaxPool2D(2)(h)\n","outputs = Conv2D(16, 3, padding=\"SAME\", activation='relu')(h)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Poib82BaERsx"},"source":["Note the input and output shapes of each of the convolutional layers."]},{"cell_type":"markdown","metadata":{"id":"KdsdyqmMERsx"},"source":["#### Strides\n","Convolutions may also use a stride $s$, which is the distance between consecutive positions of the kernel. So far, all of our examples have used $s=1$, however it is easy to see that using $s>1$ leads to a downsampling of the input. The following animation shows our input $\\mathbf{x}\\in\\mathbb{R}^{7\\times7\\times 1}$, this time with a kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$, \"SAME\" padding, and a stride $s=2$ in both spatial dimensions:\n","\n","<img src=\"figures/1_channel_stride_2.gif\" alt=\"2D convolution on a single channel input with stride 2\" style=\"width: 600px;\"/>\n","<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ with \"SAME\" padding and stride of $(2, 2)$</center>\n","\n","In the above example, the stride and kernel size exactly divides the input and padding size, however this does not need to be the case, such as in the following example where $\\mathbf{x}\\in\\mathbb{R}^{7\\times6\\times 1}$:\n","\n","<img src=\"figures/1_channel_7x6_stride_2.gif\" alt=\"2D convolution on a single channel input with stride 2\" style=\"width: 600px;\"/>\n","<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 6\\times 1}$ with \"SAME\" padding and stride of $(2, 2)$</center>\n","\n","In this example the input $\\mathbf{x}\\in\\mathbb{R}^{7\\times6\\times 1}$ has been downsampled to an output $\\mathbf{h}\\in\\mathbb{R}^{4\\times 3\\times 1}$. Note that if the input image size was instead $7\\times 5\\times 1$ then the output size would again be $4\\times 3\\times 1$.\n","\n","<img src=\"figures/1_channel_7x5_stride_2.gif\" alt=\"2D convolution on a single channel input with stride 2\" style=\"width: 600px;\"/>\n","<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 5\\times 1}$ with \"SAME\" padding and stride of $(2, 2)$</center>\n","\n","In all cases, for a spatial dimension of size $i$ with padding $p$ and a kernel of width $k$ with stride $s$, the output size $o$ is given by\n","\n","$$\n","o = \\left\\lfloor \\frac{i + p - k}{s} \\right\\rfloor + 1 \\label{stride}\\tag{6}\n","$$\n","\n","In TensorFlow, strides can be set in both convolutional and pooling layers using the `strides` keyword argument. \n","\n","Just as with the `kernel_size` argument for `Conv2D` layers (and the `pool_size` argument for `MaxPool2D` layers), either a tuple of integers can be passed in, or a single integer - which indicates the stride should be the same in all spatial dimensions. \n","\n","In `MaxPool2D` layers, the default stride is set to be equal to the `pool_size`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_m_C9jhERsy"},"outputs":[],"source":["# Create a CNN using strides in the Conv2D and MaxPool2D layers\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n","\n","inputs = Input(shape=(128, 128, 3))\n","h = Conv2D(16, (5, 5), padding=\"VALID\", strides=(2, 3), activation='relu')(inputs)\n","h = MaxPool2D(2)(h)\n","h = Conv2D(16, 7, padding=\"SAME\", activation='relu')(h)\n","h = MaxPool2D((2, 4), strides=(3, 2))(h)\n","h = Conv2D(16, 3, padding=\"SAME\", strides=2, activation='relu')(h)\n","h = Flatten()(h)\n","h = Dense(16, activation='relu')(h)\n","outputs = Dense(1, activation='sigmoid')(h)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"0iyZdUpmERsy"},"source":["Note the input and output shapes of each of each layers, and compare with the formula $(6)$ for the output size given above."]},{"cell_type":"markdown","metadata":{"id":"vfHXa1eUERsy"},"source":["<a class=\"anchor\" id=\"references\"></a>\n","## References\n","\n","<a class=\"anchor\" id=\"Dumoulin16\"></a>\n","* Dumoulin, V. & Visin, F. (2016), \"A guide to convolution arithmetic for deep learning\", arXiv preprint, abs/1603.07285.\n","<a class=\"anchor\" id=\"Fukushima80\"></a>\n","* Fukushima, K. (1980), \"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\", *Biological Cybernetics*, **3**6 (4), 193–202.\n","<a class=\"anchor\" id=\"Hubel59\"></a>\n","* Hubel, D. H. & Wiesel, T. N. (1959), \"Receptive fields of single neurones in the cat's striate cortex\", *Journal of Physiology* **148** (3), 574–91.\n","<a class=\"anchor\" id=\"LeCun89\"></a>\n","* LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989) \"Backpropagation Applied to Handwritten Zip Code Recognition\", AT&T Bell Laboratories.\n","<a class=\"anchor\" id=\"Rumelhart86b\"></a>\n","* Rumelhart, D. E., Hinton, G., and Williams, R. (1986b), \"Learning representations by back-propagating errors\", Nature, **323**, 533-536.\n","<a class=\"anchor\" id=\"Simonyan15\"></a>\n","* Simonyan, K. & Zisserman, A. (2015), \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", in *3rd International Conference on Learning Representations, (ICLR) 2015*, San Diego, CA, USA.\n","<a class=\"anchor\" id=\"Vinyals19\"></a>\n","* Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T. L., Gulcehre, C., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., & Silver, D.(2019) \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\", *Nature*, **575** (7782), 350-354.\n","<a class=\"anchor\" id=\"Zhou88\"></a>\n","* Zhou, Y. & Chellappa, R. (1988), \"Computation of optical flow using a neural network\", in *IEEE International Conference on Neural Networks*, IEEE, 71-78."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Lecture_Week 3 - ConvNets.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}